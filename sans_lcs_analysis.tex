\chapter{Erste Analyse der Agenten ohne XCS}\label{analysis_sans_lcs:cha}

In diesem Kapitel sollen erste Analysen bezüglich der verwendeten Szenarien anhand des Algorithmus zufälliger Bewegung (siehe Kapitel~\ref{randomized_movement:sec}), des Algorithmus mit einfacher Heuristik (siehe Kapitel~\ref{simple_heuristik:sec}) und des Algorithmus mit intelligenter Heuristik (siehe Kapitel~\ref{intelligent_heuristik:sec}) angefertigt werden. Die Ergebnisse aus der Analyse werden eine Grundlage für die vergleichende Betrachtung der Agenten mit XCS Algorithmen in Kapitel~\ref{lcs_analysis:cha} dienen, insbesondere werden sie Anhaltspunkte dafür geben, welche Szenarien welche Eigenschaften der Algorithmen testen. Außerdem kann der Vergleich von Agenten intelligenten Heuristik mit Agenten mit zufälliger Bewegung Aufschluss darüber geben, wieviel und welche Aspekte ein Agent in einem solchen Szenario überhaupt lernen kann. Große Unterschiede zwischen intelligenter und einfacher Heuristik weisen beispielsweise darauf hin, dass die Verteilung auf dem Torus wichtiger ist, als das Hinterherlaufen. TODO ref

Beim Testen selbst war es durch die Verwendung der in Kapitel~\ref{qualitaet:sec} erwähnten (und im Simulator berechneten) Halbzeitqualitäten sehr einfach, festzustellen, ob ein Algorithmus noch Potential hatte, d.h. ob eine Erhöhung der Schrittzahl die Qualität weiter steigern würde. Da (im Gegensatz zu den später in Kapitel~\ref{lcs_variants:cha} erwähnten lernenden Algorithmen) keiner der hier vorgestellten Algorithmen lernt und somit statische Regeln besitzt, ist es nicht notwendig, die Qualitäten der Algorithmen bei verschiedener Anzahl von Zeitschritten zu betrachten und zu vergleichen. Die Zahl der Zeitschritte wird somit für alle Tests, soweit nicht anders angegeben, standardmäßig auf 500 festgesetzt.\\

Außerdem sollen in den Statistiken die Werte jeweils über einen Lauf von 10 Experimenten mit jeweils 10 Probleminstanzen (siehe Kapitel~\ref{definition_probleminstanz:sec}) ermittelt und gemittelt werden, was erfahrungsgemäß ausreicht um ausreichend genaue Ergebnisse zu erhalten. 


\section{Definition einer Probleminstanz}\label{definition_probleminstanz:sec}

Eine einzelne Probleminstanz entspricht hier einem Torus mit einer bestimmten Anfangsbelegung mit bestimmten Objekten und bestimmten Parametern zur Sichtbarkeit, auf dem über die erwähnte Anzahl von Schritten die Simulation abläuft. Die Anfangsbelegung ist über einen \emph{random seed} Wert bestimmt, wobei bei jeder Probleminstanz mit einem neuen Wert initialisiert wird, der sich aus der Nummer des Experiments und der Nummer des Problems berechnet. Die Probleminstanzen sind also untereinander unterschiedlich, jedoch vergleichbar mit anderen Testdurchläufen mit einer anderen Konfiguration. Soweit nicht anders angegeben, sollen hier Probleminstanzen der Größe 16x16 Felder betrachtet werden, insbesondere beziehen sich die Ergebnisse der Tests auf diesen Fall. Die in den Tabellen jeweils angegebenen Werte sind auf zwei Stellen nach dem Komma gerundet.\\

Während eines Testlaufs werden eine ganze Reihe von statistischen Merkmalen erfasst. Wesentliches Merkmal zum Vergleich der Algorithmen ist der Wert der Qualität (siehe Kapitel~\ref{qualitaet:sec}), weitere Merkmale dienen zur Erklärung, warum z.B. ein Algorithmus bei einem Durchlauf schlechte Ergebnisse lieferte, bzw. dienten zum Testen und Finden von Fehlern oder Schwächen des Simulationsprogramms. Im Einzelnen sind hier zu nennen:

\begin{enumerate}

\item Anteil Sprünge des Zielobjekts (siehe Kapitel~\ref{zielobjekt:sec}), Durchläufe mit hohen Werte müssten verworfen werden
\item Anteil blockierter Bewegungen der Agenten
\item Halbzeitqualität (siehe Kapitel~\ref{qualitaet:sec}), größere Unterschiede zur ermittelten Qualität deuten darauf hin, dass sich der Algorithmus noch nicht stabilisiert hat und das Szenario mit höherer Schrittzahl erneut durchgeführt werden sollte
\item Abdeckung
\item Varianz der individuellen Punkte, ungefähres Maß, inwieweit einzelne Agenten an der Gesamtqualität beteiligt waren 

\end{enumerate}


\subsection{Abdeckung}

Die theoretisch maximal mögliche Anzahl an Felder, die die Agenten innerhalb ihrer Überwachungsreichweite zu einem Zeitpunkt haben können, entspricht der Zahl der Agenten multipliziert mit der Zahl der Felder, die ein Agent in seiner Übertragungsreichweite haben kann. Ist dieser Wert größer als die Gesamtzahl aller freien Felder, wird stattdessen dieser Wert benutzt.\\
Teilt man nun die Anzahl der momentan tatsächlich überwachten Felder durch die eben ermittelte maximal mögliche Anzahl an überwachten Felder, erhält man die Abdeckung, die die Agenten momentan erreichen.\\


\subsection{Qualität eines Algorithmus}\label{qualitaet:sec}

Die Qualität eines Algorithmus zu einem Problem wird anhand des Anteils der Zeit berechnet, die er das Zielobjekt während des Problems überwachen (d.h. das Zielobjekt innerhalb einer Distanz von höchstens \emph{reward range} halten) konnte, relativ zur Gesamtzeit.\\
Die Qualität eines Algorithmus zu einer Anzahl von Problemen (also einem Experiment) wird Anhand des Gesamtanteil der Zeit berechnet, die er das Zielobjekt während aller Probleme überwachen konnte, relativ zur Gesamtzeit aller Probleme.\\
Die Qualität eines Algorithmus entspricht dem Durchschnitt der Qualitäten des Algorithmus mehrerer Experimente.\\
Die Halbzeitqualität eines Algorithmus zu einem Problem entspricht dem Anteil der Zeit, die der Algorithmus das Zielobjekt während jeweils der zweiten Hälfte des Problems überwachen konnte, relativ zur halben Gesamtzeit.\\
Die Halbzeitqualität eines Algorithmus zu einer Anzahl von Problemen entspricht dem Anteil der Zeit, die der Algorithmus das Zielobjekt während jeweils der zweiten Hälfte des Problems überwachen konnte, relativ zur halben Gesamtzeit aller Probleme.\\
Die Halbzeitqualität eines Algorithmus entspricht dem Durchschnitt aller Halbzeitqualitäten des Algorithmus mehrerer Experimente.\\
Ein Vergleich der Qualität mit der Halbzeitqualität eines Algorithmus ermöglicht einen Einblick, wie gut sich der Algorithmus verhält, nachdem er sich auf das Problem bereits eine Zeit lang einstellen konnte.\\


\section{Ablauf der Simulation}\label{reihenfolge:sec}

Die Simulation selbst läuft in ineinander geschachtelten Schleifen ab. Jede Konfiguration (in den abgedruckten Programmen jeweils über die globale Variable \emph{Configuration} angesprochen) wird über eine Reihe von Experimenten getestet (10 soweit nicht anders angegeben). Für einen Test wird die Funktion \emph{doOneMultiStepExperiment()} (siehe Programm~\ref{mainExperiment:pro}) mit der aktuellen Nummer des Experiments als Parameter aufgerufen. In der Funktion wird ein neuer \emph{random seed} Wert initialisiert, der Torus auf den Startzustand gesetzt und schließlich das eigentliche Problem mit der Funktion \emph{doOneMultiStepProblem()} aufgerufen, welche in Programm~\ref{mainProblem:pro} abgebildet ist. Dort werden in einer Schleife alle Schritte durchlaufen und jeweils die Objekte abgearbeitet.\\

In welcher Reihenfolge dies geschieht, soll im Folgenden geklärt werden. Zusammenfassend ist zu sagen, dass zuerst die aktuelle Qualität und die aktuellen Sensordaten bestimmt werden. Daraus ermittelt jeder Agent die Bewertung für den letzten Schritt und bestimmt eine neue Aktion. Haben Agenten und das Zielobjekt diese Schritte abgeschlossen, werden ihre ermittelten Aktionen in zufälliger Reihenfolge ausgeführt.\\


Bei der Berechnung eines einzelnen Problems in der Funktion \emph{doOneMultiStepProblem()} stellt sich die Frage nach der Genauigkeit und der Reihenfolge der Abarbeitung, da die Simulation nicht parallel, sondern schrittweise auf einem diskreten Torus abläuft. Dies kann u.U. dazu führen, dass je nach Position in der Liste abzuarbeitender Agenten die Informationen über die Umgebung unterschiedlich alt sind. Die Frage ist deshalb, in welcher Reihenfolge Sensordaten ermittelt, ausgewertet, Agenten bewegt, intern sich selbst bewertet und global die Qualität gemessen wird.\\

Da eine Aktion auf Basis der Sensordaten ausgewählt wird, ist die erste Restriktion, dass eine Aktion nach der Verarbeitung der Sensordaten stattfinden muss. Da außerdem Aktionen bewertet werden sollen, also jeweils der Zustand nach der Bewegung mit dem gewünschten Zustand verglichen werden soll, ist die zweite Restriktion, dass die Bewertung einer Aktion nach dessen Ausführung stattfinden muss.\\

Unter diesen Voraussetzungen ergeben sich folgende zwei Möglichkeiten:

\begin{enumerate}
\item Für alle Agenten werden erst einmal die neuen Sensordaten erfasst und sich für eine Aktion entschieden. Sind alle Agenten abgearbeitet, werden die Aktionen ausgeführt.
\item Die Agenten werden nacheinander abgearbeitet, es werden jeweils neue Sensordaten erfasst und sich sofort für eine neue Aktion entschieden. 
\end{enumerate}

Bei der ersten Möglichkeit haben alle Agenten die Sensordaten vom Beginn der Zeiteinheit, während bei der zweiten Möglichkeit später verarbeitete Agenten bereits die Aktionen der bereits berechneten Agenten miteinbeziehen können. Umgekehrt können dann frühere Agenten bessere Positionen früher besetzen. Da aufgrund der primitiven Sensoren nicht davon auszugehen ist, dass Agenten beginnende Bewegungen (und somit deren jeweilige Zielposition) anderer Agenten einbeziehen können, soll jeder Agent von den Sensorinformationen zu Beginn der Zeiteinheit ausgehen.\\

Wenn sich mehrere Agenten auf dasselbe Feld bewegen wollen, dann spielt die Reihenfolge der Ausführung der Aktionen eine Rolle. Wird die Liste der Agenten einfach linear abgearbeitet, können Agenten mit niedriger Position in der Liste die Aktion auf Basis jüngerer Sensordaten fällen. Dies kann dazu führen, dass Aktionen von Agenten mit höherer Position in der Liste eher fehlschlagen, da das als frei angenommene Feld nun bereits besetzt ist. Da es keinen Grund gibt, Agenten mit niedrigerer Position zu bevorteilen, werden die Aktionen der Agenten in zufälliger Reihenfolge abgearbeitet.\\

Bezüglich der Bewegung ergibt sich hierbei eine weitere Frage, nämlich wie unterschiedliche Bewegungsgeschwindigkeiten behandelt werden sollen, da alle Agenten eine Einheitsgeschwindigkeit von einem Feld pro Zeiteinheit haben, während sich das Zielobjekt je nach Szenario gleich eine ganze Anzahl von Feldern bewegen kann (siehe auch Kapitel~\ref{zielobjekt:sec}).\\

Die Entscheidung fiel hier auf eine zufällige Verteilung. Kann sich das Zielobjekt um \(n\) Schritte bewegen, so wird seine Bewegung in \(n\) Einzelschritte unterteilt, die nacheinander mit zufälligen Abständen (d.h. Bewegungen anderer Agenten) ausgeführt werden.\\

Eine weitere Frage ist, wie das Zielobjekt diese weiteren Schritte festlegen soll. Hier soll ein Sonderfall eingeführt werden, sodass das Zielobjekt in einer Zeiteinheit mehrmals (\(n\)-mal) neue Sensordaten erfassen und sich für eine neue Aktion entscheiden kann.



\subsection{Messung der Qualität}\label{qualitaetsmessung:sec}

TODO welche zwei Fragen?
Eine konkrete Antwort kann man auf diese zwei Fragen nicht geben, sie hängt  davon ab, was man denn nun eigentlich erreichen möchte, also auf welche Weise die Qualität des Algorithmus bewertet wird. Der naheliegendste Messzeitpunkt ist, nachdem sich alle Agenten bewegt haben. Da die Agenten und das Zielobjekt in einem Durchlauf gemeinsam nacheinander bewegt werden, stellt sich die Frage nicht, ob womöglich vor der Bewegung des Zielobjekts die Qualität gemessen werden sollen. Eine Messung nach der Bewegung des Zielobjekts würde diesem erlauben, sich vor jeder Messung optimal zu positionieren, was in einer geringeren Qualität für den Algorithmus resultiert, da sich das Zielobjekt aus der Überwachungsreichweite anderer Agenten hinausbewegen kann. Letztlich ist es eine Frage der Problemstellung, denn eine Messung nach Bewegung des Zielobjekts bedeutet letztlich, dass ein Agent einen gerade aus seiner Überwachungsreichweite heraus laufenden Zielobjekts in diesem Schritt nicht mehr überwachen kann.\\
Da ein wesentlicher Bestandteil die Kooperation (und somit die Abdeckung des Torus anstatt dem Verfolgen des Zielobjekts) sein soll, soll ein Bewertungskriterium sein, inwieweit der Einfluss des Zielobjekts minimiert werden soll. Auch findet, wenn man vom realistischen Fall ausgeht, die Bewegung des Zielobjekts gleichzeitig mit allen anderen Agenten statt. Die Qualität wird somit nach der Bewegung des Zielobjekts gemessen. Die Überlegung unterstreicht auch nochmal, dass es besser ist, das Zielobjekt insgesamt wie einen normalen (aber sich mehrmals bewegenden) Agenten zu behandeln.\\


\subsection{Reihenfolge der Ermittlung des \emph{base reward}}
TODO

Keine der bisher vorgestellten Varianten machen Gebrauch von einem sogenannten \emph{base reward}, d.h.  TODO

Schließlich bleibt die Frage danach, wann geprüft werden soll, ob das Zielobjekt in Überwachungsreichweite ist, und wann sich somit ein \emph{reward} ergeben soll. Wesentliche Punkte hierbei sind, dass der Algorithmus sich anhand der Sensordaten selbst bewertet und pro Zeitschritt die Sensordaten nur einmal erhoben werden. Letzteres folgt aus der Auslegung von XCS, der in der Standardimplementation darauf ausgelegt ist, dass der \emph{base reward} Wert jeweils genau einer Aktion zugeordnet ist. Daraus ergibt sich auch, dass der \emph{base reward} von binärer Natur ("`Zielobjekt in Überwachungsreichweite"' oder "`Zielobjekt nicht in Überwachungsreichweite"') ist, weshalb Zwischenzustände für diesen Wert , der sich aus der mehrfachen Bewegung des Zielobjekts ergeben könnte (z.B. "`War zwei von drei Schritten in der Überwachungsreichweite"' \(\Rightarrow \frac{2}{3}\) \emph{base reward}), ausgeschlossen werden soll. Insbesondere würde dies eine mehrfache Erhebung der Sensordaten erfordern.

Für den \emph{base reward} ergeben sich somit folgende Möglichkeiten:

\begin{enumerate}
\item Ermittlung der einzelnen \emph{reward} Werte jeweils direkt nach der Ausführung einer einzelnen Aktion
\item Ermittlung aller \emph{reward} Werte nach Ausführung aller Aktionen der Agenten und des Zielobjekts
\end{enumerate}

Werden die \emph{reward} Werte sofort ermittelt (Punkt 1), dann bezieht sich der Wert auf die veralteten Sensordaten vor der Aktion, die Aktion selbst würde bei der Ermittlung des \emph{reward} Werts also ignoriert werden. Bei Punkt 2 müsste man bis zum neuen Zeitschritt warten, bis neue Sensordaten ermittelt wurden.


\subsection{Zusammenfassung des Simulationsablaufs}

Zusammenfassend sieht der Ablauf aller Agenten (inklusive des Zielobjekts) also wie folgt aus:

\begin{figure}[H]
\setbox0\vbox{\small
\begin{enumerate}
\item Bestimmen der aktuellen \textbf{Qualität}
\item Erfassung aller \textbf{Sensordaten}
\item Bestimmung der jeweiligen {\bfseries {\em reward} Werte} für die einzelnen Objekte für den letzten Schritt (für lernende Agenten)
\item \textbf{Wahl der Aktion} anhand der Regeln des jeweiligen Agenten
\item \textbf{Ausführung der Aktion} (in zufälliger Reihenfolge, das Zielobjekt wiederholt Schritte 1 und 2 nach der Ausführung der Aktion)
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}



\section{Zielobjekt mit zufälligem Sprung}\label{zielobjekt_analyse_zufall_sprung:sec}

Im folgenden sollen alle TODO



In allen Szenarien mit dieser Form der Bewegung des Zielobjekts kommt es nur darauf an, dass die Agenten einen möglichst großen Bereich des Torus abdecken. 


\subsection{Im leeren Szenario ohne Hindernisse}\label{jump_empty_scenario:sec}

Ohne Hindernisse gibt sich ein klares Bild (siehe Tabelle~\ref{table:empty_total_random}), die intelligente Heuristik ist etwas besser als der des zufälligen Agenten und der einfachen Heuristik. Ein möglichst weiträumiges Verteilen auf dem Torus führt zum Erfolg, was sich auch in einem hohen Wert der Abdeckung zeigt, denn genau das wird mit dem völlig zufällig springenden Agenten getestet. Auch ist die Zahl der blockierten Bewegungen deutlich niedriger, was sich auch mit der Haltung des Abstands erklären lässt.\\

Die einfache Heuristik schneidet dagegen etwas schlechter als eine zufällige Bewegung ab. Zwar ist die Zahl der blockierten Bewegungen geringer, was sich dadurch erklären lässt, dass die einfache Heuristik zumindest an einem Punkt eine Sichtbarkeitsüberprüfung für die Richtung durchführt, in der sie sich bewegen möchte (nämlich wenn das Zielobjekt in Sicht ist), andererseits ist die Abdeckung etwas geringer. Dies kommt daher, dass, wenn mehrere Agenten das Zielobjekt in derselben Richtung in Sichtweite haben, mehrere Agenten sich in dieselbe Richtung bewegen. Dies beeinträchtigt die zufällige Verteilung der Agenten auf dem Spielfeld und führt somit auch zu einer niedrigeren Abdeckung des Torus.\\

Bezüglich der Anzahl der Agenten ergeben sich keine Besonderheiten, mit steigender Agentenzahl steigt die Zahl der blockierten Bewegungen (aufgrund größerer Anzahl von blockierten Feldern), während die Abdeckung sinkt (aufgrund sich überlappender Überwachungsreichweiten).

\begin{table}[ht]
\caption{Zufällige Sprünge des Zielobjekts im leeren Szenario ohne Hindernisse}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Agentenzahl & Blockierte Bewegungen & Abdeckung & Qualität \\ [0.5ex]
\hline
Zufällige Bewegung     & 8  & 2,82\% & 73,78\% & 32,36\% \\
Einfache Heuristik     & 8  & 2,79\% & 73,22\% & 32,10\% \\
Intelligente Heuristik & 8  & 0,64\% & 81,26\% & 35,91\% \\ [1ex]
\hline
Zufällige Bewegung     & 12 & 4,32\% & 69,55\% & 44,75\% \\
Einfache Heuristik     & 12 & 4,19\% & 68,88\% & 43,86\% \\
Intelligente Heuristik & 12 & 1,49\% & 77,60\% & 49,49\% \\ [1ex]
\hline
Zufällige Bewegung     & 16 & 5,82\% & 64,28\% & 54,55\% \\
Einfache Heuristik     & 16 & 5,66\% & 63,65\% & 53,99\% \\
Intelligente Heuristik & 16 & 2,85\% & 71,44\% & 60,73\% \\ [1ex]
\hline
\end{tabular}
\label{table:empty_total_random}
\end{table}


\subsection{Säulenszenario}

Für das Säulenszenario (siehe Tabelle~\ref{table:pillar_total_random}) ergeben sich erwartungsgemäß ähnliche Werte wie im Fall des leeren Szenarios ohne Hindernisse (siehe Tabelle~\ref{table:empty_total_random}). Durch geringere Sicht und höhere Zahl an blockierten Bewegungen ergibt sich jeweils eine geringere Abdeckung und auch jeweils eine geringere Qualität. Auch hier ergeben sich keine Besonderheiten bezüglich der Agenten, im Folgenden werden sich die Tests deshalb auf den Fall mit 8 Agenten beschränken.

\begin{table}[ht]
\caption{Zufällige Sprünge des Zielobjekts in einem Säulenszenario}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Agentenzahl & Blockierte Bewegungen & Abdeckung & Qualität \\ [0.5ex]
\hline
Zufällige Bewegung     & 8  & 4,45\% & 72,11\% & 32,13\% \\
Einfache Heuristik     & 8  & 4,08\% & 71,70\% & 31,99\% \\
Intelligente Heuristik & 8  & 2,34\% & 79,61\% & 35,29\% \\ [1ex]
\hline
Zufällige Bewegung     & 12 & 5,93\% & 67,72\% & 44,44\% \\
Einfache Heuristik     & 12 & 5,67\% & 67,23\% & 43,81\% \\
Intelligente Heuristik & 12 & 3,62\% & 75,86\% & 49,34\% \\ [1ex]
\hline
Zufällige Bewegung     & 16 & 7,62\% & 62,53\% & 54,26\% \\
Einfache Heuristik     & 16 & 7,23\% & 62,00\% & 53,58\% \\
Intelligente Heuristik & 16 & 5,18\% & 69,91\% & 60,43\% \\ [1ex]
\hline
\end{tabular}
\label{table:pillar_total_random}
\end{table}


\subsection{Zufällig verteilte Hindernisse}


Hier ergibt sich für alle Einstellungen für \(\lambda_{h}\) und \(\lambda_{p}\) (siehe Kapitel~\ref{random_scenario_definition:sec}) ebenfalls ein eindeutiges Bild (siehe Tabelle~\ref{table:full_total_random}), die intelligente Heuristik liegt wieder vorne, gefolgt wieder von der einfachen Heuristik und der zufälligen Bewegung. Im Fall mit vielen Hindernissen (\(\lambda_{h} = 0,2\)) liegt die einfache Heuristik trotz höherer Abdeckung hinter der zufälligen Bewegung. Dies ist wohl auf einen Zufall zurückzuführen, ändert man den \emph{random seed} Wert oder erhöht man die Anzahl der Experimente von 10 auf 30 ergibt sich wieder oben genannte Reihenfolge.\\

Dass der einfache Agent, wenn er das Zielobjekt in Sicht hat, eine geringere Zahl an blockierten Bewegungen als der zufällige Agent aufweist, lässt sich damit begründen, dass er davon ausgehen kann, dass sich in dieser Richtung wahrscheinlich eher kein Hindernis befindet (da die Sicht nicht blockiert ist), während der zufällige Agent Hindernisse überhaupt nicht beachtet, somit öfters gegen ein Hindernis läuft und letztlich öfters stehen bleibt. Der Unterschied zwischen beiden Agenten ist besonders hoch in Szenarien mit größerem Anteil an Hindernissen.\\

Im Vergleich zur einfachen Heuristik scheint insbesondere die intelligente Heuristik Probleme mit den Hindernissen zu haben (viele blockierte Bewegungen). Da Hindernisse in der Heuristik nicht beachtet werden, bewirkt die Strategie der maximalen Ausbreitung der Agenten, dass die Agenten gegen die Hindernisse gedrückt werden (andere Agenten sind bei hohem Verknüpfungsfaktor eher in einem Bereich ohne Hindernisse).\\

Schließlich ist zu sehen, dass die Agenten in einem Szenario mit höherem Verknüpfungsfaktor (der Fall mit \(\lambda_{h} = 0,1\) und \(\lambda_{p} = 0,99\) im Vergleich zum Fall mit \(\lambda_{h} = 0,1\) und \(\lambda_{p} = 0,5\)) besser abschneiden. Dies liegt daran, dass Szenarien mit hohem Verknüpfungsfaktor bedeuten, dass viele Hindernisse zusammenhängend einen großen Block bilden und somit dem Szenario ohne Hindernisse ähnlich sind, da es eher größere zusammenhängende Flächen gibt.\\

Insgesamt ist zu sagen, dass keine der Szenarien mit zufälligem Sprung des Zielobjekts sich als zu lernende Aufgabe lohnt, der Unterschied zwischen der zufälligen Bewegung und der intelligenten Heuristik ist zu gering, die Aufgabe somit zu schwierig und soll in Verbindung mit XCS, bis auf einen einfachen Test zum Vergleich (siehe Kapitel TODO), nicht weiter betrachtet werden.

TODO warum besser mit mehr HIndernissen

\begin{table}[ht]
\caption{Zufällige Sprünge des Zielobjekts in einem Szenario mit Hindernisse (8 Agenten)}
\centering
\begin{tabular}{c c c c c c}
\hline\hline
Algorithmus & \(\lambda_{h}\) & \(\lambda_{p}\) & Blockierte Bewegungen & Abdeckung & Qualität \\ [0.5ex]
\hline
Zufällige Bewegung     & 0,2 & 0,99 & 12,44\% & 62,50\% & 34,54\% \\
Einfache Heuristik     & 0,2 & 0,99 & 10,04\% & 63,02\% & 34,48\% \\
Intelligente Heuristik & 0,2 & 0,99 & 12,71\% & 68,22\% & 37,89\% \\ [1ex]
\hline
Zufällige Bewegung     & 0,1 & 0,99 &  7,58\% & 68,33\% & 32,81\% \\
Einfache Heuristik     & 0,1 & 0,99 &  6,15\% & 68,49\% & 33,36\% \\
Intelligente Heuristik & 0,1 & 0,99 &  6,50\% & 74,81\% & 36,29\% \\ [1ex]
\hline
Zufällige Bewegung     & 0,1 & 0,5  & 10,12\% & 66,01\% & 32,03\% \\
Einfache Heuristik     & 0,1 & 0,5  &  8,57\% & 66,52\% & 32,38\% \\
Intelligente Heuristik & 0,1 & 0,5  &  9,29\% & 72,63\% & 35,12\% \\ [1ex]
\hline
\end{tabular}
\label{table:full_total_random}
\end{table}





FAZIT:

Je schneller, zufälliger 



\section{Zielobjekt mit zufälliger Bewegung bzw. einfacher Richtungsänderung}

TODO kürzerer Titel für beide Bewegungsarten!!!


Wesentlicher Punkt bei beiden Bewegungstypen (siehe Kapitel~\ref{random_neighbor:sec} und Kapitel~\ref{direction_change:sec}) ist, dass der jetzige Ort des Zielobjekts maximal zwei Felder (die maximale Geschwindigkeit des Zielobjekts in den Tests) vom Ort in der vorangegangenen Zeiteinheit entfernt ist. Somit ist ein lokales Einfangen eher von Relevanz, der Ort an dem sich das Zielobjekt im nächsten Zeitschritt befinden wird, ist zumindest vom aktuellen Ort abhängig, wenn das Zielobjekt auch schneller sein kann als andere Agenten.\\

Wesentlicher Unterschied zwischen beiden Bewegungstypen ist, dass das Zielobjekt mit zufälliger Bewegung nach 2 Schritten mit Wahrscheinlichkeit von \(\frac{1}{4}\) auf das ursprüngliche Feld zurückkehrt, also stehenbleibt. Wie die Ergebnisse in Tabellen~\ref{table:neighbor_change_random} und ~\ref{table:neighbor_change_pillar} zeigen, ergibt sich dadurch ein leichteres Szenario. Ein mitunter stehenbleibender Agent kann mittels Heuristiken leichter überwacht werden, während es keine signifikante Veränderung bei der zufälligen Bewegung ergibt. In weiteren Tests soll deswegen immer nur Zielobjekten mit einfacher Richtungsänderung getestet werden.


33, 
%TODO table:neighbor_change_no_obstacles

TODO

\begin{table}[ht]
\caption{Vergleich von Zielobjekt mit zufälliger Bewegung und einfacher Richtungsänderung (8 Agenten, leeres Szenario ohne Hindernisse)}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Sprünge Zielobjekt & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Sich zufällig bewegendes Zielobjekt \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% &  2,71\% & 73,85\% & 32,57\% \\
Einfache Heuristik     & 0,06\% & 11,51\% & 63,65\% & 79,97\% \\
Intelligente Heuristik & 0,02\% &  4,71\% & 71,15\% & 81,59\% \\ [1ex]
\hline
Zielobjekt mit einfacher Richtungsänderung \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% &  2,75\% & 73,81\% & 30,99\% \\
Einfache Heuristik     & 0,01\% &  4,98\% & 66,61\% & 58,38\% \\
Intelligente Heuristik & 0,01\% &  2,93\% & 73,37\% & 62,48\% \\ [1ex]
\hline
\end{tabular}
\label{table:neighbor_change_no_obstacles}
\end{table}

\begin{table}[ht]
\caption{Vergleich von Zielobjekt mit zufälliger Bewegung und einfacher Richtungsänderung (8 Agenten, zufälliges Szenario mit $\lambda_{h} = 0,1$, $\lambda_{p} = 0,99$)}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Sprünge Zielobjekt & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Sich zufällig bewegendes Zielobjekt \\ [1ex]
\hline
Zufällige Bewegung     & 0,01\% &  7,49\% & 66,63\% & 33,96\% \\
Einfache Heuristik     & 0,41\% & 11,51\% & 59,72\% & 79,99\% \\
Intelligente Heuristik & 0,36\% & 10,76\% & 65,87\% & 81,50\% \\ [1ex]
\hline
Zielobjekt mit einfacher Richtungsänderung \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% &  7,54\% & 68,31\% & 31,66\% \\
Einfache Heuristik     & 0,06\% &  8,68\% & 62,31\% & 57,95\% \\
Intelligente Heuristik & 0,08\% &  8,57\% & 68,28\% & 61,72\% \\ [1ex]
\hline
\end{tabular}
\label{table:neighbor_change_random}
\end{table}

\begin{table}[ht]
\caption{Vergleich von Zielobjekt mit zufälliger Bewegung und einfacher Richtungsänderung (8 Agenten, Säulenszenario)}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Sprünge Zielobjekt & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Sich zufällig bewegendes Zielobjekt \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% & 4,34\% & 72,27\% & 31,80\% \\
Einfache Heuristik     & 0,07\% & 8,77\% & 62,87\% & 78,34\% \\
Intelligente Heuristik & 0,04\% & 6,40\% & 69,98\% & 80,54\% \\ [1ex]
\hline
Zielobjekt mit einfacher Richtungsänderung \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% & 4,30\% & 72,28\% & 29,17\% \\
Einfache Heuristik     & 0,01\% & 6,29\% & 65,80\% & 56,19\% \\
Intelligente Heuristik & 0,01\% & 4,58\% & 72,44\% & 60,41\% \\ [1ex]
\hline
\end{tabular}
\label{table:neighbor_change_pillar}
\end{table}



\section{Auswirkung der Geschwindigkeit des Zielobjekts}

Angesichts der Ergebnisse in den zwei vorangegangenen Kapiteln, ist zu erwarten, dass die Geschwindigkeit des Zielobjekts bei der Qualität des Agenten mit zufälliger Bewegung keine Rolle spielt, da weder das Zielobjekt noch die Agenten Informationen über ihre Umgebung benutzen um sich für ein Verhalten zu entscheiden.

TODO subsections zusammenfassen

\subsection{Zielobjekt mit einfacher Richtungsänderung}\label{speed_single_direction:sec}

In Abbildung~\ref{speed_random_goal:fig} sind die Testergebnisse für einen Test mit 8 Agenten auf dem Säulenszenario dargestellt, bei dem sich das Zielobjekt mit einfacher Richtungsänderung bewegt. Es ist keine Korrelation zwischen der Geschwindigkeit und der Qualität des Algorithmus mit zufälliger Bewegung festzustellen, nur bei Geschwindigkeit 0 scheint es ein deutlich besseres Ergebnis zu geben. Das lässt sich aber durch die Anfangskonfiguration erklären, beim Säulenszenario startet das Zielobjekt in der Mitte mit maximalem Abstand zu den Hindernissen, ist also immer optimal in Sicht.\\
Der Algorithmus mit zufälliger Bewegung stellt also eine Untergrenze dar, ein Agent muss mehr als diesen Wert erreichen, damit man sagen kann, dass er etwas gelernt hat.\\

In Abbildung~\ref{speed_random_goal_heuristik:fig} sind dagegen die Testergebnisse (im selben Szenario) für die einfache und die intelligente Heuristik zu sehen. Im Wesentlichen sind drei Punkte anzumerken, erstens existiert eine Korrelation zwischen Qualität und Geschwindigkeit, zweitens gibt es einen Knick bei Geschwindigkeit 1 und drittens ist ein fast stetiger Anstieg der Differenz zwischen der einfachen und der intelligenten Heuristik zu verzeichnen. Der Knick lässt sich dadurch erklären, dass es ab dieser Geschwindigkeit möglich ist, dass das Zielobjekt Verfolger abschütteln kann, der Anstieg der Differenz lässt sich dadurch erklären, dass es Abdeckung des Gebiets eine immer größere Rolle spielt, als die Verfolgung des Zielobjekts.

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_random_goal.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit auf Agenten mit zufälliger Bewegung]{Auswirkung der Zielgeschwindigkeit auf Agenten mit zufälliger Bewegung, bis auf den Sonderfall bei 0 ist keine Korrelation zu entdecken TODO Szenario}
\label{speed_random_goal:fig}
\end{figure}

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_random_goal_heuristik.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit auf Agenten mit Heuristik]{Auswirkung der Zielgeschwindigkeit auf Agenten mit Heuristik}
\label{speed_random_goal_heuristik:fig}
\end{figure}


\subsection{Zielobjekt mit intelligenter Bewegung}\label{zielagent_analyse_intelligent:sec}

In Abbildung~\ref{speed_intelligent_goal:fig} und Abbildung~\ref{speed_intelligent_goal_obst2:fig} werden im Säulenszenario bzw. Szenario mit zufällig verteilten Hindernissen wieder die Heuristiken bei unterschiedlichen Geschwindigkeiten des Zielobjekts verglichen. Beim Säulenszenario ist wieder der Knick wie beim Fall mit Zielobjekt mit einfacher Richtungsänderung (siehe Kapitel~\ref{speed_single_direction:sec}) zu beobachten. Im Fall mit  TODO

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_intelligent_goal.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt, Säulenszenario) auf Agenten mit Heuristik]{Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt) auf Agenten mit Heuristik}
\label{speed_intelligent_goal:fig}
\end{figure}

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_intelligent_goal_obst2.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt, Szenario mit zufällig verteilten Hindernissen, $\lambda_{h}=0.2$, $\lambda_{p}=0.99$) auf Agenten mit Heuristik]{Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt, Szenario mit zufällig verteilten Hindernissen, $\lambda_{h}=0.2$, $\lambda_{p}=0.99$) auf Agenten mit Heuristik}
\label{speed_intelligent_goal_obst2:fig}
\end{figure}


Ein sich schnell bewegender, intelligenter Agent 

Dass dies doch nicht stimmt... TODO

\ref{table:intelligent_open_hide_no_obstacles}
\ref{table:intelligent_open_hide_random}
\ref{table:intelligent_open_hide_pillar}

TODO: Erläuterung!

Zu beachten sei, dass im Fall von ``Intelligent Hide'' eine relativ große Nummer an Sprüngen des Zielobjekts (siehe Kapitel~\ref{zielobjekt:sec}) stattgefunden hat, was die Ergebnisse etwas verzerrt, die Zahl hält sich aber noch in Grenzen (bis zu ca. 0.5\% im Fall der einfachen und intelligenten Heuristik im Fall mit vielen Hindernissen).

TODO neu, weg
\begin{table}[ht]
\caption{Vergleich von ``Intelligent Open'' und ``Intelligent Hide'' (8 Agenten, leeres Szenario ohne Hindernisse)}
\centering
\begin{tabular}{c c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent Open'' \\ [1ex]
\hline
Zufällige Bewegung     & 74,15\% & 11,32\% \\
Einfache Heuristik     & 60,90\% & 82,86\% \\
Intelligente Heuristik & 69,62\% & 85,74\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_no_obstacles}
\end{table}

\begin{table}[ht]
\caption{Vergleich von ``Intelligent Open'' und ``Intelligent Hide'' (8 Agenten, zufälliges Szenario mit $\lambda_{h} = 0,2$, $\lambda_{p} = 0,99$)}
\centering
\begin{tabular}{c c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent Open'' \\ [1ex]
\hline
Zufällige Bewegung     & 62,54\% & 13,37\% \\
Einfache Heuristik     & 52,23\% & 84,33\% \\
Intelligente Heuristik & 56,92\% & 85,12\% \\ [1ex]
\hline
``Intelligent Hide'' \\ [1ex]
\hline
Zufällige Bewegung     & 62,52\% & 13,10\% \\
Einfache Heuristik     & 50,17\% & 90,32\% \\
Intelligente Heuristik & 56,94\% & 90,45\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_random}
\end{table}

\begin{table}[ht]
\caption{Vergleich von ``Intelligent (Open)'' und ``Intelligent (Hide)'' (8 Agenten, Säulenszenario)}
\centering
\begin{tabular}{c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent (Open)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72,55\% & 11,58\% \\
Einfache Heuristik     & 57,19\% & 85,58\% \\
Intelligente Heuristik & 64,26\% & 91,18\% \\ [1ex]
\hline
``Intelligent (Hide)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72,56\% & 11,78\% \\
Einfache Heuristik     & 58,45\% & 80,98\% \\
Intelligente Heuristik & 65,65\% & 86,38\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_pillar}
\end{table}


\subsection{Schwieriges Szenario}\label{test_schwieriges_szenario:sec}

Für das sogenannte schwierige Szenario aus Kapitel~\ref{difficult_scenario:sec} erscheint nur der in Kapitel~\ref{no_direction_change:sec} vorgestellte Typ von Zielobjekt mit Beibehaltung der Richtung sinnvoll, da das Ziel für die Agenten sein soll, bis in den letzten Abschnitt vorzudringen und dem Zielobjekt nicht schon auf halbem Weg zu begegnen.\\
Für verschiedene Anzahl von Schritten sind für die drei Agententypen in Abbildung~\ref{steps_direction_difficult_heuristics:fig} die jeweiligen Qualitäten aufgeführt. Wie man beim Vergleich zwischen zufälliger Bewegung und einfacher Heuristik sehen kann, ist es nicht nur entscheidend, in den letzten Bereich am rechten Rand des Szenarios vorzudringen, sondern auch, dort den Agenten zu verfolgen und in diesem Bereich zu bleiben. Deutlich zeigen sich hier die Vorzüge der intelligenten Heuristik, durch das Bestreben, Agenten auszuweichen, hat es dieser Algorithmus leichter, durch die Öffnungen in von Agenten unbesetzte Bereiche vorzudringen. Der Unterschied zwischen einfacher und intelligenter Heuristik zeigt auch, dass in diesem Szenario ein deutlich größeres Lernpotential, was die Einbeziehung von wahrgenommenen Agentenpositionen betrifft, für Agenten besteht. Wie später in Kapitel~\ref{xcs_difficult_scenario:sec} gezeigt wird, können in diesem Szenario unter anderem deshalb auf XCS basierte Agenten ihre Vorteile besonders gut ausspielen und erreichen sogar bessere Ergebnisse als die intelligente Heuristik.

\begin{figure}[htbp]
\centerline{	
\includegraphics{steps_direction_difficult_heuristics.eps}
}
\caption[Auswirkung der Anzahl der Schritte (schwieriges Szenario, Geschwindigkeit 2, ohne Richtungsänderung) auf Qualität von Agenten mit Heuristik]{Auswirkung der Anzahl der Schritte (schwieriges Szenario, Geschwindigkeit 2, ohne Richtungsänderung) auf Qualität von Agenten mit Heuristik}
\label{steps_direction_difficult_heuristics:fig}
\end{figure}

\section{Zusammenfassung}

Wie man sehen konnte, existieren also Szenarien in denen Abdeckung kaum eine Rolle spielt und lokale Entscheidungen eine wesentliche Rolle spielen. Dies wird es erleichtern, geeignete Szenarien im Kapitel~\ref{communication:cha}  zu finden.

TODO
