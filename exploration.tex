\section{Auswahlart der \emph{classifier}}\label{auswahlart:sec}

In der Standardimplementierung von XCS wird in jeder Probleminstanz entweder zufällig oder in jedem Schritt anhand eines Parameters zwischen der \emph{explore} und \emph{exploit} Phase gewechselt. Da dies im Überwachungsszenario in dieser Art nicht möglich ist, werden in diesem Kapitel mehrere Auswahlarten und insbesondere der Wechsel zwischen den Auswahlarten vorgestellt und diskutiert.\\

Eine Auswahl von \emph{classifier} ist notwendig, da ein Sensordatensatz von mehreren \emph{classifier} erkannt werden kann und in jedem Schritt somit mehrere passende \emph{classifier} samt Aktionen ausgewählt werden können (siehe Kapitel~\ref{platzhalter:sec}). Deshalb stellt sich die Frage, welche der Aktionen im jeweiligen Schritt ausgeführt werden soll. Als Basis der Entscheidung hat ein Agent zum einen die Sensordaten und zum anderen die eigene \emph{classifier set} Liste zur Verfügung.\\

In XCS wird dazu die zur jeweiligen Sensordatensatz passenden \emph{match set} Liste in vier (Anzahl der möglichen Aktionen) Gruppen entsprechend des \emph{action} Werts des jeweiligen \emph{classifier} aufgeteilt. Danach werden alle Produkte aus den \emph{fitness} und \emph{reward prediction} Werten der \emph{classifier} aus der jeweiligen Gruppe aufaddiert und durch die Summe der \emph{fitness} Werte der \emph{classifier} der jeweiligen Gruppe geteilt. Dieser Wert wird im Folgenden \emph{predictionFitnessProductSum} genannt.\\

In der ursprünglichen Implementierung \cite{Butz_xcsclassifier} werden folgende Arten beschrieben, wie eine der vier Aktionen ausgewählt werden kann:\\

\begin{figure}[H]
\setbox0\vbox{\small
\begin{description}
\item[\emph{random selection}] Zufällige Auswahl einer Aktion (siehe Kapitel~\ref{zufaellige_auswahlart:sec})
\item[\emph{best selection}] Auswahl der Aktion mit dem höchsten \emph{predictionFitnessProductSum} Wert der jeweiligen Gruppe (siehe Kapitel~\ref{best_selection_auswahlart:sec})
\item[\emph{roulette wheel selection}] Zufällige Auswahl einer Aktion, Wahrscheinlichkeit abhängig vom \emph{predictionFitnessProductSum} Wert der jeweiligen Gruppe (siehe Kapitel~\ref{auswahlart_roulette:sec})
\end{description}
}
\centerline{\fbox{\box0}}
\end{figure}

Im Folgenden werden nun diese Auswahlarten vorgestellt sowie eine weitere Auswahlart, die Auswahlart \emph{tournament selection} (siehe Kapitel~\ref{tournament_selection:sec}), aus der Literatur besprochen. Im nächsten Abschnitt (Kapitel~\ref{exploreexploit:sec}) wird dann der Wechsel zwischen diesen Auswahlarten untersucht. Neben der einfachen Auswahlart \emph{best selection} scheint insbesondere der dynamische Wechsel zwischen den Auswahlarten erfolgsversprechend zu sein.\\


\subsection{Auswahlart \emph{random selection}}\label{zufaellige_auswahlart:sec}

Prägende Idee für diese Auswahlart in einem statischen Szenario ist, das XCS möglichst vielen verschiedenen Situationen auszusetzen. Da in einem statischen Szenario die Zielposition sowie die Hindernisse fest sind, ist es wichtig, durch \emph{random selection} dem XCS einen gewissen Spielraum zu geben.\\

Im Vergleich zu standardmäßig statischen Szenario ist dagegen es bei einem dynamischen Überwachungsszenario (siehe Kapitel~\ref{dynamisch_kollaborativ:sec}) weder nötig noch hilfreich, die Auswahlart \emph{random selection} zu nutzen, da

\begin{itemize}
\item zum einen, aufgrund ständiger Bewegung anderer Agenten und des Zielobjekts und fehlendem Neustart beim Erreichen des Ziels, das Problem dynamisch ist und die Agenten mit vielen verschiedenen Situationen konfrontiert werden und

\item zum anderen, da es für ein erfolgreiches Bestehen in einem Überwachungsszenario wichtig ist, dass ein Agent über eine längere Zeit hinweg eine hohe Qualität liefert, also stetig gute Entscheidungen trifft.
\end{itemize}

Eine zufällige Auswahl scheint also wenig zielführend zu sein, weshalb im folgenden Auswahlarten besprochen werden, die auf die bisherige Qualität der in Frage kommenden \emph{classifier} eingehen.


\subsection{Auswahlart \emph{best selection}}\label{best_selection_auswahlart:sec}

Bei der Auswahlart~\emph{best selection} wird nur die Aktion mit dem höchsten \emph{predictionFitnessProductSum} Wert ausgewählt. Die Verwendung dieser Auswahlart kann u.U. zu langen Folgen gleicher Aktionen führen, sofern sich die Umwelt nicht ändert. Beispielsweise würde ein Agent damit in einer Sackgasse andauernd gegen eine Wand laufen.\\

Zur Verfolgung von einem Zielobjekt erscheint ein kompromissloses Verhalten sinnvoll, wie die Ergebnisse der Heuristiken zeigen (siehe beispielsweise die Qualitäten in Tabelle~\ref{table:neighbor_change_random} der Fall mit einfacher Richtungsänderung). Allerdings wechseln hierbei die Heuristiken zwischen zwei verschiedenen Phasen. Ist das Zielobjekt nicht in Sicht, bewegen sie sich  zufällig auf dem Torus. Gerade bei Szenarien mit Hindernissen kann ein solcher Wechsel sinnvoll sein, um die oben genannte Gefahr der großen Anzahl an blockierten Bewegungen zu vermeiden.\\

Auch sind die in dieser Arbeit verwendeten Sensorfähigkeiten beschränkt, der Agent weiß nicht, wo genau sich das Zielobjekt befindet, selbst wenn es in Sicht ist. Dementsprechend könnte es sein, dass eine optimale Verhaltensstrategie Entscheidungen eher auf Basis von Wahrscheinlichkeitsverteilungen treffen sollte.\\


\subsection{Auswahlart \emph{roulette wheel selection}}\label{auswahlart_roulette:sec}

Bei dieser Auswahlart bestimmt der \emph{predictionFitnessProductSum} Wert, relativ zu den anderen \emph{predictionFitnessProductSum} Werten, die Wahrscheinlichkeit, ausgewählt zu werden. Wie die Auswahlart \emph{best selection} erscheint diese Auswahlart also grundsätzlich sinnvoll, da sie anhand bisheriger Erfahrungen des Agenten versucht, Entscheidungen zu treffen.\\

Allerdings kann eine auf Proportionen basierte Auswahlart wie \emph{roulette wheel selection} problematisch sein, wenn sich die \emph{predictionFitnessProductSum} Werte einander ähneln und somit die Aktionen mit nahezu gleicher Wahrscheinlichkeit gewählt werden. Dies kann aufgrund der stochastischen Natur und der unscharfen Sensorinformationen gegeben sein, weshalb diese Auswahlart eher der Auswahlart \emph{random selection} als \emph{best selection} ähnelt.\\


\subsection{Auswahlart \emph{tournament selection}}\label{tournament_selection:sec}

Zusätzlich zu den oben erwähnten drei Möglichkeiten wurde in \cite{Butz2003} eine weitere Auswahlart vorgestellt und in Bezug auf XCS diskutiert. Sie wurde dort mit \emph{tournament selection} bezeichnet. Als Vorteile gegenüber den herkömmlichen Auswahlarten werden u.a. geringerer Selektionsdruck, höhere Effizienz und geringerer Einfluss von Störungen genannt. Durch die Anpassung der Turniergröße ergibt sich außerdem eine flexible Anpassungsmöglichkeit.\\

In den dort vorgestellten Experimenten mit einem \emph{single step} Problem wurden signifikante Vorteile dieser, auf proportionaler Selektion beruhender, Auswahlart gefunden. Daher soll sie auch im Zusammenhang mit Überwachungsszenarien getestet werden.\\

Allerdings findet in der Literatur die Auswahl auf Basis von einzelnen \emph{classifier} statt. Dagegen wird in dieser Arbeit wie auch in der Standardimplementierung von XCS in \cite{Butz_xcsclassifier} alle \emph{classifier} in nach \emph{action} Werten eingeteilten Gruppen sortiert und deren \emph{reward prediction} und \emph{fitness} Werte zusammengenommen. Deshalb wird hier eine Implementierung der Auswahlart \emph{tournament selection} gewählt, die näher am ursprünglichen Algorithmus aus dem Bereich der genetischen Algorithmen liegt \cite{Miller95geneticalgorithms}.\\

\begin{figure}[H]
\setbox0\vbox{\small

Charakteristisch für diese Auswahlart ist, dass 
\begin{enumerate}
\item \(k\) Elemente aus einer Menge zufällig ausgewählt werden,
\item nach ihrem zugehörigen Wert sortiert werden und
\item absteigend mit Wahrscheinlichkeit \(p\) das jeweilige Element gewählt wird.\\

\(\Rightarrow\) Das beste Element wird mit Wahrscheinlichkeit \(p\), das zweitbeste mit Wahrscheinlichkeit \((1-p)p\), das drittbeste mit Wahrscheinlichkeit \((1-p)^{2}p\) und das letzte (bei $k=4$) mit Wahrscheinlichkeit \((1-p)^3\) gewählt.
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}


In dem hier besprochenen Fall enthalten die Mengen immer vier (Anzahl der Aktionen) Elemente. Diese entsprechen jeweils den berechneten \emph{predictionFitnessProductSum} Werten. Zur Vereinfachung der Tests wird \(k\) auf den Maximalwert gesetzt, damit alle Aktionen zumindest eine geringe Wahrscheinlichkeit besitzen, ausgewählt zu werden.\\

Im Grunde ist diese Auswahlart deckungsgleich mit der \emph{roulette wheel selection}, allerdings ohne dem Problem, dass die Auswahlwahrscheinlichkeit aufgrund ähnlicher Produkte sich ebenfalls ähneln. Außerdem ist die Darstellung selbst sehr flexibel, beispielsweise wäre \emph{tournament selection} mit \(p = 1,0\) und \(k = 4\) identisch mit \emph{best selection} und mit \(p = 1,0\) und \(k = 1\) wäre es identisch mit \emph{random selection}. Diese Form der Auswahl, bei geeigneter Wahl von \(k\) und \(p\), scheint also sehr vielversprechend zu sein.\\

Wie in Abbildung~\ref{darstellung_tournament:fig} zu sehen, wird für Werte \(p < 0,5\) das schlechteste Element nicht mehr mit geringster Wahrscheinlichkeit gewählt. Außerdem verhält sich die Auswahlart im Bereich zwischen \(0,2\) und \(0,4\) ähnlich der Auswahlart \emph{random selection}.  Die Verwendung der Auswahlart \emph{tournament selection} soll sich hier deshalb auf Werte \(p \geq 0,6\) beschränken.\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{darstellung_tournament.eps}
}
\caption[Darstellung der Auswahlfunktion der Auswahlart \emph{tournament selection}]{Darstellung der Auswahlfunktion der Auswahlart \emph{tournament selection}}
\label{darstellung_tournament:fig}
\end{figure}


Bei der Implementierung ist darauf zu achten, dass bei der Sortierung Einträge mit gleichem Produkt aus den \emph{fitness} und \emph{reward prediction} Werten in zufälliger Reihenfolge aufgeführt werden. Andernfalls würden insbesondere am Anfang alle Agenten in dieselbe Richtung laufen, da alle \emph{predictionFitnessProductSum} Werte identisch sind.\\


\section{Abwechselnde \emph{explore}/\emph{exploit} Phasen}\label{exploreexploit:sec}

In der Standardimplementierung von XCS wird zwischen verschiedenen Auswahlarten gewechselt. Die Auswahlarten werden hierzu in zwei Gruppen geteilt, in die  \emph{explore} Phase und in die \emph{exploit} Phase.\\

In der \emph{exploit} Phase soll der jeweilige \emph{predictionFitnessProductSum} Wert stärker als in der \emph{explore} Phase gewichtet werden. Beispielsweise könnte man für die \emph{exploit} Phase die Auswahlart \emph{best selection} und für die \emph{explore} Phase die Auswahlart \emph{roulette wheel selection} festsetzen.\\

Wesentlicher Leitgedanke ist es, mit Hilfe der \emph{explore} Phasen den Suchraum besser erforschen zu können, dann aber zur eigentlichen Problemlösung in der \emph{exploit} Phase möglichst direkt auf das Ziel zuzugehen um \emph{classifier} stärker zu belohnen, die am kürzesten Weg beteiligt sind.\\

Die Wahl der Auswahlart in Kapitel~\ref{ablauf_lcs:sec} für \emph{classifier} in Punkt (3) kann auf verschiedene Weise erfolgen. In der Standardimplementierung von XCS wird nach jedem Erreichen des Ziels zwischen \emph{exploit} und \emph{explore} entweder umgeschalten oder zufällig mit einer bestimmten Wahrscheinlichkeit eine Auswahlart ermittelt. Es werden also abwechselnd ganze Probleminstanzen entweder nur in der \emph{exploit} oder nur in der \emph{explore} Phase berechnet. Dies erscheint sinnvoll für die erwähnten Standardprobleme, da nach Erreichen des Ziels eine neue Probleminstanz gestartet wird und die Entscheidungen, die während der Lösung einer Probleminstanz getroffen wurden, keine externen Auswirkungen auf die folgenden Probleminstanzen haben, die Probleminstanzen also nicht miteinander zusammenhängen.\\

Bei dem hier vorgestellten Überwachungsszenario kann dagegen nicht neu gestartet werden, es gibt keine "`Trockenübung"'. Die Qualität eines Algorithmus soll deshalb davon abhängen, wie gut sich der Algorithmus während der gesamten Berechnung, inklusive der Lernphasen, verhält. Es ist nicht möglich bei diesem Szenario zwischen \emph{exploit} und \emph{explore} Phasen in dem Sinne zu differenzieren, wie dies bei XCS geschieht. Dort wird die Qualität nur während der \emph{exploit} Phase gemessen.\\

Desweiteren greift auch die Implementierung der Idee einer reinen \emph{explore} Phase beim Überwachungsszenario nicht, da das Szenario nicht statisch, sondern dynamisch ist. Ein zufälliges Herumlaufen kann, im Vergleich zur gewichteten Auswahl der Aktionen, dazu führen, dass der Agent mit bestimmten Situationen mit deutlich niedrigerer Wahrscheinlichkeit konfrontiert wird, da der Agent sich in Hindernissen verfängt oder das Zielobjekt (z.B. mit "`Intelligentem Verhalten"' aus Kapitel~\ref{zielobjekt_intelligentes_verhalten:sec}) ihm andauernd ausweicht. Daher erscheint es sinnvoll, weitere Formen des Wechsels zwischen diesen Phasen zu untersuchen.\\

Bei der Standardimplementierung für den statischen Fall ist allerdings das Erreichen eines positiven \emph{base reward} Werts äquivalent mit einem Neustart der Probleminstanz. Während dort das gesamte Szenario (alle Agenten, Hindernisse und das Zielobjekt) auf den Startzustand zurückgesetzt werden, läuft das Überwachungsszenario weiter.\\

Als erweiterten Ansatz wird deshalb eine neue Problemdefinition gelten, bei der nicht das Erreichen eines positiven \emph{base reward} Werts (also ein Neustart der Probleminstanz) einen Phasenwechsel auslöst, sondern stattdessen eine \emph{Änderung} des \emph{base reward} Werts ausschlaggebend ist und einen Wechsel zwischen der \emph{explore} und \emph{exploit} Phase auslöst. Bei einer anfänglichen \emph{explore} Phase würde dann immer in die \emph{exploit} Phase gewechselt werden, wenn das Zielobjekt in Sicht ist bzw. wenn keine Agenten in Sicht sind.\\

\begin{figure}[H]
\setbox0\vbox{\small
Es werden nun also folgende Varianten des Wechsels zwischen den Phasen näher betrachtet:
\begin{enumerate}
\item Andauernde \emph{explore} Phase;
\item andauernde \emph{exploit} Phase;
\item abwechselnd \emph{explore} und \emph{exploit} Phase (bei Änderung des \emph{base reward}, beginnend mit \emph{explore}) sowie
\item in jedem Schritt zufällig entweder \emph{explore} oder \emph{exploit} Phase (50\% Wahrscheinlichkeit jeweils)
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}

Hervorzuheben ist, dass die Varianten 3 und 4 angewendet auf die nicht an Überwachungsszenarien angepasste Standardimplementierung des \emph{multi step} XCS Verfahrens keinen Unterschied machen würden. Dies liegt daran, dass beim Erreichen eines positiven \emph{base reward} Werts sowieso eine neue Probleminstanz gestartet wird, die \emph{explore} und \emph{exploit} Phasen separat betrachtet werden können und zwischen den Probleminstanzen zwischen der \emph{exploit} und \emph{explore} Phase gewechselt wird. Variante (4.) wird in der Arbeit nicht weiter beachtet, da kein Grund erkennbar ist, weshalb ein andauernder, zufälliger Wechsel einen Vorteil erbringen könnte.\\



