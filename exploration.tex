\section{Auswahlart der \emph{classifier}}\label{auswahlart:sec}

In jedem Zeitschritt gilt es zu entscheiden, welche Bewegung ein Agent ausführen soll. Als Basis der Entscheidung hat ein Agent zum einen die Sensordaten und zum anderen das eigene \emph{classifier set} zur Verfügung. Da ein Sensordatensatz von mehreren \emph{classifier} erkannt werden kann (siehe Kapitel~\ref{platzhalter:sec}), stellt sich die Frage, welchen \emph{classifier} (und den dazugehörigen \emph{action} Wert, der die Bewegung bestimmt) man aus der gebildeten \emph{match set} Liste auswählen soll. In der ursprünglichen Implementierung \cite{Butz_xcsclassifier} wurden folgende Auswahlarten benutzt:

\begin{enumerate}
\item \emph{random selection} : Zufällige Auswahl eines \emph{classifiers}
\item \emph{roulette wheel selection} : Zufällige Auswahl eines \emph{classifier}, mit Wahrscheinlichkeit abhängig vom Produkt seines \emph{fitness} und \emph{reward prediction} Werts
\item \emph{best selection} : Auswahl des \emph{classifiers} mit dem höchsten Produkt aus seinen \emph{fitness} und \emph{reward prediction} Werten
\end{enumerate}

Im Folgenden sollen diese Auswahlarten näher vorgestellt und außerdem noch eine weitere Auswahlart aus der Literatur besprochen werden. Im nächsten Abschnitt (Kapitel~\ref{exploreexploit:sec}) soll dann der Wechsel zwischen diesen Auswahlarten näher untersucht und die tatsächlichen Testergebnisse zwischen den vorgestellten Varianten präsentiert werden.


\subsection{Auswahlart \emph{random selection}}

Bei einem dynamischen Überwachungsszenario ist es im Vergleich zu standardmäßigen statischen Szenarien weder nötig noch hilfreich \emph{random selection} zu nutzen. Die Idee für diese Auswahlart in einem statischen Szenario ist, dass man möchte, dass das XCS möglichst vielen verschiedenen Situationen ausgesetzt ist. Da in einem statischen Szenario Start- und Zielposition wie auch die Hindernisse fest sind, ist es wichtig, durch \emph{random selection} dem XCS einen gewissen Spielraum zu geben.\\
Bei einem dynamischen Szenario ergibt sich dieses Problem nicht, andere Agenten und das Zielobjekt sind in stetiger Bewegung, der eigene Startpunkt ist nicht fixiert und das Problem wird bei Erreichen des Ziels nicht neugestartet. Aufgrund der Natur der Aufgabenstellung ist es in einem Überwachungsszenario außerdem wichtig, dass das XCS über eine längere Zeit hinweg eine gute Leistung liefert, also stetig gute Entscheidungen trifft, eine zufällige Auswahl scheint also wenig zielführend zu sein.\\


\subsection{Auswahlart \emph{best selection}}

Bei der Auswahlart~\emph{best selection} wird einfach nur der \emph{classifier} ausgewählt, bei dem das Produkt aus \emph{fitness} und \emph{reward prediction} am Größten ist. Die Verwendung dieser Auswahlart kann u.U. schnell in eine Sackgasse bzw. zu langen Folgen gleicher Aktionen (beispielsweise andauernd gegen eine Wand laufen) führen, sofern sich die Umwelt nicht ändert. Auf den ersten Blick scheint es zwar, dass z.B. zur Verfolgung von einem Zielobjekt ein kompromissloses Verhalten sinnvoll ist, jedoch bedarf dies zum einen bereits guter, gelernter \emph{classifier} und zum anderen vollständige Information. In dem in dieser Arbeit betrachteten Szenario sind die Sensordaten allerdings beschränkt, der Agent weiß nicht genau, wo sich das Zielobjekt befindet, selbst wenn es in Sicht ist. Eine optimale Verhaltensstrategie muss hier also auf Fuzzy Logik, also auf Wahrscheinlichkeiten basierenden Entscheidungen basieren, weshalb die alleinige Verwendung der Auswahlart \emph{best selection} eher nicht in Frage kommt.\\


\subsection{Auswahlart \emph{roulette wheel selection}}

Bei dieser Auswahlart bestimmt das Produkt aus \emph{fitness} und \emph{reward prediction} eines \emph{classifier} relativ zur Summe der Produkte aller \emph{classifier} das Verhältnis der Wahrscheinlichkeit, ausgewählt zu werden. Diese Auswahlart erscheint sinnvoll, jedoch wurde in Kapitel~\ref{prediction_init:sec} dargelegt, dass sich die \emph{reward prediction} Werte der \emph{classifier} ähneln. Eine auf Proportionen ausgelegte Auswahlart wie \emph{roulette wheel selection} kann deshalb dazu führen, dass es kaum Unterschiede in den Auswahlwahrscheinlichkeiten gibt, mit der ein \emph{classifier} ausgewählt wird. Diese Auswahlart ähnelt somit eher der Auswahlart \emph{random selection} als \emph{best selection}.


\subsection{Auswahlart \emph{tournament selection}}\label{tournament_selection:sec}

Zu den oben erwähnten drei Möglichkeiten wurde in \cite{Butz2003} eine weitere vorgestellt und in Bezug auf XCS diskutiert, die sogenannte \emph{tournament selection}. Als Vorteile werden geringerer Selektionsdruck, höhere Effizienz, geringerer Einfluss von Störungen, wie auch Flexibilität der Anpassung über zwei Parameter, \(k\) und \(p\), genannt. Dabei werden \(k\) \emph{classifier} aus der \emph{match set} Liste zufällig ausgewählt, nach ihrem Produkt aus den jeweiligen \emph{fitness} und \emph{reward prediction} Werten sortiert und absteigend mit Wahrscheinlichkeit \(p\) der jeweilige \emph{classifier} ausgewählt (d.h. der erste mit \(p\), der zweite mit \((1,0-p)*p\), der dritte mit \((1,0-p)^{2}*p\) usw.). Mit \(p = 1,0\) und \(k = n\) (wobei \(n\) der Größe der \emph{match set} Liste entspricht) wäre \emph{tournament selection} identisch mit \emph{best selection} und mit \(k = 1\) wäre es identisch mit \emph{random selection}. In diesem Zusammenhang soll \(k = \) \(<\)Größe der jeweiligen \emph{action set} Liste\(>\) sein, für \(p\) scheint nach Abbildung~\ref{test_tournament_selectionp:fig} \(0,8\) ein passender Wert zu sein.\\

Im Grunde entspricht diese Auswahlart also der \emph{roulette wheel selection}, allerdings ohne dem Problem, dass die Auswahlwahrscheinlichkeit aufgrund ähnlicher Produkte sich ebenfalls ähneln. Diese Form der Auswahl, bei geeigneter Wahl von \(k\) und \(p\), scheint also am vielversprechend zu sein.\\

Bei der Implementierung dieser Auswahlart muss man aufpassen, dass bei der Sortierung Einträge mit gleichem Produkt aus \emph{fitness} und \emph{reward prediction} in zufälliger Reihenfolge aufgeführt werden. Insbesondere am Anfang kann es sonst dazu kommen, dass alle Agenten in die selbe Richtung laufen.\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{test_tournament_selectionp.eps}
}
\caption[Vergleich verschiedener Werte $p$ für Auswahlart \emph{tournament selection}]{Vergleich verschiedener Werte $p$ für Auswahlart \emph{tournament selection} (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario, 8 Agenten, SXCS)}
\label{test_tournament_selectionp:fig}
\end{figure}

\section{Wechsel zwischen den \emph{explore} und \emph{exploit} Phasen}\label{exploreexploit:sec}

In der Standardimplementierung von XCS wird zwischen verschiedenen Auswahlarten hin und her geschalten. Die Auswahlarten werden in zwei Gruppen geteilt, in die sogenannte \emph{explore} Phase und in die \emph{exploit} Phase. In der \emph{exploit} Phase soll bevorzugt eine Auswahlart ausgeführt werden, die das Produkt aus den Werten \emph{fitness} und \emph{reward prediction} möglichst stark gewichtet, \emph{best selection} und \emph{tournament selection} sind Kandidaten für die \emph{exploit} Phase, während \emph{random selection} und \emph{roulette wheel selection} Kandidaten für die \emph{explore} Phase wären. Idee ist, dass man mit Hilfe der \emph{explore} Phasen den Suchraum besser erforschen kann, dann aber zur eigentlichen Problemlösung in der \emph{exploit} Phase möglichst direkt auf das Ziel zugeht um \emph{classifier} stärker zu belohnen, die am kürzesten Weg beteiligt sind.\\

Die Wahl der Auswahlart in Kapitel~\ref{ablauf_lcs:sec} für \emph{classifier} in Punkt (3) kann auf verschiedene Weise erfolgen. In der Standardimplementierung von XCS wird zwischen \emph{exploit} und \emph{explore} nach jedem Erreichen des Ziels entweder umgeschalten oder zufällig mit einer bestimmten Wahrscheinlichkeit eine Auswahlart ermittelt. Es werden also abwechselnd ganze Probleme entweder im \emph{exploit} oder im \emph{explore} Modus berechnet. Dies erscheint sinnvoll für die erwähnten Standardprobleme, da nach Erreichen des Ziels ein neues Problem gestartet wird und die Entscheidungen die während der Lösung eines Problems getroffen werden keine Auswirkungen auf die folgenden Probleme hat, die Probleme also nicht miteinander zusammenhängen.\\

Bei dem hier vorgestellten Überwachungsszenario kann dagegen nicht neugestartet werden, es gibt keine "`Trockenübung"', die Qualität eines Algorithmus soll deshalb davon abhängen, wie gut sich der Algorithmus während der gesamten Berechnung, inklusive der Lernphasen, verhält. Es ist nicht möglich bei diesem Szenario zwischen \emph{exploit} und \emph{explore} Phasen in dem Sinne zu differenzieren, wie dies in den Standardszenarien bei XCS der Fall ist, bei denen u.a. die Qualität nur während der \emph{exploit} Phase gemessen wird.\\

Desweiteren greift auch die Idee einer reinen \emph{explore} Phase beim Überwachungsszenario nicht, da das Szenario nicht statisch, sondern dynamisch ist. Ein zufälliges Herumlaufen kann, im Vergleich zur gewichteten Auswahl der Aktionen, dazu führen, dass der Agent mit bestimmten Situationen mit deutlich niedrigerer Wahrscheinlichkeit konfrontiert wird, da der Agent sich in Hindernissen verfängt oder das Zielobjekt (z.B. mit "`Intelligentem Verhalten"' aus Kapitel~\ref{zielobjekt_intelligentes_verhalten:sec}) ihm andauernd ausweicht. Aus diesen Gründen erscheint es sinnvoll, weitere Formen des Wechsels zwischen diesen Phasen zu untersuchen.\\

Bei der Standardimplementierung für den statischen Fall ist allerdings das Erreichen eines positiven \emph{base reward} äquivalent mit einem Neustart des Problems. Während dort beim Neustart des Problems das gesamte Szenario (alle Agenten, Hindernisse und das Zielobjekt) auf den Startzustand zurückgesetzt werden, läuft das Überwachungsszenario weiter. Als erweiterten Ansatz soll nun deshalb eine neue Problemdefinition gelten, dass nicht das Erreichen eines positiven \emph{base rewards} (also ein Neustart des Problems) einen Phasenwechsel auslöst, sondern eine \emph{Änderung} des \emph{base rewards}, so dass mit anfänglicher \emph{explore} Phase immer dann in die \emph{exploit} Phase gewechselt wird, wenn das Zielobjekt in Sicht ist (bzw. umgekehrt, wenn mit der \emph{exploit} Phase begonnen wird). Als Vergleich soll der andauernde, zufällige Wechsel zwischen der \emph{explore} und \emph{exploit} Phase, eine andauernde \emph{exploit} und andauernde \emph{explore} Phase dienen. Es sollen nun also folgende Arten des Wechsel zwischen den Phasen untersucht werden:

\begin{enumerate}
\item Andauernde \emph{explore} Phase
\item Andauernde \emph{exploit} Phase
\item Abwechselnd \emph{explore} und \emph{exploit} Phase (bei Änderung des \emph{base reward}, beginnend mit \emph{explore})
\item Abwechselnd \emph{explore} und \emph{exploit} Phase (bei Änderung des \emph{base reward}, beginnend mit \emph{exploit})
\item In jedem Schritt zufällig entweder \emph{explore} oder \emph{exploit} Phase (50\% Wahrscheinlichkeit jeweils)
\end{enumerate}

Anzumerken sei hier, dass Punkt (3.), (4.) und (5.) in der Standardimplementierung praktisch äquivalent sind, da die Phasen separat betrachtet werden können. TODO evtl
entspricht dem Fall in der Standardimplementierung von XCS. Dabei wird bei jedem Erreichen eines positiven \emph{reward} zwischen \emph{explore} und \emph{exploit} hin und hergeschaltet, was in der Standardimplementierung dem Beginn eines neuen Problems entspricht.


\section{Test der verschiedenen Auswahlarten}\label{test_auswahlarten:sec}

In Tabelle~\ref{table:auswahlarten_vergleich_direction} kann man die bisherigen Vermutungen sehr gut erkennen. Die Auswahlarten \emph{random selection} und \emph{roulette wheel selection}  sind für sich alleine kaum brauchbar, das Ergebnis ist nicht besser als des des sich zufällig bewegenden Agenten. Die Auswahlart \emph{best selection} sorgt gar für über 40\% blockierte Bewegungen und einer deutlich schlechteren Abdeckung. Für die \emph{exploit} Phase scheint nur \emph{tournament selection} deutlich bessere Ergebnisse zu liefern, wenn auch mit relativ hoher Zahl blockierter Bewegungen. Da die \emph{roulette wheel} Auswahlart etwas bessere Ergebnisse liefert, soll sie für die \emph{explore} Phase benutzt werden.\\
Für den Wechsel zwischen der \emph{explore} und \emph{exploit} Phase sieht man bei zufälligem Wechsel, dass die statistischen Werte zwischen denen der \emph{roulette wheel} und \emph{tournament selection} Auswahlart liegen, stellt angesichts der minimalen Steigerung zur Qualität von der \emph{roulette wheel} Auswahlart also kein signifikante Verbesserung dar. Wechselt man bei einer Änderung des \emph{base reward} Werts und startet in der \emph{explore} Phase ergibt sich ein deutlich schlechteres Ergebnis, der Algorithmus scheint sich also genau falsch zu verhalten. Umgekehrt, startet man in der \emph{exploit} Phase, ergibt sich dagegen ein deutlich besseres Ergebnis.\\
Insgesamt soll also im Weiteren die \emph{tournament selection} und der Wechsel zwischen \emph{tournament selection} und \emph{roulette wheel selection} als Auswahlart benutzt werden.

\begin{table}[ht]
\caption{Vergleich der verschiedenen Auswahlarten (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario, Geschwindigkeit 1, 8 Agenten mit SXCS Algorithmus)}
\centering
\begin{tabular}{c c c c}
\hline\hline
Auswahlart & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Agent mit zufälliger Bewegung                      &  4,46\% & 72,12\% & 29,05\% \\[1ex]
\hline
\emph{roulette wheel selection} &  4,54\% & 72,10\% & 30,30\% \\
\emph{random selection}         &  4,34\% & 72,21\% & 28,50\% \\
\bf{\emph{tournament selection}}     & 11,21\% & 70,20\% & \bf{33,39\%} \\
\emph{best selection}           & 41,16\% & 63,64\% & 29,22\% \\
Zufällig \emph{explore}/\emph{exploit}             &  6,29\% & 71,18\% & 30,58\% \\
Abwechselnd, zuerst \emph{explore}          &  5,63\% & 71,37\% & 26,30\% \\
\bf{Abwechselnd, zuerst \emph{exploit}}          &  9,28\% & 70,40\% & \bf{35,36\%} \\[1ex]
\hline
\end{tabular}
\label{table:auswahlarten_vergleich_direction}
\end{table}
