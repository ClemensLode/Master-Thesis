\chapter{Ablauf der Simulation}

\section{Hauptschleife}


In Programm~\ref{mainExperiment:fig}

TODO9



\begin{program}
  \begin{verbatim}
/**
 * Führt eine Anzahl von Problemen aus
 * @param experiment_nr Nummer des auszuführenden Experiments
 */
  public void doOneMultiStepExperiment(int experiment_nr) {
    int currentTimestep = 0;

  /**
   * number of problems for the same population
   */
    for (int i = 0; i < Configuration.getNumberOfProblems(); i++) {

    /**
     * creates a new grid and deploys agents and goal at random positions
     */
      BaseAgent.grid.resetState();

    /**
     * Führe Problem aus und aktualisiere aktuellen Zeitschritt
     */
      currentTimestep = doOneMultiStepProblem(currentTimestep);

    /**
     * Initialisiere neuen "Random Seed" Wert
     */
      Misc.initSeed(Configuration.getRandomSeed() + 
        experiment_nr * Configuration.getNumberOfProblems() + 1 + i);
    }
  }
  \end{verbatim}
 \label{mainExperiment:fig}
  \caption{Zentrale Schleife für einzelne Experimente}
\end{program}


\section{Reihenfolge der Ausführung}\label{reihenfolge:sec}

Bei Simulationen am Computer stellt sich sofort die Frage nach der Genauigkeit. Die Agenten werden bei dieser Simulation nacheinander abgearbeitet und bewegen sich auf einem Torus mit diskreten Feldern. Dies kann u.U. dazu führen, dass je nach Position in der Liste abzuarbeitender Agenten die Informationen über die Umgebung unterschiedlich alt sind. Die große Frage ist deshalb, in welcher Reihenfolge Sensordaten ermittelt, ausgewertet, Agenten bewegt, intern sich selbst bewertet und global die Qualität gemessen wird.\\
Da eine Aktion auf Basis der Sensordaten ausgewählt wird, ist die erste Restriktion, dass eine Aktion nach der Verarbeitung der Sensordaten stattfinden muss. Und da Aktionen bewertet werden sollen, also jeweils der Zustand nach der Bewegung mit dem gewünschten Zustand verglichen werden soll, ist die zweite Restriktion, dass die Bewertung einer Aktion nach dessen Ausführung stattfinden muss.\\
Ansonsten gibt es folgende Möglichkeiten:


\begin{enumerate}
\item Für alle Agenten werden erst einmal die neuen Sensordaten erfasst und sich für eine Aktion entschieden. Sind alle Agenten abgearbeitet, werden die Aktionen ausgeführt.
\item Die Agenten werden nacheinander abgearbeitet, es werden jeweils neue Sensordaten erfasst und sich sofort für eine neue Aktion entschieden. 
\end{enumerate}

Bei der ersten Möglichkeit haben alle Agenten die Sensordaten vom Beginn der Zeiteinheit, während bei der zweiten Möglichkeit später verarbeitete Agenten bereits die Aktionen der bereits berechneten Agenten miteinbeziehen können. Umgekehrt können dann frühere Agenten bessere Positionen früher besetzen. Da aufgrund der primitiven Sensoren nicht davon auszugehen ist, dass Agenten beginnende Bewegungen (und somit deren Zielposition) anderer Agenten einbeziehen können, soll jeder Agent von den Sensorinformationen zu Beginn der Zeiteinheit ausgehen.\\
Wenn sich mehrere Agenten auf dasselbe Feld bewegen wollen, dann spielt die Reihenfolge der Ausführung der Aktionen eine Rolle. Wird die Liste der Agenten einfach linear abgearbeitet, können Agenten mit niedriger Position in der Liste die Aktion auf Basis jüngerer Sensordaten fällen. Dies kann dazu führen, dass Aktionen von Agenten mit höherer Position in der Liste eher fehlschlagen, da das als frei angenommene Feld nun bereits besetzt ist. Da es keinen Grund gibt, Agenten mit niedrigerer Position zu bevorteilen, werden die Aktionen der Agenten in zufälliger Reihenfolge abgearbeitet.\\
Bezüglich der Bewegung ergibt sich hierbei eine weitere Frage, nämlich wie unterschiedliche Bewegungsgeschwindigkeiten behandelt werden sollen, da alle Agenten eine Einheitsgeschwindigkeit von maximal einem Feld pro Zeiteinheit haben, während sich das Zielobjekt je nach Szenario gleich eine ganze Anzahl von Feldern bewegen kann.\\

Auch hier habe ich mich für eine zufällige Verteilung entschieden. Kann sich das Zielobjekt um \(n\) Schritte bewegen, so wird seine Bewegung in \(n\) Einzelschritte unterteilt, die nacheinander mit zufälligen Abständen (d.h. Bewegungen anderer Agenten) ausgeführt werden.\\
Eine weitere Frage ist, wie das Zielobjekt diese weiteren Schritte festlegen soll. Hier soll ein Sonderfall eingeführt werden, so dass das Zielobjekt in einer Zeiteinheit mehrmals (\(n\)-mal) neue Sensordaten erfassen und sich für eine neue Aktion entscheiden kann.

TODO Begründung

\section{Messung der Qualität}\label{qualitaetsmessung:sec}

Eine konkrete Antwort kann man auf diese zwei Fragen nicht geben, sie hängt  davon ab, was man denn nun eigentlich erreichen möchte, also auf welche Weise die Qualität des Algorithmus bewertet wird. Der naheliegendste Messzeitpunkt ist, nachdem sich alle Agenten bewegt haben. Da wir die Agenten und das Zielobjekt in einem Durchlauf gemeinsam bewegen, stellt sich die Frage nicht, ob wir womöglich vor der Bewegung des Zielobjekts die Qualität messen sollen. Eine Messung nach der Bewegung des Zielobjekts würde diesem erlauben, sich vor jeder Messung optimal zu positionieren, was in einer geringeren Qualität für den Algorithmus resultiert, da sich das Zielobjekt aus der Überwachungsreichweite anderer Agenten hinausbewegen kann. Letztlich ist es eine Frage der Problemstellung, denn eine Messung nach Bewegung des Zielobjekts bedeutet letztlich, dass ein Agent einen gerade aus seiner Überwachungsreichweite heraus laufenden Zielobjekts in diesem Schritt nicht mehr überwachen kann.\\
Da ein wesentlicher Bestandteil die Kooperation (und somit die Abdeckung des Grids anstatt dem Verfolgen des Zielobjekts) sein soll, soll ein Bewertungskriterium sein, inwieweit der Einfluss des Zielobjekts minimiert werden soll. Auch findet, wenn wir vom realistischen Fall ausgehen, die Bewegung des Zielobjekts gleichzeitig mit allen anderen Agenten statt. Die Qualität wird somit nach der Bewegung des Zielobjekts gemessen. Die Überlegung unterstreicht auch nochmal, dass es besser ist, den Zielobjekts insgesamt wie einen normalen (aber sich mehrmals bewegenden) Agenten zu behandeln.\\

\section{Reihenfolge der Rewardermittlung}
Schließlich bleibt die Frage danach, wann geprüft werden soll, ob das Zielobjekt in Überwachungsreichweite ist, und wann sich somit ein Reward ergibt. Wesentliche Punkte hierbei sind, dass der Algorithmus sich anhand der Sensordaten selbst bewertet und pro Zeitschritt die Sensordaten nur einmal erhoben werden. Letzteres folgt aus der Auslegung von XCS, der in der Standardimplementation darauf ausgelegt ist, dass der Reward jeweils genau einer Aktion zugeordnet ist. Daraus ergibt sich auch, dass der Reward von binärer Natur ist (``Zielobjekt in Überwachungsreichweite'' oder ``Zielobjekt nicht in Überwachungsreichweite''), weshalb Zwischenzustände für den Reward, der sich aus der mehrfachen Bewegung des Zielagenten ergeben könnte, ausgeschlossen werden soll (z.B. ``War zwei von drei Schritten in der Überwachungsreichweite'' \(\Rightarrow \frac{2}{3}\) Reward). Insbesondere würde dies eine mehrfache Erhebung der Sensordaten erfordern.\\
Für den Reward gibt es somit folgende Möglichkeiten:

\begin{enumerate}
\item Rewards werden direkt nach der Ausführung einer einzelnen Aktion ermittelt.
\item Rewards werden nach Ausführung aller Aktionen der Agenten und des Zielobjekts ermittelt.
\end{enumerate}

Werden die Rewards sofort ermittelt (Punkt 1), dann bezieht sich der Reward auf die veralteten Sensordaten vor der Aktion, die Aktion selbst würde bei der Rewardvergabe also ignoriert werden. Bei Punkt 2 müssen wir bis zum neuen Zeitschritt warten, bis neue Sensordaten ermittelt wurden.


\section{Zusammenfassung}
Zusammenfassend sieht der Ablauf aller Agenten (inklusive des Zielobjekts) also wie folgt aus:

\begin{figure}[H]
\setbox0\vbox{\small
\begin{enumerate}
\item Erfassung aller Sensordaten
\item Bestimmung des jeweiligen Rewards für die einzelnen Objekte für den letzten Schritt
\item Bestimmen der aktuellen Qualität
\item Wahl der Aktion anhand der Regeln des jeweiligen Agenten
\item Ausführung der Aktion (in zufälliger Reihenfolge, das Zielobjekt führt nach dem ersten Schritt außerdem Schritt 1 und 2 für alle weiteren Schritte nochmals durch)
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}

Die Implementierung Programm~\ref{mainCalculate:fig} wird in einer Schleife aufgerufen von Programm~\ref{mainProblem:fig}, welches in der oben erwähnten Schleife 

\begin{program}
  \begin{verbatim}
/**
 * Führt eine Anzahl von Schritten auf dem aktuellen Torus aus
 * @param stepCounter Aktuelle Zeitschritt
 * @return Der Zeitschritt nach der Ausführung
 */
  private int doOneMultiStepProblem(int stepCounter) {
  /**
   * Zeitpunkt bis zu dem das Problem ausgeführt wird
   */
    int steps_next_problem = 
      Configuration.getNumberOfSteps() + stepCounter;
    
    for (int currentTimestep = stepCounter; 
         currentTimestep < steps_next_problem; currentTimestep++) {

    /**
     * Ermittle die Sichtbarkeit aller Felder
     */
      BaseAgent.grid.updateSight();

    /**
     * Ermittle die Statistiken (Abdeckung, Distanz, Qualität etc.)
     */ 
      BaseAgent.grid.updateStatistics(currentTimestep, findBestAgent());

    /**
     * Ermittle den Reward für alle Objekte (nach dem ersten Schritt)
     */
      if(currentTimestep > stepCounter) {
        rewardAgents(currentTimestep);
      }

    /**
     * Berechne und bewege alle Agenten
     */
      calculateAgents(currentTimestep);
    }

    /**
     * Abschließende Ermittlung des Rewards
     */
    BaseAgent.grid.updateSight();
    rewardAgents(steps_next_problem);
    return steps_next_problem;
  }
  \end{verbatim}
 \label{mainProblem:fig}
  \caption{Zentrale Schleife für einzelne Probleme}
\end{program}

\begin{program}
  \begin{verbatim}
/**
 * Berechnet die Aktionen und führt sie in zufälliger Reihenfolge aus
 * @param gaTimestep der aktuelle Zeitschritt
 */
  private void calculateAgents(final long gaTimestep) {

  /**
   * Erstelle Ausführungsliste für alle Objekte (Zielobjekt mehrfach)
   */
    int goal_speed = Configuration.getGoalAgentMovementSpeed();
    ArrayList<BaseAgent> random_list = 
      new ArrayList<BaseAgent>(agentList.size() + goal_speed);

    random_list.addAll(agentList);
    for(int i = 0; i < goal_speed; i++) {
      random_list.add(BaseAgent.goalAgent);
    }

  /**
   * Ermittle Sensordaten und bestimme nächste Bewegung
   */
    for(BaseAgent a : random_list) {
      a.aquireNewSensorData();
      a.calculateNextMove(gaTimestep);
    }

  /**
   * Führe die ermittelten Aktionen in zufälliger Reihenfolge aus
   *   (Zielobjekt kann mehrfach ausgeführt werden).
   */
    int[] array = Misc.getRandomArray(random_list.size());
    for(int i = 0; i < array.length; i++) {
      BaseAgent a = random_list.get(array[i]);
      a.doNextMove();
      if(a.isGoalAgent()) {
        a.aquireNewSensorData();
        a.calculateNextMove(0);
      }
    }
  }
  \end{verbatim}
 \label{mainCalculate:fig}
  \caption{Zentrale Bearbeitung aller Agenten innerhalb eines Problems}
\end{program}
