\chapter{Verzögertes SXCS}

\section{Verzögerter Reward}

Der wesentliche Unterschied zur ersten XCS Variante SXCS ist, dass jeglicher ermittelter \emph{reward} Wert und der jeweils zugehörige Faktor lediglich erst einmal zusammen mit den jeweiligen \emph{actionSets} in einer Liste (\emph{historicActionSet} TODO Bezeichnung) gespeichert werden und in jedem Schritt immer nur die \emph{classifiers} des \emph{actionSets} des ältesten Eintrags in der \emph{historicActionSet} Liste aktualisiert wird. Somit haben wir also eine zeitlich beliebig verzögerbare Aktualisierungsfunktion, welche uns erlaubt, mehrere gleichzeitig stattgefundene (aber erst verzögert eintreffende, wegen z.B. Kommunikationsschwierigkeiten) Ereignisse zusammen auszuwerten. Dies ist eine wesentliche Voraussetzung für Kommunikation zwischen den Agenten. TODO


Die Funktion \emph{calculateReward()} ist identisch mit der in Kapitel ~\ref{calculateRewardLCS:fig}besprochenen Funktion bei der SXCS Variante ohne verzögerten Reward.

\begin{program}
  \begin{verbatim}
/**
 * Diese Funktion verarbeitet den übergebenen Reward und gibt ihn an die
 * zugehörigen ActionSets weiter. Wesentlicher Unterschied zum LCS ohne 
 * Verzögerung ist, dass maxPrediction erst bei der endgültigen 
 * Verarbeitung des historicActionSets ermittelt wird.
 *
 * @param reward Wahr wenn der Zielagent in Sicht war.
 * @param best_value Bester Wert des vorangegangenen actionSets
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, d.h.
 *        einem positiven Reward, aufgerufen wurde
 */

  public void collectReward(
                boolean reward, double best_value, boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;

  /**
   * Aktualisiere eine ganze Anzahl von Einträgen im historicActionSet
   */
     for(int i = 0; i < action_set_size; i++) {

  /**
   * Benutze aufsteigenden bzw. absteigenden Reward bei einem positiven 
   * bzw. negativen Ereignis
   */
       if(is_event) {
         corrected_reward = reward ? 
           calculateReward(i, action_set_size) : 
           calculateReward(action_set_size - i, action_set_size);
       }

  /**
   * Füge den ermittelten Reward zum historicActionSet
   */
       historicActionSet.get(start_index - i).
         addReward(corrected_reward, factor);

    }

\end{verbatim}
  \caption{Zweites Kernstück des verzögerten SXCS-Algorithmus (\emph{collectReward()} - Verteilung des Rewards auf die ActionSets)}
\end{program}

\begin{program}
  \begin{verbatim}

/**
 * Der erste Teil der Funktion ist identisch mit dem calculateNextMove
 * der LCS Variante ohne Kommunikation. Der Zusatz ist, dass beim 
 * Überlauf die im HistoricActionSet gespeicherte Rewards verarbeitet
 * werden
 */

  public void calculateNextMove(long gaTimestep) {
 
  // ... 

  /**
   * HistoryActionSet voll? Dann verarbeite den dort gespeicherten Reward
   */
     if (historicActionSet.size() > Configuration.getMaxStackSize()) {
      HistoryActionClassifierSet first = historicActionSet.pop();
      last.processReward(historicActionSet.getFirst().getBestValue());
    }
  }
\end{verbatim}
  \caption{Auszug aus dem dritten Kernstück des verzögerten SXCS-Algorithmus (\emph{calculateNextMove()})}
\end{program}

\begin{program}
  \begin{verbatim}

/**
 * Zentrale Routine des HistoryActionSets zur Verarbeitung aller 
 * eingegangenen Rewards bis zu diesem Punkt.
 */

  public void processReward(double max_prediction) {

    double max = 0.0;
    double max_factor = 0.0;
  /**
   * Finde das größte reward / factor Paar TODO Verbessern
   */
    for(RewardHelper r : reward) {
      if(r.reward >= max && r.factor >= max_factor) {
        max = r.reward;
        max_factor = r.factor;
      }
    }
  /**
   * Aktualisiere den Reward mit den ermittelten Werten und dem
   * übergebenen maxPrediction Wert
   */
    actionClassifierSet.updateReward(max, max_prediction, max_factor);
  }
\end{verbatim}
  \caption{Auszug aus dem vierten Kernstück des verzögerten SXCS-Algorithmus (Verarbeitung des Rewards, \emph{processReward()})}
\end{program}


