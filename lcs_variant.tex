\chapter{XCS Varianten}\label{lcs_variants:cha}

Ziel der Arbeit war es, wie man den XCS Algorithmus auf ein Überwachungsszenario anwenden kann. Notwendig dafür war es, die XCS Implementierung vollständig nachzuvollziehen, um für jeden Bestandteil entscheiden zu können, welche Rolle er bezüglich eines solchen Szenarios spielt. Für die Tests wurde nicht auf bestehende Pakete (z.B. XCSlib~\cite{xcslib}) zurückgegriffen, wenn auch der Quelltext von \cite{Butz_xcsclassifier} Modell stand und aus ihm große Teile entnommen wurden.\\

Im Vordergrund stand zum einen die grundsätzliche Frage, ob XCS in einem solchen Szenario überhaupt besser als ein Algorithmus sein kann, der sich rein zufällig verhält, und wie mögliche Ansätze aussehen können, den Algorithmus zu verbessern.\\

Zuerst sollen allgemeine Anpassungen des Algorithmus und der Implementation besprochen werden (siehe Kapitel~\ref{allgemeine_anpassungen:sec}) um dann auf die konkreten Veränderungen der einzelnen XCS Varianten einzugehen. Zum einen wird der XCS Algorithmus selbst in Kapitel~\ref{standardxcs:sec} vorgestellt, dort wird insbesondere die Behandlung des Neustarts eines Problems diskutiert. Zum anderen wird eine an Überwachungsszenarios angepasste Variante, der sogenannte SXCS Algorithmus, vorgestellt werden. Dieser Algorithmus wurde unter dem Gesichtspunkt des Problems einer kontinuierlichen Überwachung eines Zielobjekts entwickelt, also nicht, wie viele der Standardprobleme beim originalen XCS \emph{multi step} Verfahren, einen Weg durch ein Labyrinth zu einem Ziel finden. Schließlich soll in Kapitel~\ref{communication:cha} eine Variante mit verzögerter Aktualisierung des \emph{reward} Werts in den \emph{action set} Listen und eine darauf aufbauende Variante mit Kommunikation mit anderen Agenten vorgestellt werden (der sogenannte DSXCS Algorithmus, siehe Kapitel~\ref{communication:cha}).


\section{Allgemeine Anpassungen}\label{allgemeine_anpassungen:sec}

Eine Anzahl allgemeine Änderungen an der Implementation und am Algorithmus waren notwendig, um XCS in einem Überwachungsszenario laufen zu lassen. Unter anderen sind dies:

\begin{enumerate}
\item Die Berechnung der Summe der \emph{numerosity} Werte wurde vollständig neuorganisiert wie auch ein Fehler bei der Aktualisierung des \emph{numerosity} Werts in der Implementierung korrigiert (siehe Kapitel~\ref{corrected_numerosity_function:sec})

\item Der genetischer Operator wird hier zwei feste anstatt zufällige Schnittpunkte für das \emph{two point crossover} verwenden (siehe Kapitel~\ref{genetische_operatoren:sec}).

\item Die Qualität des Algorithmus wird nicht nur in der \emph{exploit} Phase gemessen werden, da ein fortlaufendes Problem und kein statisches Szenario betrachtet wird (siehe Kapitel~\ref{exploreexploit:sec}).

\item Mehrere XCS Parameter wurden angepasst (siehe Kapitel~\ref{cha:parameter}).

\item Das Erreichen des Ziels wurde für das Überwachungsszenario neu verfasst wie auch der Neustart von Probleminstanzen neu geregelt wurde (siehe Kapitel~\ref{bewertung:sec}).

\item Die Reihenfolge bei der Bewertung, Entscheidung und Aktion in einem Multiagentensystem auf einem diskreten Torus musste überdacht werden (siehe Kapitel~\ref{ablauf_lcs:sec})
\end{enumerate}


\section{XCS \emph{multi step} Verfahren}\label{standardxcs:sec}

Idee dieses Verfahrens ist, dass der \emph{reward} Wert, den eine Aktion (bzw. der jeweils zugehörigen \emph{action set} Liste und die dortigen \emph{classifier}) erhält, vom erwarteten \emph{reward} Wert der folgenden Aktion abhängen soll. Somit wird, rückführend vom letzten Schritt auf das Ziel, der \emph{reward} Wert schrittweise (mit jeder neuen Probleminstanz) an vorgehende Aktionen verteilt. Die Annahme ist, dass dann, durch mehrfache Wiederholung des Lernprozesses, mit dem sich dadurch ergebenen Regelsatz mit höherer Wahrscheinlichkeit das Ziel gefunden wird.\\

Dies entspricht dem aus \cite{butz01algorithmic} bekannten XCS \emph{multi step} Verfahren. Der wesentliche Unterschied der Implementierung in dieser Arbeit ist, dass das Szenario bei einem positiven \emph{base reward} nicht neugestartet wird, algorithmisch ist die Implementierung ansonsten identisch. Dies zeigt sich in Programm~\ref{multistep_calc_reward:pro} (Zeilen 22-27), zwar wird die \emph{action set} Liste gelöscht, das Szenario selbst läuft aber weiter. In der originalen Implementierung in~\cite{Butz_xcsclassifier} wird an dieser Stelle im Algorithmus die aktuelle Probleminstanz abgebrochen (in \emph{XCS.java} in der Funktion \emph{doOneMultiStepProblemExploit()} bzw. \emph{doOneMultiStepProblemExplore()}). Liegt kein positiver \emph{base reward} Wert vor, so wird lediglich der für diesen Schritt erwartete \emph{reward} Wert (nämlich der \emph{maxPrediction} Wert) an die letzte \emph{action set} Liste gegeben.\\

In den Programmen~\ref{multistep_collect_reward:pro} und \ref{multistep_calc_move:pro} finden sich, neben Anpassungen an den Simulator, keine wesentlichen Änderungen. In Programm~\ref{multistep_collect_reward:pro} wird der ermittelte \emph{base reward} zusammen mit dem ermittelten \emph{maxPrediction} Wert an die Aktualisierungsfunktion der jeweiligen \emph{action set} Liste weitergegeben und in Programm~\ref{multistep_calc_move:pro} wird eine Aktion ausgewählt und entsprechende \emph{match set} und \emph{action set} Listen erstellt.


\section{XCS Variante für Überwachungsszenarien (SXCS)}\label{sxcs_variant:sec}

Die Hypothese bei der Aufstellung dieser XCS Variante ist im Grunde dieselbe wie beim XCS \emph{multi step} Verfahren selbst, nämlich dass die Kombination mehrerer Aktionen zum Ziel führt. Beim \emph{multi step} Verfahren besteht die wesentliche Verbindung zwischen den \emph{action set} Listen jeweils nur zwischen zwei direkt aufeinanderfolgenden \emph{action set} Listen über den \emph{maxPrediction} Wert. In einer statischen Umgebung kann dadurch über mehrere (identische) Probleme hinweg eine optimale Einstellung (des \emph{fitness} und \emph{reward prediction} Werts) für die \emph{classifier} gefunden werden.\\

Bei der hier besprochenen SXCS Variante (\emph{Supervising eXtended Classifier System}) soll in Kapitel~\ref{umsetzung_sxcs:sec} zuerst die Umsetzung dieser Idee diskutiert. Insbesondere baut sie auf sogenannten Ereignissen auf, die mit einer Änderung des \emph{base reward} Werts einhergehen, welche in Kapitel~\ref{sec:events} erklärt werden. Die Implementierung selbst wird dann in Kapitel~\ref{sxcs_implementation:sec} vorgestellt.\\


\subsection{Umsetzung von SXCS}\label{umsetzung_sxcs:sec}

Bei SXCS Variante soll die Verbindung zwischen den \emph{action set} Listen direkt durch die zeitliche Nähe zur Vergabe des \emph{base reward} gegeben sein. Es wird in jedem Schritt die jeweilige \emph{action set} Liste gespeichert und aufgehoben, bis ein neues Ereignis (siehe Kapitel~\ref{sec:events}) eintritt und dann in Abhängigkeit des Alters mit einem entsprechenden \emph{reward} Wert aktualisiert.\\
Bezeichne \(r(a)\) den \emph{reward} Wert für die \emph{action set} Liste mit Alter \(a\), bei linearer Verteilung des \emph{base reward} ergibt sich dann:
$$
r(a) = \left\{ \begin{array}{rl}
  \frac{a}{\mathrm{size(actionSet)}} &\mbox{, falls base reward = $1$} \\
  \frac{1 - a}{\mathrm{size(actionSet)}} &\mbox{, falls base reward = $0$}
       \end{array} \right.
$$

bzw. bei quadratischer Verteilung des \emph{base reward}:

$$
r(a) = \left\{ \begin{array}{rl}
  \frac{{a}^{2}}{\mathrm{size(actionSet)}} &\mbox{ falls base reward = $1$} \\
  \frac{{1 - a}^{2}}{\mathrm{size(actionSet)}} &\mbox{ falls base reward = $0$}
       \end{array} \right.
$$

Die schematische Abbildung~\ref{positive_negative_reward:fig} demonstriert diesen Sachverhalt nochmals anschaulich. In Tests ergab sich für die quadratische Verteilung des \emph{base reward} ein minimal besseres Ergebnis, weitere Grafiken werden auf die lineare Verteilung des \emph{base reward} beschränkt sein, um eine verständliche Darstellung zu ermöglichen, während in den Simulationen die quadratische Vergabe des \emph{base reward} benutzt wird.

\begin{figure}[htbp]
\centerline{	
\includegraphics{positive_negative_reward.eps}
}
\caption[Schematische Darstellung der Verteilung des \emph{reward} an \emph{action set} Listen] {Schematische Darstellung der (quadratischen) Verteilung des \emph{reward} an gespeicherte \emph{action set} Listen bei einem positiven bzw. negativen Ereignis}
\label{positive_negative_reward:fig}
\end{figure}




\subsection{Ereignisse}\label{sec:events}

In XCS wird lediglich das jeweils letzte \emph{action set} Liste aus dem vorherigen Zeitschritt gespeichert, in der neuen Implementierung werden dagegen eine ganze Anzahl (bis zum Wert \emph{maxStackSize}) von \emph{action set} Listen gespeichert. Die Speicherung erlaubt zum einen eine Vorverarbeitung des \emph{reward} anhand der vergangenen Zeitschritte und auf Basis einer größeren Zahl von \emph{action set} Listen und zum anderen die zeitliche Relativierung einer \emph{action set} Liste zu einem Ereignis. Die \emph{classifier} werden dann jeweils rückwirkend anhand des jeweiligen \emph{reward} Werts aktualisiert, sobald bestimmte Bedingungen eingetreten sind.\\

Von einem positiven bzw. negativen Ereignis spricht man, wenn sich der \emph{base reward} im Vergleich zum vorangegangenen Zeitschritt verändert hat, also wenn das Zielobjekt sich in Überwachungsreichweite bzw. aus ihr heraus bewegt hat (siehe Abbildung~\ref{saved_rewards:fig}).\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{saved_rewards.eps}
}
\caption[Schematische Darstellung der zeitlichen Verteilung des \emph{reward} an und der Speicherung von \emph{action set} Listen] {Schematische Darstellung der zeitlichen Verteilung des \emph{reward} an \emph{action set} Listen nach mehreren positiven und negativen Ereignissen und der Speicherung der letzten \emph{action set} Liste}
\label{saved_rewards:fig}
\end{figure}

Bei der Benutzung eines solchen Stacks entsteht eine Zeitverzögerung, d.h. die \emph{classifier} erhalten jeweils Information, die bis zu \emph{maxStackSize} Schritte zu alt sein kann. Tritt beim Stack ein Überlauf ein, gab es also \emph{maxStackSize} Schritte lang keine Änderung des \emph{base reward} Werts, wird abgebrochen und die \(\frac{maxStackSize}{2}\) ältesten Einträge vom Stack genommen.\\

Alle diese Einträge werden vorher dabei mit diesem \emph{base reward} Wert aktualisiert. Abbildung~\ref{neutral_reward:fig} zeigt die Bewertung bei einem solchen neutralen Ereignis, bei dem nach Überlauf die erste Hälfte mit \(1\) bewertet wurde. Außerdem ist dort der maximale Fehler dargestellt, welcher eintreten würde, wenn direkt beim Schritt nach dem Abbruch eine Änderung des \emph{base reward} Werts auftritt, im dargestellten Fall also der \emph{base reward} sich beim aktuellen Zeitpunkt auf \(0\) verändern würde.\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{neutral_reward.eps}
}
\caption[Schematische Darstellung der Bewertung von \emph{action set} Listen bei einem neutralen Ereignis] {Schematische Darstellung der Bewertung von \emph{action set} Listen bei einem neutralen Ereignis (mit \emph{base reward} = 1)}
\label{neutral_reward:fig}
\end{figure}


\subsection{Größe des Stacks (\emph{maxStackSize})}

Offen bleibt die Frage nach der Größe des Stacks. Mangels theoretischem Fundament muss man zwischen den drei wirkenden Faktoren einen Kompromiss finden. Erstens gibt es die Verzögerung zu Beginn eines Problems und insbesondere zu Beginn eines Experiments, es kann u.U. bis zu \(\frac{maxStackSize}{2}\) Schritte dauern, bis das erste Mal ein \emph{classifier} aktualisiert wird. Auch werden bei einem großen \emph{maxStackSize} Wert womöglich Aktionen positiv (oder negativ) bewertet, die an der Situation nicht beteiligt waren, vor allem wenn es sich um kurze lokale Entscheidungen handelt. Umgekehrt, wählt man den Stack zu klein, kann es sein, dass ein Überlauf und somit u.U. ein gewisser Fehler auftritt. Der Wert \emph{maxStackSize} stellt also einen Kompromiss zwischen Zeitverzögerung bzw. Reaktionsgeschwindigkeit und Genauigkeit dar.\\


Wie Abbildung~\ref{max_stack_size:fig} zeigt, ist dies bei größerer Schrittzahl (2000 Schritte) aber vernachlässigbar, die erreichten Qualitäten unterscheiden sich im betrachteten Wertebereich kaum voneinander. Es gibt bei geringen Werten einen kleinen Anstieg, außerdem einen kleinen Abfall beim schwierigen Szenario. Da während der Entwicklung die meisten Tests mit dem Wert 128 durchgeführt wurden, wird dieser Wert belassen. Nur für das schwierige Szenario ist womöglich ein Wert von 64 vorzuziehen.

\begin{figure}[htbp]
\centerline{	
\includegraphics{max_stack_size.eps}
}
\caption[Vergleich verschiedener Werte für \emph{maxStackSize}] {Vergleich verschiedener Werte für \emph{maxStackSize} (2000 Schritte, SXCS Agenten)}
\label{max_stack_size:fig}
\end{figure}


\subsection{Zusammenfassung der Ereignisse}

Zusammengefasst ergeben sich also folgende Ereignisse:

\begin{figure}[H]
\setbox0\vbox{\small
Ein Ereignis tritt auf, wenn:
\begin{enumerate}
\item Änderung des \emph{base reward} Werts von 0 auf 1 (Zielobjekt war im letzten Zeitschritt nicht in Überwachungsreichweite) \(\Rightarrow\) positives Ereignis
\item Änderung des \emph{base reward} Werts von 1 auf 0 (Zielobjekt war im letzten Zeitschritt in Überwachungsreichweite) \(\Rightarrow\) negatives Ereignis
\item Überlauf des Stacks (kein positives oder negatives Ereignis in den letzten \emph{maxStackSize} Schritten), Zielobjekt ist in Überwachungsreichweite \(\Rightarrow\) neutrales Ereignis (mit \emph{base reward} = \(1\))
\item Überlauf des Stacks (kein positives oder negatives Ereignis in den letzten \emph{maxStackSize} Schritten), Zielobjekt ist nicht in Überwachungsreichweite \(\Rightarrow\) neutrales Ereignis (mit \emph{base reward} = \(0\))
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}




\subsection{Implementierung von SXCS}\label{sxcs_implementation:sec}

Im Wesentlichen entspricht die Implementierung von SXCS der bekannten Implementierung von XCS (siehe Kapitel~\ref{standardxcs:sec}). Unterschiede gibt es erstens bei Berechnung des \emph{reward} Werts in der Funktion \emph{calculateReward()} in Programm~\ref{sxcs_calc_reward:pro}, bei der zwischen zwei Fällen unterschieden wird. Zum einen gibt es die Behandlung negativer und positiver Ereignisse (Zeile 17-21) und zum anderen die Behandlung des Überlaufs des Stacks (Zeile 24-30), während bei der Implementierung von XCS in Programm~\ref{multistep_calc_reward:pro} in fast jedem Schritt unabhängig von Ereignissen eine Aktualisierung stattfindet. Zweitens gibt es Unterschiede in der Funktion \emph{collectReward()} in Programm~\ref{sxcs_collect_reward:pro}. Dort werden nicht nur die aktuelle bzw. letzte \emph{action set} Liste aktualisiert, sondern eine ganze Reihe aus dem gespeicherten Stack. Insbesondere werden dort die auf- bzw. absteigenden \emph{reward} Werte nach einem positiven bzw. negativen Ereignis berechnet (Zeile 31-33). Bei der Berechnung der nächsten Aktion hingegen (Funktion \emph{calculateNextMove()} in Programm~\ref{sxcs_calc_move:pro}) wurde lediglich die Behandlung des Stacks hinzugefügt (Zeile 39-43).


\subsection{Zielobjekt mit XCS und SXCS}\label{variant_zielobjekt_xcs_sxcs:sec}

Wie bereits in Kapitel~\ref{zielobjekt_sxcs_einfuehrung:sec} erwähnt, soll hier eine Implementierung von XCS und SXCS für das Zielobjekt diskutiert werden. Der Grund für die Untersuchung liegt mehr darin, eine weitere Anwendungsmöglichkeit aufzuzeigen und XCS und SXCS nochmals zu vergleichen, anstatt konkrete neue Erkenntnisse zu gewinnen. Insbesondere handelt es sich hierbei nicht mehr um ein kollaboratives Multiagentensystem, da es zum einen nur ein einziges Zielobjekt im Szenario gibt und zum anderen die Aufgabe, anderen Agenten auszuweichen, durch Zusammenarbeit nicht besser gelöst werden kann (mal von hochentwickelten Verhaltensweisen abgesehen, bei der ein Zielobjekt das andere dadurch "`rettet"', dass es einen Agent weglocken kann). Die Ergebnisse der Analyse folgt in Kapitel~\ref{goal_agent_with_xcs:sec}, auch hier ist der Ansatz von SXCS gegenüber dem von XCS überlegen.\\

Was die Implementierung betrifft ist sie, bis auf die Funktion \emph{checkRewardPoints()}, für das Zielobjekt fast identisch. Dort wird ein positiver \emph{base reward} Wert dann vergeben, wenn kein Agent in Sicht ist (während bei den Agenten ein positiver \emph{base reward} Wert dann vergeben wurde, wenn das Zielobjekt in Sicht ist). Die einzige zweite Änderung ist in der Funktion \emph{calculateNextMove()} (siehe Programm~\ref{multistep_calc_move:pro} (XCS) bzw. Programm~\ref{sxcs_calc_move:pro} (SXCS)), bei der die zusätzliche Sprungeigenschaft des Zielobjekts hinzugefügt ist (siehe Kapitel~\ref{zielobjekt:sec}).\\




