\chapter{XCS Varianten}\label{lcs_variants:cha}

Ziel der Arbeit ist es, herauszufinden, wie man den XCS Algorithmus auf ein Überwachungsszenario anwenden kann. Notwendig dafür war es, die XCS Implementierung vollständig nachzuvollziehen, um für jeden Bestandteil entscheiden zu können, welche Rolle er bezüglich eines solchen Szenarios spielt. Für die Tests wurde nicht auf bestehende Pakete (z.B. XCSlib~\cite{xcslib}) zurückgegriffen, wenn auch der Quelltext von \cite{Butz_xcsclassifier} Modell stand und aus ihm große Teile entnommen wurden.\\
Im Vordergrund stehen zwei grundsätzliche Fragen:
\begin{itemize}
\item zum einen, ob XCS für die Lösung solcher Szenarien besser eignet als ein sich rein zufällig verhaltender Algorithmus und
\item zum anderen, wie mögliche Verbesserungsansätze für den Algorithmus aussehen könnten.
\end{itemize}

Zuerst werden allgemeine Anpassungen des Algorithmus und der Implementation besprochen (siehe Kapitel~\ref{allgemeine_anpassungen:sec}) um dann auf die konkreten Veränderungen der einzelnen XCS Varianten einzugehen. Zum einen wird der XCS Algorithmus selbst in Kapitel~\ref{standardxcs:sec} vorgestellt, dort wird insbesondere die Behandlung des Neustarts eines Problems diskutiert. Zum anderen wird eine an Überwachungsszenarios angepasste Variante, der sogenannte SXCS Algorithmus, vorgestellt. Dieser Algorithmus wurde unter dem Gesichtspunkt des Problems einer kontinuierlichen Überwachung eines Zielobjekts entwickelt, also nicht, wie viele der Standardprobleme beim originalen XCS \emph{multi step} Verfahren, einen Weg durch ein Labyrinth zu einem Ziel finden. 

%Schließlich stellt das Kapitel~\ref{communication:cha} eine Variante mit verzögerter Aktualisierung des \emph{reward} Werts in den \emph{action set} Listen und eine darauf aufbauende Variante mit Kommunikation mit anderen Agenten vor (der sogenannte DSXCS Algorithmus, siehe Kapitel~\ref{communication:cha} TODO).


\section{Allgemeine Anpassungen}\label{allgemeine_anpassungen:sec}

Eine Anzahl allgemeiner Änderungen an der Implementation und am Algorithmus waren notwendig, um XCS in einem Überwachungsszenario laufen zu lassen. Unter anderen sind dies:


\begin{itemize}
\item Berechnung der Summe der \emph{numerosity} Werte wurde neuorganisiert und ein Fehler bei der Aktualisierung des \emph{numerosity} Werts in der Implementierung korrigiert (siehe Kapitel~\ref{corrected_numerosity_function:sec}).

\item Der genetische Operator wird hier zwei feste, anstatt zufällige Schnittpunkte für das \emph{two point crossover} verwenden (siehe Kapitel~\ref{genetische_operatoren:sec}).

\item Die Qualität des Algorithmus wird nicht nur in der \emph{exploit} Phase gemessen werden, da ein fortlaufendes Problem und kein statisches Szenario betrachtet wird (siehe Kapitel~\ref{exploreexploit:sec}).

\item Mehrere XCS Parameter wurden angepasst (siehe Kapitel~\ref{cha:parameter}).

\item Das Erreichen des Ziels wurde für das Überwachungsszenario neu verfasst, wie auch der Neustart von Probleminstanzen neu geregelt wurde (siehe Kapitel~\ref{bewertung:sec}).

\item Die Reihenfolge bei der Bewertung, Entscheidung und Aktion in einem Multiagentensystem auf einem diskreten Torus musste überdacht werden (siehe Kapitel~\ref{ablauf_lcs:sec})

\end{itemize}



\section{XCS \emph{multi step} Verfahren}\label{standardxcs:sec}

Idee dieses Verfahrens ist, dass der \emph{reward} Wert, den eine Aktion (bzw. der jeweils zugehörigen \emph{action set} Liste und die dortigen \emph{classifier}) erhält, vom erwarteten \emph{reward} Wert der folgenden Aktion abhängen soll. Somit wird, rückführend vom letzten Schritt auf das Ziel, der \emph{reward} Wert schrittweise (mit jeder neuen Probleminstanz) an vorgehende Aktionen verteilt. Dabei gilt die Annahme, dass durch mehrfache Wiederholung des Lernprozesses sich ein Regelsatz ergibt, mit dem das Ziel mit höherer Wahrscheinlichkeit gefunden wird.\\

Dies entspricht dem aus \cite{butz01algorithmic} bekannten XCS \emph{multi step} Verfahren. Der wesentliche Unterschied der Implementierung in dieser Arbeit ist, dass das Szenario bei einem positiven \emph{base reward} nicht neugestartet wird. Algorithmisch ist die Implementierung ansonsten identisch. Dies zeigt sich in Programm~\ref{multistep_calc_reward:pro} (Zeilen 22-27). Zwar wird hier die \emph{action set} Liste gelöscht, das Szenario selbst läuft aber weiter. In der originalen Implementierung in~\cite{Butz_xcsclassifier} wird an dieser Stelle im Algorithmus die aktuelle Probleminstanz abgebrochen (in \emph{XCS.java} in der Funktion \emph{doOneMultiStepProblemExploit()} bzw. \emph{doOneMultiStepProblemExplore()}). Liegt kein positiver \emph{base reward} Wert vor, so wird lediglich der für diesen Schritt erwartete \emph{reward} Wert (nämlich der \emph{maxPrediction} Wert) an die letzte \emph{action set} Liste gegeben.\\

In den Programmen~\ref{multistep_collect_reward:pro} und \ref{multistep_calc_move:pro} finden sich, neben Anpassungen an den Simulator, keine wesentlichen Änderungen. In Programm~\ref{multistep_collect_reward:pro} wird der ermittelte \emph{base reward} zusammen mit dem ermittelten \emph{maxPrediction} Wert an die Aktualisierungsfunktion der jeweiligen \emph{action set} Liste weitergegeben und in Programm~\ref{multistep_calc_move:pro} wird eine Aktion ausgewählt und entsprechende \emph{match set} und \emph{action set} Listen erstellt.


\section{XCS Variante für Überwachungsszenarien (SXCS)}\label{sxcs_variant:sec}

Die Hypothese bei der Aufstellung dieser XCS Variante ist im Grunde dieselbe wie beim XCS \emph{multi step} Verfahren selbst, nämlich dass die Kombination mehrerer Aktionen zum Ziel führt. Beim \emph{multi step} Verfahren besteht die wesentliche Verbindung zwischen den \emph{action set} Listen jeweils nur zwischen zwei direkt aufeinanderfolgenden \emph{action set} Listen über den \emph{maxPrediction} Wert. In einer statischen Umgebung kann dadurch über mehrere (identische) Probleme hinweg eine optimale Einstellung (des \emph{fitness} und \emph{reward prediction} Werts) für die \emph{classifier} gefunden werden.\\

Bei der hier besprochenen SXCS Variante (\emph{Supervising eXtended Classifier System}) wird in Kapitel~\ref{umsetzung_sxcs:sec} zuerst die Umsetzung dieser Idee diskutiert werden. Insbesondere baut sie auf sogenannten Ereignissen auf, die mit einer Änderung des \emph{base reward} Werts einhergehen, welche in Kapitel~\ref{sec:events} erklärt werden. Die Implementierung selbst wird dann in Kapitel~\ref{sxcs_implementation:sec} vorgestellt.\\


\subsection{Umsetzung von SXCS}\label{umsetzung_sxcs:sec}

Bei SXCS Variante soll die Verbindung zwischen den \emph{action set} Listen direkt durch die zeitliche Nähe zur Vergabe des \emph{base reward} gegeben sein. Es wird in jedem Schritt die jeweilige \emph{action set} Liste gespeichert und aufgehoben, bis ein neues Ereignis (siehe Kapitel~\ref{sec:events}) eintritt und dann in Abhängigkeit des Alters mit einem entsprechenden \emph{reward} Wert aktualisiert.\\
Bezeichne \(r(a)\) den \emph{reward} Wert für die \emph{action set} Liste mit Alter \(a\), bei linearer Verteilung des \emph{base reward} ergibt sich dann:
$$
r(a) = \left\{ \begin{array}{rl}
  \frac{a}{\mathrm{size(\emph{action set})}} &\mbox{, falls \emph{base reward} = $1$} \\
  \frac{1 - a}{\mathrm{size(\emph{action set})}} &\mbox{, falls \emph{base reward} = $0$}
       \end{array} \right.
$$

bzw. bei quadratischer Verteilung des \emph{base reward}:

$$
r(a) = \left\{ \begin{array}{rl}
  \frac{{a}^{2}}{{\mathrm{size(\emph{action set})}}^{2}} &\mbox{ falls \emph{base reward} = $1$} \\
  \frac{{(1 - a)}^{2}}{{\mathrm{size(\emph{action set})}}^{2}} &\mbox{ falls \emph{base reward} = $0$}
       \end{array} \right.
$$

Die schematische Abbildung~\ref{positive_negative_reward:fig} demonstriert diesen Sachverhalt nochmals anschaulich. In Tests ergab sich für die quadratische Verteilung des \emph{base reward} ein minimal besseres Ergebnis, weitere Grafiken werden auf die lineare Verteilung des \emph{base reward} beschränkt sein, um eine verständliche Darstellung zu ermöglichen, während in den Simulationen die quadratische Vergabe des \emph{base reward} benutzt wird.

\begin{figure}[htbp]
\centerline{	
\includegraphics{positive_negative_reward.eps}
}
\caption[Schematische Darstellung der Verteilung des \emph{reward} an \emph{action set} Listen] {Schematische Darstellung der (quadratischen) Verteilung des \emph{reward} an gespeicherte \emph{action set} Listen bei einem positiven bzw. negativen Ereignis}
\label{positive_negative_reward:fig}
\end{figure}




\subsection{Ereignisse}\label{sec:events}

In XCS wird lediglich das jeweils letzte \emph{action set} Liste aus dem vorherigen Schritt gespeichert, in der neuen Implementierung werden dagegen eine ganze Anzahl (bis zum Wert \emph{maxStackSize}) von \emph{action set} Listen gespeichert. Die Speicherung erlaubt zum einen eine Vorverarbeitung des \emph{reward} anhand der vergangenen Schritte und auf Basis einer größeren Zahl von \emph{action set} Listen und zum anderen die zeitliche Relativierung einer \emph{action set} Liste zu einem Ereignis. Die \emph{classifier} werden dann jeweils rückwirkend anhand des jeweiligen \emph{reward} Werts aktualisiert, sobald bestimmte Bedingungen eingetreten sind.\\

Von einem positiven bzw. negativen Ereignis spricht man, wenn sich der \emph{base reward} im Vergleich zum vorangegangenen Schritt verändert hat, also wenn das Zielobjekt sich in Überwachungsreichweite bzw. aus ihr heraus bewegt hat (siehe Abbildung~\ref{saved_rewards:fig}).\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{saved_rewards.eps}
}
\caption[Schematische Darstellung der zeitlichen Verteilung des \emph{reward} an und der Speicherung von \emph{action set} Listen] {Schematische Darstellung der zeitlichen Verteilung des \emph{reward} an \emph{action set} Listen nach mehreren positiven und negativen Ereignissen und der Speicherung der letzten \emph{action set} Liste}
\label{saved_rewards:fig}
\end{figure}

Bei der Benutzung eines solchen Stacks entsteht eine Zeitverzögerung, d.h. die \emph{classifier} erhalten jeweils Information, die bis zu \emph{maxStackSize} Schritte zu alt sein kann. Tritt beim Stack ein Überlauf ein (gab es also \emph{maxStackSize} Schritte lang keine Änderung des \emph{base reward} Werts) dann wird abgebrochen und die \(\frac{maxStackSize}{2}\) ältesten Einträge vom Stack genommen.\\

Alle diese Einträge werden vorher dabei mit diesem \emph{base reward} Wert aktualisiert. Abbildung~\ref{neutral_reward:fig} zeigt die Bewertung bei einem solchen neutralen Ereignis, bei dem nach Überlauf die erste Hälfte mit \(1\) bewertet wurde. Außerdem ist dort der maximale Fehler dargestellt, welcher eintreten würde, wenn direkt beim Schritt nach dem Abbruch eine Änderung des \emph{base reward} Werts auftritt, im dargestellten Fall also der \emph{base reward} sich beim aktuellen Zeitpunkt auf \(0\) verändern würde.\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{neutral_reward.eps}
}
\caption[Schematische Darstellung der Bewertung von \emph{action set} Listen bei einem neutralen Ereignis] {Schematische Darstellung der Bewertung von \emph{action set} Listen bei einem neutralen Ereignis (mit \emph{base reward} = 1)}
\label{neutral_reward:fig}
\end{figure}

\begin{figure}[H]
\setbox0\vbox{\small
Ein Ereignis tritt auf, wenn:
\begin{itemize}
\item Änderung des \emph{base reward} Werts von 0 auf 1 (Zielobjekt war im letzten Schritt nicht in Überwachungsreichweite) \(\Rightarrow\) positives Ereignis
\item Änderung des \emph{base reward} Werts von 1 auf 0 (Zielobjekt war im letzten Schritt in Überwachungsreichweite) \(\Rightarrow\) negatives Ereignis
\item Überlauf des Stacks (kein positives oder negatives Ereignis in den letzten \emph{maxStackSize} Schritten), Zielobjekt ist in Überwachungsreichweite \(\Rightarrow\) neutrales Ereignis (mit \emph{base reward} = \(1\))
\item Überlauf des Stacks (kein positives oder negatives Ereignis in den letzten \emph{maxStackSize} Schritten), Zielobjekt ist nicht in Überwachungsreichweite \(\Rightarrow\) neutrales Ereignis (mit \emph{base reward} = \(0\))
\end{itemize}
}
\centerline{\fbox{\box0}}
\end{figure}


\subsection{Größe des Stacks (\emph{maxStackSize})}

Offen bleibt die Frage nach der Größe des Stacks. Mangels theoretischem Fundament muss man zwischen den drei wirkenden Faktoren einen Kompromiss finden. Erstens gibt es die Verzögerung zu Beginn eines Problems und insbesondere zu Beginn eines Experiments, es kann u.U. bis zu \(\frac{maxStackSize}{2}\) Schritte dauern, bis das erste Mal ein \emph{classifier} aktualisiert wird. Auch werden bei einem großen \emph{maxStackSize} Wert womöglich Aktionen positiv (oder negativ) bewertet, die an der Situation nicht beteiligt waren, vor allem wenn es sich um kurze lokale Entscheidungen handelt. Umgekehrt, wählt man den Stack zu klein, kann es sein, dass ein Überlauf und somit u.U. ein gewisser Fehler auftritt. Der Wert \emph{maxStackSize} stellt also einen Kompromiss zwischen Zeitverzögerung bzw. Reaktionsgeschwindigkeit und Genauigkeit dar.\\

Wie Abbildung~\ref{max_stack_size:fig} zeigt, ist dies bei größerer Schrittzahl (2000 Schritte) aber vernachlässigbar, die erreichten Qualitäten unterscheiden sich im betrachteten Wertebereich kaum voneinander. Es gibt bei geringen Werten einen kleinen Anstieg, außerdem einen kleinen Abfall beim schwierigen Szenario. Da während der Entwicklung die meisten Tests mit dem Wert 128 durchgeführt wurden, wird dieser Wert belassen. Nur für das schwierige Szenario ist womöglich ein Wert von 64 vorzuziehen.\\

\begin{figure}[htbp]
\centerline{	
\includegraphics{max_stack_size.eps}
}
\caption[Vergleich verschiedener Werte für \emph{maxStackSize}] {Vergleich verschiedener Werte für \emph{maxStackSize} (2000 Schritte, SXCS Agenten)}
\label{max_stack_size:fig}
\end{figure}


\subsection{Implementierung von SXCS}\label{sxcs_implementation:sec}

Im Wesentlichen entspricht die Implementierung von SXCS der bekannten Implementierung von XCS (siehe Kapitel~\ref{standardxcs:sec}). Als Unterschiede sind festzuhalten:

\begin{itemize}

\item In der Funktion \emph{calculateReward()} in Programm~\ref{sxcs_calc_reward:pro} bei der Berechnung des \emph{reward} Werts wird zwischen zwei Fällen unterschieden. Zum einen gibt es die Behandlung negativer und positiver Ereignisse (Zeile 17-21) und zum anderen die Behandlung des Überlaufs des Stacks (Zeile 24-30), während bei der Implementierung von XCS in Programm~\ref{multistep_calc_reward:pro} in fast jedem Schritt unabhängig von Ereignissen eine Aktualisierung stattfindet. 

\item In der Funktion \emph{collectReward()} in Programm~\ref{sxcs_collect_reward:pro} werden nicht nur die aktuelle bzw. letzte \emph{action set} Liste aktualisiert, sondern eine ganze Reihe aus dem gespeicherten Stack. Insbesondere werden dort die auf- bzw. absteigenden \emph{reward} Werte nach einem positiven bzw. negativen Ereignis berechnet (Zeile 31-33). Bei der Berechnung der nächsten Aktion hingegen (Funktion \emph{calculateNextMove()} in Programm~\ref{sxcs_calc_move:pro}) wurde lediglich die Behandlung des Stacks hinzugefügt (Zeile 39-43).

\end{itemize}








