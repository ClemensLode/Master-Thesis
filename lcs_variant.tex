\chapter{XCS Varianten}\label{lcs_variants:cha}

TODO, evtl maxprediction komplett raus bei SXCS, testen!

TODO!!

Ziel der Arbeit war es, wie man den XCS Algorithmus auf ein Überwachungsszenario anwenden kann. Notwendig dafür war es, die XCS Implementierung vollständig nachzuvollziehen, um für jeden Bestandteil entscheiden zu können, welche Rolle es bezüglich eines solchen Szenarios spielt. Für die Tests wurde nicht auf bestehende Pakete (z.B. XCSlib~\cite{xcslib}) zurückgegriffen, wenn auch der Quelltext von \cite{Butz_xcsclassifier} Modell stand. 

Bild mit rückwirkender Rewardvergabe

Im Vordergrund stand zum einen die grundsätzliche Frage, ob XCS in einem solchen Szenario überhaupt besser als ein Algorithmus sein kann, der sich rein zufällig verhält und wie mögliche Ansätze aussehen können, den Algorithmus zu verbessern.

Der hier entwickelte Algorithmus muss primär nicht einen Weg zum Ziel erkennen, sondern eine möglichst optimale (und auch an andere Agenten angepasste) Verhaltensstrategie finden.


In Kapitel~\ref{cha:parameter} wurden mögliche Optimierungen zu den Parametern vorgestellt, in Kapitel~\ref{ablauf_lcs:sec} wurde diskutiert, in welcher Reihenfolge bei einem Multiagentensystem auf einem diskreten Torus die einzelnen Teile ausgeführt werden sollen.

Besonders die Verwaltung der Numerosity und die Verwendung des maxPrediction bereitete 

Das Multistepverfahren baut darauf auf, dass die Qualität der Agenten sich sukzessive mit jeder Probleminstanz verbessert, der Reward eben an immer weiter vom Ziel entfernte Aktionen TODO weitergereicht wird.


Da sich das Ziel schneller bewegt, kann eine einfache Verfolgungsstrategie nicht zum Erfolg führen. Eine einfache Implementation mit einem simplen Agenten der auf das Ziel zugeht, wenn es in Sicht ist und sich sonst wie ein sich zufällig bewegender Agent verhält, schneidet grundsätzlich schlechter ab.

TODO!

\section{Allgemeine Anpassungen und Verbesserungen}

\subsection{Verschiedenes, Numerosity, TODO}

Durch die Benutzung von \emph{macro classifiers} ergibt sich allerdings das programmiertechnische Problem, dass man nicht mehr direkt weiß, wieviele \emph{micro classifiers} sich in einer Population befinden, bei jeder Benutzung des Werts der Populationsgröße müssten die \emph{numerosity} Werte aller \emph{classifiers} jedes Mal addiert werden. In der Standardimplementierung \cite{Butz_xcsclassifier} ist die Behandlung des \emph{numerosity} Werts deswegen stark optimiert, jedes \emph{classifier set} trägt eine temporäre Variable \emph{numerositySum} mit sich, in der die aktuelle Summe gespeichert ist. Die Aktualisierung ist jedoch zum einen mangelhaft umgesetzt, zum anderen auf die Verwendung von einer einzelnen \emph{action set} Liste optimiert, während die hier verwendete Implementierung jeweils mit bis über 100 \emph{action set} Listen programmiert wurde, denen ein \emph{classifier} Mitglied sein kann. Deswegen wurde die Optimierung entfernt und durch eine dezentrale Verwaltung mit einem \emph{Observer} ersetzt, jede Änderung des \emph{numerosity} Wertes hat also die Änderung aller \emph{action set} Listen zur Folge, in der der \emph{classifier} Mitglied ist.\\

Wird also z.B. ein \emph{micro classifier} entfernt, dann wird lediglich die Änderungsfunktion des \emph{classifiers} aufgerufen, der dann wiederum den \emph{numerositySum} Wert der jeweiligen Eltern anpasst. Dies macht einige Optimierungen rückgängig, erspart aber sehr viel Umstände, den \emph{numerositySum} der Eltern immer auf den aktuellen Stand zu halten und einzelne \emph{classifiers} zu löschen.\\

Positiver Nebeneffekt durch die verbesserte Struktur ist, dass man dadurch leicht auf die Menge der \emph{action set} Listen zugreifen kann, denen ein \emph{classifier} angehört, hierfür wurde aber im Rahmen dieser Arbeit keine Verwendung gefunden.\\

Ein weiteres Problem der Standardimplementierung ist, dass der \emph{fitness} Wert eines \emph{classifiers} als Optimierung bereits den \emph{numerosity} Wert als Faktor enthält, während bei der Aktualisierung des \emph{numerosity} Werts der \emph{fitness} Wert nicht aktualisiert wurde. Das hat zur Folge, dass theoretisch \emph{fitness} Werte von \emph{classifiers} fast den \emph{max population} Wert annehmen kann, wenn ein \emph{classifier} mit \emph{numerosity} und \emph{fitness} Wert in der Höhe von \emph{max population} auf einen \emph{numerosity} Wert von 1 reduziert wird.\\
Dies betrifft die Funktion \emph{public void addNumerosity(int num)} der Klasse \emph{XClassifier} in der Datei \emph{XClassifier.java}. Die korrigierte Fassung ist in Programm~\ref{corrected_numerosity_function:pro} gelistet, ein Vergleich der Qualität, mit und ohne Korrektur, ist in Abbildung~\ref{correct_numerosity_graph:fig} dargestellt.

\newlisting{Korrigierte Version der \emph{addNumerosity()} Funktion}{corrected_numerosity_function:pro}
/**
 * Adds to the numerosity of the classifier.
 * @param num The added numerosity (can be negative!).
 */
  public void addNumerosity(int num) {
    int old_num = numerosity;
   
    numerosity += num;

  /**
   * Korrektur der fitness
   */
    fitness = fitness * (double)numerosity / (double)old_num;

  /**
   * Aktualisierung der Eltern
   */
    for (ClassifierSet p : parents) {
      p.changeNumerositySum(num);
      if (numerosity == 0) {
        p.removeClassifier(this);
      }
    }
  }
\end{lstlisting}

TODO Vergleich!
TODO wenig Unterschied sensoragent, evtl wieder raus



\section{Standard XCS Multistepverfahren}\label{standardxcs:sec}

Idee dieses Verfahrens ist, dass der \emph{reward} Wert, den eine Aktion (bzw. der jeweils zugehörigen \emph{action set} Liste und die dortigen \emph{classifier}) erhält, vom erwarteten \emph{reward} Wert der folgenden Aktion abhängt. Somit wird, rückführend vom letzten Schritt auf das Ziel, der \emph{reward} Wert schrittweise an vorgehende Aktionen verteilt, mit der Annahme, dass dann, durch mehrfache Wiederholung des Lernprozesses, mit dem sich dadurch ergebenen Regelsatz mit höherer Wahrscheinlichkeit das Ziel gefunden wird.\\

Kern des Verfahrens ist die Vergabe des \emph{base rewards}. Wird das Ziel erreicht, d.h. erhält der Algorithmus einen positiven \emph{base reward} Wert, so wird der \emph{reward} \(1.0\) an die letzte \emph{action set} Liste gegeben. Liegt kein positiver \emph{base reward} Wert vor, so wird lediglich der für diesen Schritt erwartete \emph{reward} Wert (nämlich der \emph{maxPrediction} Wert) an die letzte \emph{action set} Liste gegeben.\\

Als Vergleich wurde das bekannte Verfahren \cite{butz01algorithmic} fast unverändert übernommen. Der wesentliche Unterschied ist, dass das Szenario bei einem positiven \emph{base reward} nicht neugestartet wird, algorithmisch ist die Implementierung ansonsten identisch. Außerdem, wie schon in Kapitel~\ref{exploreexploit:sec} erwähnt, soll die Qualität des Algorithmus nicht nur in der \emph{exploit} Phase gemessen werden, da ein fortlaufendes Problem und kein statisches Szenario betrachtet wird. Schließlich sollen, neben den Parametereinstellungen im Kapitel~\ref{cha:parameter}, feste anstatt zufällige Schnittpunkte für das \emph{two point crossover} beim genetischen Operator (siehe Kapitel~\ref{genetische_operatoren:sec}) verwendet werden.\\

\newlisting{Erstes Kernstück des Standard XCS Multistepverfahrens (\emph{calculateReward()}, Bestimmung des Rewards anhand der Sensordaten), angepasst an ein dynamisches Überwachungsszenario}{multistep_calc_reward:pro}
/**
 * Diese Funktion wird in jedem Schritt aufgerufen um den aktuellen
 * reward zu bestimmen, den besten Wert der ermittelten match set Liste
 * weiterzugeben und, bei aktuell positivem reward, die aktuelle
 * action set Liste zu belohnen.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateReward(final long gaTimestep) {
  /**
   * checkRewardPoints liefert "wahr" wenn sich das Zielobjekt in
   * Überwachungsreichweite befindet
   */
    boolean reward = checkRewardPoints();

    if(prevActionSet != null){
      collectReward(lastReward, lastMatchSet.getBestValue(), false);
      prevActionSet.evolutionaryAlgorithm(classifierSet, gaTimestep);
    }

    if(reward) {
      collectReward(reward, 0.0, true);
      lastActionSet.evolutionaryAlgorithm(classifierSet, gaTimestep);
      prevActionSet = null;
      return;
    }
    prevActionSet = lastActionSet;
    lastReward = reward;
  }
\end{lstlisting}

TODO Programm~\ref{multistep_collect_reward:pro}

\newlisting{Zweites Kernstück des Multistepverfahrens (\emph{collectReward()} - Verteilung des Rewards auf die \emph{action set} Listen), angepasst an ein dynamisches Überwachungsszenario}{multistep_collect_reward:pro}
/**
 * Diese Funktion verarbeitet den übergebenen Reward und gibt ihn an die
 * zugehörigen action set Listen weiter.
 *
 * @param reward Wahr wenn das Zielobjekt in Sicht war.
 * @param best_value Bester Wert des vorangegangenen action set Listen
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, d.h.
 *        einem positiven Reward, aufgerufen wurde
 */

  public void collectReward(boolean reward, 
                double best_value, boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;

  /**
   * Falls der Reward von einem Ereignis rührt, aktualisiere die 
   * aktuelle action set Liste und lösche das vorherige
   */
    if(is_event) {
      if(lastActionSet != null) {
        lastActionSet.updateReward(corrected_reward, best_value, factor);
        prevActionSet = null;
      }
    } 

  /**
   * Kein Ereignis, also nur die letzte action set Liste aktualisieren
   */
    else 
    {
      if(prevActionSet != null) {
        prevActionSet.updateReward(corrected_reward, best_value, factor);
      }
    }
  }
\end{lstlisting}

TODO Programm~\ref{multistep_calc_move:pro}

\newlisting{Drittes Kernstück des Multistepverfahrens (\emph{calculateNextMove()} - Auswahl der nächsten Aktion und Ermittlung der zugehörigen action set Liste), angepasst an ein dynamisches Überwachungsszenario}{multistep_calc_move:pro}
/**
 * Bestimmt die zum letzten bekannten Status passenden Classifier und
 * wählt aus dieser Menge eine Aktion. Außerdem wird das aktuelle 
 * ActionClassifierSet mithilfe der gewählten Aktion ermittelt.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateNextMove(long gaTimestep) {

 /**
  * Überdecke das classifierSet mit zum Status passenden Classifiern
  * welche insgesamt alle möglichen Aktionen abdecken.
  */
    classifierSet.coverAllValidActions(
                    lastState, getPosition(), gaTimestep);

 /**
  * Bestimme alle zum Status passenden Classifier.
  */
    lastMatchSet = new AppliedClassifierSet(lastState, classifierSet);

 /**
  * Entscheide auf welche Weise die Aktion ausgewählt werden soll.
  */
    lastExplore = checkIfExplore(lastState.getSensorGoalAgent(),
                                           lastExplore, gaTimestep);

 /**
  * Wähle Aktion und bestimme zugehörige action set Liste
  */
    calculatedAction = lastMatchSet.chooseAbsoluteDirection(lastExplore);
    lastActionSet = new ActionClassifierSet(lastState, lastMatchSet,
                                                      calculatedAction);
  }
\end{lstlisting}



\section{XCS Variante für Überwachungsszenarien (SXCS)}

Die Hypothese bei der Aufstellung dieser Variante des XCS-Algorithmus ist im Grunde dieselbe wie beim XCS-Multistepverfahren, nämlich dass die Kombination mehrerer Aktionen zum Ziel führt. Beim Multistepverfahren besteht die wesentliche Verbindung zwischen den \emph{action set} Listen jeweils nur zwischen zwei direkt aufeinanderfolgenden \emph{action set} Listen über den \emph{maxPrediction} Wert. In einer statischen Umgebung kann dadurch über mehrere (identische) Probleme hinweg eine optimale Einstellung (der \emph{fitness} und der \emph{reward prediction} Wert) für die \emph{classifier} gefunden werden.\\
Bei der veränderten XCS Variante SXCS soll die Verbindung zwischen den \emph{action set} Listen zusätzlich direkt durch die zeitliche Nähe zum Ziel gegeben sein. Es wird in jedem Schritt die jeweilige \emph{action set} Liste gespeichert und aufgehoben, bis ein neues Ereignis (siehe Kapitel~\ref{sec:events}) eintritt und dann in Abhängigkeit des Alters mit einem entsprechenden \emph{reward} Wert aktualisiert.\\
\(r(a)\) bezeichnet den \emph{reward} Wert für die \emph{action set} Liste mit Alter \(a\).

Bei linearer Vergabe des \emph{reward}:
$$
r(a) = \left\{ \begin{array}{rl}
  \frac{a}{\mathrm{size(actionSet)}} &\mbox{, falls reward = $1$} \\
  \frac{1 - a}{\mathrm{size(actionSet)}} &\mbox{, falls reward = $0$}
       \end{array} \right.
$$

bzw. bei quadratischer Vergabe des \emph{reward}:

$$
r(a) = \left\{ \begin{array}{rl}
  \frac{{a}^{2}}{\mathrm{size(actionSet)}} &\mbox{ falls reward = $1$} \\
  \frac{{1 - a}^{2}}{\mathrm{size(actionSet)}} &\mbox{ falls reward = $0$}
       \end{array} \right.
$$

In Tests ergab sich für die quadratische Vergabe des \emph{reward} ein minimal besseres Ergebnis (TODO zeigen), weitere Grafiken werden auf die lineare Vergabe des \emph{reward} beschränkt sein, um eine verständliche Darstellung zu ermöglichen, während in den Simulationen die quadratische Vergabe des \emph{reward} benutzt wird.

\begin{figure}[htbp]
\centerline{	
\includegraphics{positive_negative_reward.eps}
}
\caption[Schematische Darstellung der Verteilung des \emph{reward} an \emph{action set} Listen] {Schematische Darstellung der (quadratischen) Verteilung des \emph{reward} an gespeicherte \emph{action set} Listen bei einem positiven bzw. negativen Ereignis}
\label{positive_negative_reward:fig}
\end{figure}

\subsection{Ereignisse}\label{sec:events}

TODO

In XCS wird lediglich das jeweils letzte \emph{action set} Liste aus dem vorherigen Zeitschritt gespeichert, in der neuen Implementierung werden dagegen eine ganze Anzahl (bis zum Wert \emph{maxStackSize}) von \emph{action set} Listen gespeichert. Die Speicherung erlaubt zum einen eine Vorverarbeitung des \emph{reward} anhand der vergangenen Zeitschritte und auf Basis einer größeren Zahl von \emph{action set} Listen und zum anderen die zeitliche Relativierung einer \emph{action set} Liste zu einem Ereignis. Die \emph{classifier} werden dann jeweils rückwirkend anhand des \emph{reward} Werts aktualisiert, sobald bestimmte Bedingungen eingetreten sind.\\

Von einem positiven bzw. negativen Ereignis spricht man, wenn sich der Reward im Vergleich zum vorangegangenen Zeitschritt verändert hat, also wenn das Zielobjekt sich in Übertragungsreichweite bzw. aus ihr heraus bewegt hat (siehe Abbildung~\ref{saved_rewards:fig}).\\

Bei der Benutzung eines solchen Stacks entsteht eine Zeitverzögerung, d.h. die \emph{classifier} besitzen jeweils Information, die bis zu \emph{maxStackSize} Schritte zu alt sind. Wählt man den Stack zu groß, nimmt die Konvergenzgeschwindigkeit und Reaktionsfähigkeit des Systems zu stark ab, wählt man ihn zu klein, kann es sein, dass ein Überlauf auftritt, also \emph{maxStackSize} Schritte lang keine Änderung des \emph{base reward} aufgetreten ist. Im letzteren Fall wird deswegen abgebrochen, die \emph{action set} Listen der ersten Hälfte des Stacks (also die \(\frac{maxStackSize}{2}\) ältesten Einträge) mit dem damals vergebenem konstanten \emph{base reward} Wert (welcher dem aktuellen \emph{base reward} Wert entspricht, es ist ja keine Änderung des \emph{base reward} eingetreten) aktualisiert und vom Stack genommen (siehe Abbildung~\ref{neutral_reward:fig}). Anschließend wird normal weiter verfahren bis der Stack wieder voll ist bzw. bis eine Rewardänderung auftritt. 
Das Szenario mit dem maximalen Fehler wäre das, bei dem ein Schritt nach des Abbruchs eine Rewardänderung auftritt. Der Wert \emph{maxStackSize} stellt also einen Kompromiss zwischen Zeitverzögerung bzw. Reaktionsgeschwindigkeit und Genauigkeit dar.

\begin{figure}[H]
\setbox0\vbox{\small
Ein Ereignis tritt auf, wenn:
\begin{enumerate}
\item Positive Rewardänderung (Zielobjekt war im letzten Zeitschritt nicht in Überwachungsreichweite) \(\Rightarrow\) positives Ereignis (mit reward = \(1\))
\item Negative Rewardänderung (Zielobjekt war im letzten Zeitschritt in Überwachungsreichweite) \(\Rightarrow\) negatives Ereignis (mit reward = \(0\))
\item Überlauf des Stacks (keine Rewardänderung in den letzten ``maxStackSize'' Schritten), Zielobjekt ist in Überwachungsreichweite \(\Rightarrow\) neutrales Ereignis (mit reward = \(1\))
\item Überlauf des Stacks (keine Rewardänderung in den letzten ``maxStackSize'' Schritten), Zielobjekt ist nicht in Überwachungsreichweite \(\Rightarrow\) neutrales Ereignis (mit reward = \(0\))
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}

\begin{figure}[htbp]
\centerline{	
\includegraphics{saved_rewards.eps}
}
\caption[Schematische Darstellung der zeitlichen Verteilung des \emph{reward} an und der Speicherung von \emph{action set} Listen] {Schematische Darstellung der zeitlichen Verteilung des \emph{reward} an \emph{action set} Listen nach mehreren positiven und negativen Ereignissen und der Speicherung der letzten \emph{action set} Liste}
\label{saved_rewards:fig}
\end{figure}

\begin{figure}[htbp]
\centerline{	
\includegraphics{neutral_reward.eps}
}
\caption[Schematische Darstellung der Verteilung des \emph{reward} an \emph{action set} Listen bei einem neutralen Ereignis] {Schematische Darstellung der Verteilung des \emph{reward} an \emph{action set} Listen bei einem neutralen Ereignis}
\label{neutral_reward:fig}
\end{figure}

\subsection{Implementierung von SXCS}\label{sxcs_implementation:sec}

TODO Erläuterung

TODO Programm~\ref{sxcs_calc_reward:pro}

\newlisting{Erstes Kernstück des SXCS-Algorithmus (\emph{calculateReward()}, Bestimmung des Rewards anhand der Sensordaten)}{sxcs_calc_reward:pro}
/**
 * Diese Funktion wird in jedem Schritt aufgerufen um den aktuellen
 * reward zu bestimmen und positive, negative und neutrale Ereignisse 
 * den besten Wert der ermittelten match set Liste weiterzugeben und, bei 
 * aktuell positivem reward, die aktuelle action set Liste zu belohnen.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateReward(final long gaTimestep) {
  /**
   * checkRewardPoints liefert "wahr" wenn sich der Zielobjekt in
   * Überwachungsreichweite befindet
   */
    boolean reward = checkRewardPoints();

    if (reward != lastReward) {
      int start_index = historicActionSet.size() - 1;
      collectReward(start_index, actionSetSize, reward, 1.0, true);
      actionSetSize = 0;
    }
    else 

    if(actionSetSize >= Configuration.getMaxStackSize())
    {
      int start_index = Configuration.getMaxStackSize() / 2;
      int length = actionSetSize - start_index;
      collectReward(start_index, length, reward, 1.0, false);
      actionSetSize = start_index;
    }

    lastReward = reward;
  }
\end{lstlisting}



TODO Programm~\ref{sxcs_collect_reward:pro}

\newlisting{Zweites Kernstück des SXCS-Algorithmus (\emph{collectReward()} - Verteilung des reward auf die action set Listen)}{sxcs_collect_reward:pro}
/**
 * Diese Funktion verarbeitet den übergebenen reward und gibt ihn an die
 * zugehörigen action set Listen weiter.
 *
 * @param reward Wahr wenn der Zielobjekt in Sicht war.
 * @param best_value Bester Wert des vorangegangenen action set Listen
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, d.h.
 *        einem positiven reward, aufgerufen wurde
 */

  public void collectReward(
                boolean reward, double best_value, boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;
  /**
   * Wenn es kein Event ist, dann gebe den Reward weiter wie beim 
   * Multistepverfahren
   */
    double max_prediction = is_event ? 0.0 : 
      historicActionSet.get(start_index+1).getMatchSet().getBestValue();

  /**
   * Aktualisiere eine ganze Anzahl von action set Listen
   */
    for(int i = 0; i < action_set_size; i++) {

  /**
   * Benutze aufsteigenden bzw. absteigenden Reward bei einem positiven 
   * bzw. negativen Ereignis
   */
      if(is_event) {
        corrected_reward = reward ? 
          calculateReward(i, action_set_size) : 
          calculateReward(action_set_size - i, action_set_size);
      }
  /**
   * Aktualisiere die action set Liste mit dem bestimmten reward und
   * gebe bei allen anderen action set Listen den reward weiter wie 
   * beim Multistepverfahren 
   */
      ActionClassifierSet action_classifier_set = 
        historicActionSet.get(start_index - i);
      action_classifier_set.updateReward(
        corrected_reward, max_prediction, factor);

      max_prediction = 
        action_classifier_set.getMatchSet().getBestValue();
    }
  }
\end{lstlisting}


TODO Programm~\ref{sxcs_calc_move:pro}

\newlisting{Drittes Kernstück des SXCS-Algorithmus (\emph{calculateNextMove()} - Auswahl der nächsten Aktion und Ermittlung und Speicherung der zugehörigen \emph{action set} Liste)}{sxcs_calc_move:pro}
/**
 * Bestimmt die zum letzten bekannten Status passenden classifier und
 * wählt aus dieser Menge eine Aktion. Außerdem wird die aktuelle 
 * action set Liste mithilfe der gewählten Aktion ermittelt.
 * Im Vergleich zum originalen multi step Verfahren wird am Schluss noch 
 * die ermittelte action set Liste gespeichert.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateNextMove(long gaTimestep) {

 /**
  * Überdecke das classifierSet mit zum Status passenden Classifiern
  * welche insgesamt alle möglichen Aktionen abdecken.
  */
    classifierSet.coverAllValidActions(
                    lastState, getPosition(), gaTimestep);

 /**
  * Bestimme alle zum Status passenden classifier.
  */
    lastMatchSet = new AppliedClassifierSet(lastState, classifierSet);

 /**
  * Entscheide auf welche Weise die Aktion ausgewählt werden soll,
  * wähle Aktion und bestimme zugehöriges action set Liste
  */
    lastExplore = checkIfExplore(lastState.getSensorGoalAgent(),
                                           lastExplore, gaTimestep);

    calculatedAction = lastMatchSet.chooseAbsoluteDirection(lastExplore);
    lastActionSet = new ActionClassifierSet(lastState, lastMatchSet,
                                                      calculatedAction);

 /**
  * Speichere die action set Liste und passe den Stack bei einem Überlauf an
  */
    actionSetSize++;
    historicActionSet.addLast(lastActionSet);
    if (historicActionSet.size() > Configuration.getMaxStackSize()) {
      historicActionSet.removeFirst();
    }
  }
\end{lstlisting}



\subsection{Zielobjekt mit XCS und SXCS}\label{variant_zielobjekt_xcs_sxcs:sec}

Wie bereits in Kapitel~\ref{zielobjekt_sxcs_einfuehrung:sec} erwähnt, soll hier eine Implementierung von XCS und SXCS für das Zielobjekt diskutiert werden. Der Grund für die Untersuchung liegt mehr darin, eine weitere Anwendungsmöglichkeit aufzuzeigen und XCS und SXCS nochmals zu vergleichen, anstatt konkrete neue Erkenntnisse zu gewinnen. Insbesondere handelt es sich hierbei nicht mehr um ein Multiagentensystem, obwohl auch mit weitere Zielobjekten sich kein kollaboratives Szenario ergeben würde, da die Zielobjekte keinen Vorteil von einer gemeinsamen Flucht haben (im Gegensatz zum gemeinsamen Einfangen seitens der Agenten). Die Ergebnisse der Analyse folgt in Kapitel~\ref{goal_agent_with_xcs:sec}, auch hier ist der Ansatz von SXCS überlegen.\\

Bis auf die Funktion \emph{checkRewardPoints()} (siehe Programm~\ref{agent_check_reward:pro}) ist die Implementierung für das Zielobjekt fast identisch. Die einzige zweite Änderung ist in der Funktion \emph{calculateNextMove()} (siehe Programm~\ref{multistep_calc_move:pro} (XCS) bzw. Programm~\ref{sxcs_calc_move:pro} (SXCS)), bei der die zusätzliche Sprungeigenschaft des Zielobjekts hinzugefügt ist (siehe Kapitel~\ref{base_properties_goal:sec}).\\

Die abgeänderte Version ist in Programm~\ref{goal_check_reward:pro} aufgelistet, sie unterscheidet sich in den Zeilen 6 und 9, in der Sensordaten über andere Agenten anstatt über das Zielobjekt geholt und geprüft werden, also ein Rollentausch zwischen Zielobjekt und Agenten stattfindet.\\

\newlisting{Bestimmung des \emph{base rewards} für das Zielobjekt}{goal_check_reward:pro}
    /**
     * @return true Falls das Zielobjekt von keinem Agenten überwacht wird
     */
    @Override
    public boolean checkRewardPoints() {
      boolean[] sensor_agent = lastState.getSensorAgent();

      for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
        if(sensor_agent[2*i+1]) {
          return false;
        }
      }

      return true;
    }
\end{lstlisting}
