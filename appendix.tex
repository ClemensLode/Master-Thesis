\begin{appendix}
\chapter{Implementation}\label{implementation:cha}

\section{Implementierung eines Problemablaufs}\label{implementierung_ablauf:sec}

In der Schleife der Funktion zur Berechnung eines Experiments (Programm~\ref{mainExperiment:pro}) wird die Funktion zur Berechnung des Problems (\emph{doOneMultiStepProblem()} in Programm~\ref{mainProblem:pro}) aufgerufen. Dort wird in einer weiteren Schleife über die Anzahl der maximalen Schritte die Sicht aktualisiert (\emph{updateSight()}), die Qualität bestimmt (\emph{updateStatistics()}), die neuen Sensordaten und die nächste Aktion ermittelt (\emph{calculateAgents()}, siehe Programm~\ref{mainCalculate:pro}), der \emph{reward} Wert ermittelt (\emph{rewardAgents()}, siehe Programm~\ref{mainReward:pro}) und schließlich werden die Objekte bewegt (\emph{moveAgents()}, siehe Programm~\ref{mainMove:pro}). Die konkrete Umsetzung der dort aufgerufenen Funktionen (insbesondere \emph{calculateNextMove()} und \emph{calculateReward()}) wird im Kapitel~\ref{lcs_variants:cha} erläutert (bzw. in Kapitel~\ref{agents:cha}, was die Heuristiken betrifft, wobei \emph{calculateReward()} dort keine Rolle spielt und eine leere Funktion aufgerufen wird).

\newlisting{Zentrale Schleife für einzelne Experimente}{mainExperiment:pro}
/**
 * Führt eine Anzahl von Problemen aus
 * @param experiment_nr Nummer des auszuführenden Experiments
 */
  public void doOneMultiStepExperiment(int experiment_nr) {
    int currentTimestep = 0;

  /**
   * number of problems for the same population
   */
    for (int i = 0; i < Configuration.getNumberOfProblems(); i++) {

    /**
     * Initialisierung des neuen "Random Seed" Wert
     */
      Misc.initSeed(Configuration.getRandomSeed() + 
        experiment_nr * Configuration.getNumberOfProblems() + i);

    /**
     * Erstellt einen neuen Torus und verteilt Agenten und 
     *   das Zielobjekt neu
     */
      BaseAgent.grid.resetState();

    /**
     * Führe Problem aus und aktualisiere aktuellen Zeitschritt
     */
      currentTimestep = doOneMultiStepProblem(currentTimestep);
    }
  }
\end{lstlisting}

\newlisting{Zentrale Schleife für einzelne Probleme}{mainProblem:pro}
/**
 * Führt eine Anzahl von Schritten auf dem aktuellen Torus aus
 * @param stepCounter Aktuelle Zeitschritt
 * @return Der Zeitschritt nach der Ausführung
 */
  private int doOneMultiStepProblem(int stepCounter) {
  /**
   * Zeitpunkt bis zu dem das Problem ausgeführt wird
   */
    int steps_next_problem = 
      Configuration.getNumberOfSteps() + stepCounter;    
    for (int currentTimestep = stepCounter; 
         currentTimestep < steps_next_problem; currentTimestep++) {

    /**
     * Ermittle die Sichtbarkeit und erhebe Statistiken
     */
      BaseAgent.grid.updateSight();
      BaseAgent.grid.updateStatistics(currentTimestep);

    /**
     * Ermittle neue Sensordaten und berechne Aktionen der Agenten
     */
      calculateAgents(currentTimestep);

    /**
     * Ermittle den Reward für alle Agenten (nach dem ersten Schritt)
     */
      if(currentTimestep > stepCounter) {
        rewardAgents(currentTimestep);
      }

    /**
     * Führe zuvor berechnete Aktionen aus
     */
      moveAgents();
    }

    /**
     * Abschließende Ermittlung des Rewards
     */
    BaseAgent.grid.updateSight();
    rewardAgents(steps_next_problem);
    return steps_next_problem;
  }
\end{lstlisting}





\newlisting{Zentrale Bearbeitung (Sensordaten und Berechnung der neuen Aktion) aller Agenten und des Zielobjekts innerhalb eines Problems}{mainCalculate:pro}
/**
 * Berechnet die Aktionen und führt sie in zufälliger Reihenfolge aus
 * @param gaTimestep der aktuelle Zeitschritt
 */
  private void calculateAgents(final long gaTimestep) {

  /**
   * Ermittle Sensordaten und bestimme nächste Bewegung
   */
    for(BaseAgent a : agentList) {
      a.aquireNewSensorData();
      a.calculateNextMove(gaTimestep);
    }
    BaseAgent.goalAgent.aquireNewSensorData();
    BaseAgent.goalAgent.calculateNextMove(gaTimestep);
  }
\end{lstlisting}



\newlisting{Zentrale Bearbeitung (Verteilung des \emph{reward} Werts) aller Agenten und des Zielobjekts innerhalb eines Problems}{mainReward:pro}
/**
 * Verteilt den Reward an alle Agenten
 */
  private void rewardAgents(final long gaTimestep) {
    for(BaseAgent a : agentList) {
      a.calculateReward(gaTimestep);
    }
    BaseAgent.goalAgent.calculateReward(gaTimestep);
  }
\end{lstlisting}



\newlisting{Zentrale Bearbeitung (Ausführung der Bewegung) aller Agenten und des Zielobjekts innerhalb eines Problems}{mainMove:pro}
/**
 * Berechnet die Aktionen und führt sie in zufälliger Reihenfolge aus
 * @param gaTimestep der aktuelle Zeitschritt
 */
  private void moveAgents(long gaTimestep) {
  /**
   * Erstelle Ausführungsliste für alle Objekte (Zielobjekt mehrfach)
   */
    int goal_speed = Configuration.getGoalAgentMovementSpeed();
    ArrayList<BaseAgent> random_list = 
      new ArrayList<BaseAgent>(agentList.size() + goal_speed);

    random_list.addAll(agentList);
    for(int i = 0; i < goal_speed; i++) {
      random_list.add(BaseAgent.goalAgent);
    }

  /**
   * Führe die ermittelten Aktionen in zufälliger Reihenfolge aus
   *   (Zielobjekt kann mehrfach ausgeführt werden).
   */
    int[] array = Misc.getRandomArray(random_list.size());
    for(int i = 0; i < array.length; i++) {
      BaseAgent a = random_list.get(array[i]);
      a.doNextMove();
      if(a.isGoalAgent() && goal_speed > 1) {
        goal_speed--;
        a.aquireNewSensorData();
        a.calculateNextMove(gaTimestep);
        a.calculateReward(gaTimestep);
      }
    }
  }
\end{lstlisting}

\clearpage



\section{Typen von Agentenbewegungen}

\newlisting{Berechnung der nächsten Aktion bei der Benutzung des Algorithmus mit zufälliger Bewegung}{calculateNextMoveRandomAlgorithm:pro}
/**
 * Berechne nächste Aktion (zufälliger Algorithmus)
 */
  private void calculateNextMove() {
  /**
   * Wähle zufällige Richtung als nächste Aktion
   */
    calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);
  }
\end{lstlisting}


\newlisting{Berechnung der nächsten Aktion bei der Benutzung der einfachen Heuristik}{calculateNextMove_SimpleHeuristic:pro}
/**
 * Berechne nächste Aktion (einfache Heuristik)
 */
  private void calculateNextMove() {
  /**
   * Holt sich die Informationen der Gruppe der Sensoren, die auf 
   * das Zielobjekt ausgerichtet sind
   */
    boolean[] goal_sensor = lastState.getSensorGoal();
    calculatedAction = -1;
    for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
    /**
     * Zielagent in Sicht in dieser Richtung?
     */
      if(goal_sensor[2*i]) {
        calculatedAction = i;
        break;
      }
    }

  /**
   * Sonst wähle zufällige Richtung als nächste Aktion
   */
    if(calculatedAction == -1) {
      calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);
    }      

  }
\end{lstlisting}

\newlisting{Berechnung der nächsten Aktion bei der Benutzung der intelligenten Heuristik}{calculateNextMove_IntelligentHeuristic:pro}
/**
 * Berechne nächste Aktion (intelligente Heuristik)
 */
private void calculateNextMove() {
  /**
   * Holt sich die Informationen der Gruppe der Sensoren, die auf 
   * das Zielobjekt ausgerichtet sind
   */
    boolean[] goal_sensor = lastState.getSensorGoal();

    calculatedAction = -1;
    for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
    /**
     * Zielagent in Sicht in dieser Richtung?
     */
      if(goal_sensor[2*i]) {
        calculatedAction = i;
        break;
      }
    }

  /**
   * Zielobjekt nicht in Sicht? Dann bewege von Agenten weg
   */
    if(calculatedAction == -1) {
      calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);

      boolean[] agent_sensors = lastState.getSensorAgent();
      boolean one_free = false;
      for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
        if(!agent_sensors[2*i]) {
          one_free = true;
          break;
        }
      }

      if(one_free) {
        while(agent_sensors[2*calculatedAction]) {
          calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);
        }
      }
    } 
  }
\end{lstlisting}

\clearpage



\section{Korrigierte \emph{addNumerosity()} Funktion}\label{corrected_numerosity_function:sec}

Durch die Benutzung von \emph{macro classifier} ergibt sich das programmiertechnische Problem, dass man nicht mehr direkt weiß, wieviele \emph{micro classifier} sich in einer Population befinden, bei jeder Benutzung des Werts der Populationsgröße müssten die \emph{numerosity} Werte aller \emph{classifier} jedes Mal addiert werden. In der Standardimplementierung \cite{Butz_xcsclassifier} ist die Behandlung des \emph{numerosity} Werts deswegen stark optimiert, jedes \emph{classifier set} trägt eine temporäre Variable \emph{numerositySum} mit sich, in der die aktuelle Summe gespeichert ist. Die Aktualisierung ist jedoch zum einen mangelhaft umgesetzt, zum anderen auf die Verwendung von einer einzelnen \emph{action set} Liste optimiert, während die hier verwendete Implementierung jeweils mit bis über 100 \emph{action set} Listen programmiert wurde, denen ein \emph{classifier} Mitglied sein kann. Deswegen wurde die Optimierung entfernt und durch eine dezentrale Verwaltung mit einem \emph{Observer} ersetzt, jede Änderung des \emph{numerosity} Wertes hat also die Änderung aller \emph{action set} Listen zur Folge, in der der \emph{classifier} Mitglied ist.\\

Wird also z.B. ein \emph{micro classifier} entfernt, dann wird lediglich die Änderungsfunktion des \emph{classifiers} aufgerufen, der dann wiederum den \emph{numerositySum} Wert der jeweiligen Eltern anpasst. Dies macht einige Optimierungen rückgängig, erspart aber sehr viel Umstände, den \emph{numerositySum} der Eltern immer auf den aktuellen Stand zu halten und einzelne \emph{classifiers} zu löschen.\\

Positiver Nebeneffekt durch die verbesserte Struktur ist, dass man dadurch leicht auf die Menge der \emph{action set} Listen zugreifen kann, denen ein \emph{classifier} angehört, hierfür wurde aber im Rahmen dieser Arbeit keine Verwendung gefunden.\\

Ein weiteres Problem der Standardimplementierung ist, dass der \emph{fitness} Wert eines \emph{classifiers} als Optimierung bereits den \emph{numerosity} Wert als Faktor enthält, während bei der Aktualisierung des \emph{numerosity} Werts der \emph{fitness} Wert nicht aktualisiert wurde. Das hat zur Folge, dass theoretisch \emph{fitness} Werte von \emph{classifiers} fast den \emph{max population} Wert annehmen kann, wenn ein \emph{classifier} mit \emph{numerosity} und \emph{fitness} Wert in der Höhe von \emph{max population} auf einen \emph{numerosity} Wert von \(1,0\) reduziert wird.\\
Dies betrifft die Funktion \emph{public void addNumerosity(int num)} der Klasse \emph{XClassifier} in der Datei \emph{XClassifier.java}. Die Korrektur besteht darin, den \emph{fitness} Wert mit dem Quotienten aus dem neuen durch den alten \emph{numerosity} Wert zu multiplizieren. Die korrigierte Fassung ist in Programm~\ref{corrected_numerosity_function:pro} dargestellt.\\

Möglicherweise kann man diesen Fehler durch Veränderung der Parameter oder längere Laufzeiten kompensieren, logisch betrachtet macht es aber keinen Sinn, dass beim Subsummieren bzw. Löschen eines \emph{micro classifier} der \emph{fitness} Wert verändert wird. In Tests haben sich nur minimale Unterschiede ergeben. Beispielsweise ergab sich (auf dem Säulenszenario mit 8 Agenten mit SXCS und einem Zielobjekt mit einfacher Richtungsänderung) eine Qualität von \(39,15\%\) im Vergleich zur originalen Implementierung von \(39,95\%\) bei 500 Schritten bzw. \(35,42\%\) zu \(35,01\%\) bei 2000 Schritten. Der Fehler scheint sich also eher langfristig auszuwirken, wenn auch der Unterschied so klein ist, dass man ihn vernachlässigen kann. Problematisch wird es, wenn Modifikationen von XCS darauf aufbauen, dass der \emph{fitness} Wert für jeden \emph{micro classifier} immer kleiner gleich \emph{1.0} ist.\\

Alles in allem betrachtet soll im Rahmen dieser Arbeit soll die korrigierte Fassung benutzt werden.

\newlisting{Korrigierte Version der \emph{addNumerosity()} Funktion}{corrected_numerosity_function:pro}
/**
 * Erhöht oder erniedrigt den numerosity Wert des classifier
 * @param num Der zur numerosity zu addierende Wert (kann negativ sein).
 */
  public void addNumerosity(int num) {
    int old_num = numerosity;
   
    numerosity += num;

  /**
   * Korrektur der fitness
   */
    if(old_num > 0) {
      fitness = fitness * (double)numerosity / (double)old_num;
    } else {
      fitness = Configuration.
    }

  /**
   * Aktualisierung der Eltern
   */
    for (ClassifierSet p : parents) {
      p.changeNumerositySum(num);
      if (numerosity == 0) {
        p.removeClassifier(this);
      }
    }
  }
\end{lstlisting}


\clearpage

\section{Implementierung XCS Multistepverfahrens }

\newlisting{Erstes Kernstück des Standard XCS Multistepverfahrens (\emph{calculateReward()}, Bestimmung und Verarbeitung des \emph{reward} Werts anhand der Sensordaten), angepasst an ein dynamisches Überwachungsszenario, bei positivem \emph{reward} Wert wird nicht abgebrochen}{multistep_calc_reward:pro}
/**
 * Diese Funktion wird in jedem Schritt aufgerufen um den aktuellen
 * reward zu bestimmen, den besten Wert der ermittelten match set Liste
 * weiterzugeben und, bei aktuell positivem reward, die aktuelle
 * action set Liste zu belohnen.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateReward(final long gaTimestep) {
  /**
   * checkRewardPoints liefert "wahr" wenn sich das Zielobjekt in
   * Überwachungsreichweite befindet
   */
    boolean reward = checkRewardPoints();

    if(prevActionSet != null){
      collectReward(lastReward, lastMatchSet.getBestValue(), false);
      prevActionSet.evolutionaryAlgorithm(classifierSet, gaTimestep);
    }

    if(reward) {
      collectReward(reward, 0.0, true);
      lastActionSet.evolutionaryAlgorithm(classifierSet, gaTimestep);
      prevActionSet = null;
      return;
    }
    prevActionSet = lastActionSet;
    lastReward = reward;
  }
\end{lstlisting}


\newlisting{Zweites Kernstück des XCS \emph{multi step} Verfahrens (\emph{collectReward()} - Verteilung des \emph{reward} Werts auf die \emph{action set} Listen), angepasst an ein dynamisches Überwachungsszenario}{multistep_collect_reward:pro}
/**
 * Diese Funktion verarbeitet den übergebenen reward Wert und 
 * gibt ihn an die zugehörigen action set Listen weiter.
 *
 * @param reward Wahr wenn das Zielobjekt in Sicht war.
 * @param best_value Bester Wert des vorangegangenen action set Listen
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, 
 *          d.h. einem positiven reward Wert, aufgerufen wurde
 */

  public void collectReward(boolean reward, 
                double best_value, boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;

  /**
   * Falls der reward Wert von einem Ereignis rührt, aktualisiere die 
   * aktuelle action set Liste und lösche das vorherige
   */
    if(is_event) {
      if(lastActionSet != null) {
        lastActionSet.updateReward(corrected_reward, best_value, factor);
        prevActionSet = null;
      }
    } 

  /**
   * Kein Ereignis, also nur die letzte action set Liste aktualisieren
   */
    else 
    {
      if(prevActionSet != null) {
        prevActionSet.updateReward(corrected_reward, best_value, factor);
      }
    }
  }
\end{lstlisting}



\newlisting{Drittes Kernstück des XCS \emph{multi step} Verfahrens (\emph{calculateNextMove()}, Auswahl der nächsten Aktion und Ermittlung der zugehörigen \emph{action set} Liste), angepasst an ein dynamisches Überwachungsszenario}{multistep_calc_move:pro}
/**
 * Bestimmt die zum letzten bekannten Status passenden classifier und
 * wählt aus dieser Menge eine Aktion. Außerdem wird die aktuelle 
 * action set Liste mithilfe der gewählten Aktion ermittelt.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateNextMove(long gaTimestep) {

 /**
  * Überdecke das classifierSet mit zum Status passenden Classifiern
  * welche insgesamt alle möglichen Aktionen abdecken.
  */
    classifierSet.coverAllValidActions(
                    lastState, getPosition(), gaTimestep);

 /**
  * Bestimme alle zum Status passenden Classifier.
  */
    lastMatchSet = new AppliedClassifierSet(lastState, classifierSet);

 /**
  * Entscheide auf welche Weise die Aktion ausgewählt werden soll.
  */
    lastExplore = checkIfExplore(lastState.getSensorGoalAgent(),
                                           lastExplore, gaTimestep);

 /**
  * Wähle Aktion und bestimme zugehörige action set Liste
  */
    calculatedAction = lastMatchSet.chooseAbsoluteDirection(lastExplore);
    lastActionSet = new ActionClassifierSet(lastState, lastMatchSet,
                                                      calculatedAction);
  }
\end{lstlisting}



\clearpage

\section{Implementierung des SXCS Verfahrens}

\newlisting{Erstes Kernstück des SXCS-Algorithmus (\emph{calculateReward()}, Bestimmung des \emph{reward} Werts anhand der Sensordaten)}{sxcs_calc_reward:pro}
/**
 * Diese Funktion wird in jedem Schritt aufgerufen um den aktuellen
 * reward Wert zu bestimmen und positive, negative und neutrale 
 * Ereignisse den besten Wert der ermittelten match set Liste 
 * weiterzugeben und, bei aktuell positivem reward Wert, die 
 * aktuelle action set Liste zu belohnen.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateReward(final long gaTimestep) {
  /**
   * checkRewardPoints() liefert "wahr" wenn sich das Zielobjekt in
   * Überwachungsreichweite befindet
   */
    boolean reward = checkRewardPoints();

    if (reward != lastReward) {
      int start_index = historicActionSet.size() - 1;
      collectReward(start_index, actionSetSize, reward, 1.0, true);
      actionSetSize = 0;
    }
    else 

    if(actionSetSize >= Configuration.getMaxStackSize())
    {
      int start_index = Configuration.getMaxStackSize() / 2;
      int length = actionSetSize - start_index;
      collectReward(start_index, length, reward, 1.0, false);
      actionSetSize = start_index;
    }

    lastReward = reward;
  }
\end{lstlisting}




\newlisting{Zweites Kernstück des SXCS-Algorithmus (\emph{collectReward()} - Verteilung des \emph{reward} Werts auf die \emph{action set} Listen)}{sxcs_collect_reward:pro}
/**
 * Diese Funktion verarbeitet den übergebenen reward und gibt ihn an 
 * die zugehörigen action set Listen weiter.
 *
 * @param reward Wahr wenn der Zielobjekt in Sicht war.
 * @param best_value Bester Wert des vorangegangenen action set Listen
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, 
 *          d.h. einem positiven reward Wert, aufgerufen wurde
 */

  public void collectReward(int start_index, int action_set_size, 
                boolean reward, double best_value, boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;
  /**
   * Keine Weitergabe des reward Werts wie bei XCS
   */
    double max_prediction = 0.0;

  /**
   * Aktualisiere eine ganze Anzahl von action set Listen
   */
    for(int i = 0; i < action_set_size; i++) {

  /**
   * Benutze aufsteigenden bzw. absteigenden reward Wert bei einem 
   * positiven bzw. negativen Ereignis
   */
      if(is_event) {
        corrected_reward = reward ? 
          calculateReward(i, action_set_size) : 
          calculateReward(action_set_size - i, 
action_set_size);
      }
  /**
   * Aktualisiere die action set Liste mit dem bestimmten reward Wert 
   * und gebe bei allen anderen action set Listen den reward Wert 
   * weiter wie beim multi step Verfahren 
   */
      ActionClassifierSet action_classifier_set = 
        historicActionSet.get(start_index - i);
      action_classifier_set.updateReward(
        corrected_reward, max_prediction, factor);
    }
  }
\end{lstlisting}



\newlisting{Drittes Kernstück des SXCS-Algorithmus (\emph{calculateNextMove()} - Auswahl der nächsten Aktion und Ermittlung und Speicherung der zugehörigen \emph{action set} Liste)}{sxcs_calc_move:pro}
/**
 * Bestimmt die zum letzten bekannten Status passenden classifier und
 * wählt aus dieser Menge eine Aktion. Außerdem wird die aktuelle 
 * action set Liste mithilfe der gewählten Aktion ermittelt.
 * Im Vergleich zum originalen multi step Verfahren wird am Schluss noch 
 * die ermittelte action set Liste gespeichert.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateNextMove(long gaTimestep) {

 /**
  * Überdecke das classifierSet mit zum Status passenden Classifiern
  * welche insgesamt alle möglichen Aktionen abdecken.
  */
    classifierSet.coverAllValidActions(
                    lastState, getPosition(), gaTimestep);

 /**
  * Bestimme alle zum Status passenden classifier.
  */
    lastMatchSet = new AppliedClassifierSet(lastState, classifierSet);

 /**
  * Entscheide auf welche Weise die Aktion ausgewählt werden soll,
  * wähle Aktion und bestimme zugehöriges action set Liste
  */
    lastExplore = checkIfExplore(lastState.getSensorGoalAgent(),
                                           lastExplore, gaTimestep);

    calculatedAction = lastMatchSet.chooseAbsoluteDirection(lastExplore);
    lastActionSet = new ActionClassifierSet(lastState, lastMatchSet,
                                                      calculatedAction);

 /**
  * Speichere die action set Liste, erhöhe die Größe des Stacks und
  * und passe den Stack bei einem Überlauf an
  */
    actionSetSize++;
    historicActionSet.addLast(lastActionSet);
    if (historicActionSet.size() > Configuration.getMaxStackSize()) {
      historicActionSet.removeFirst();
    }
  }
\end{lstlisting}

\clearpage


\section{Implementation des DSXCS Algorithmus}

\newlisting{Zweites Kernstück des verzögerten SXCS Algorithmus DSXCS (\emph{collectReward()} - Bewertung der \emph{action set} Listen)}{collect_reward_dsxcs:pro}
/**
 * Diese Funktion verarbeitet den übergebenen reward Wert und gibt ihn an 
 * die zugehörigen action set Listen weiter. Wesentlicher Unterschied zum 
 * SXCS Algorithmus ist, dass der maxPrediction Wert erst bei der 
 * endgültigen Verarbeitung des historicActionSet Listen ermittelt wird.
 *
 * @param reward Wahr wenn das Zielobjekt in Sicht war.
 * @param best_value Bester Wert des vorangegangenen action set Listen
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, 
 *          d.h. einem positiven reward Wert, aufgerufen wurde
 */

  public void collectReward(int start_index, int action_set_size, 
                boolean reward, double best_value, boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;

  /**
   * Aktualisiere eine ganze Anzahl von Einträgen im historicActionSet
   */
     for(int i = 0; i < action_set_size; i++) {

  /**
   * Benutze aufsteigenden bzw. absteigenden reward Wert bei 
   * einem positiven bzw. negativen Ereignis
   */
       if(is_event) {
         corrected_reward = reward ? 
           calculateReward(i, action_set_size) : 
           calculateReward(action_set_size - i, action_set_size);
       } else {
         if(corrected_reward == 1.0 && factor == 1.0) {
           historicActionSet.get(start_index - i).rewardPrematurely();
         }
       }

  /**
   * Füge den ermittelten reward Wert zur historicActionSet Liste
   */
       historicActionSet.get(start_index - i).
         addReward(corrected_reward, factor);

    }
  }
\end{lstlisting}


\newlisting{Auszug aus dem dritten Kernstück des verzögerten SXCS Algorithmus DSXCS (\emph{calculateNextMove()})}{dsxcs_calc_move:pro}

/**
 * Der erste Teil der Funktion ist identisch mit der calculateNextMove()
 * Funktion der SXCS Variante ohne Kommunikation. Der Zusatz ist, dass beim 
 * Überlauf die in der historicActionSet Liste gespeicherte reward Werte
 * verarbeitet werden.
 */

  public void calculateNextMove(long gaTimestep) {

  // ... 

  /**
   * historicActionSet voll? Dann verarbeite den dortigen reward Wert
   */
     if (historicActionSet.size() > Configuration.getMaxStackSize()) {
       historicActionSet.pop().processReward();
    }
  }
\end{lstlisting}


\newlisting{Viertes Kernstück des verzögerten SXCS Algorithmus DSXCS  (Verarbeitung des jeweiligen reward Werts, \emph{processReward()})}{process_reward_dsxcs1:pro}
/**
 * Zentrale Routine der historicActionSet Liste zur Verarbeitung aller 
 * eingegangenen reward Werte bis zu diesem Punkt.
 */

  public void processReward() {
  /**
   * Finde das größte reward / factor Paar
   */
    for(RewardHelper r : reward) {
    /**
     * Dieser Eintrag wurde schon in collectReward() verwertet
     */
      if(r.reward == 1.0 && r.factor == 1.0) {
        continue;
      }
    /**
     * Aktualisiere den Eintrag mit den entsprechenden Werten
     */
      actionClassifierSet.updateReward(r.reward, 0.0, r.factor);
    }
  }
\end{lstlisting}


\newlisting{Verbesserte Variante des vierten Kernstück des verzögerten SXCS Algorithmus DSXCS (Verarbeitung des reward Werts, \emph{processReward()})}{process_reward_dsxcs2:pro}
/**
 * Zentrale Routine der historicActionSet Liste zur Verarbeitung aller 
 * eingegangenen reward Wert bis zu diesem Punkt.
 */

  public void processReward() {

    double max_value = 0.0;
    double max_reward = 0.0;
  /**
   * Finde das größte reward / factor Paar
   */
    for(RewardHelper r : reward) {
    /**
     * Dieser Eintrag wurde schon in collectReward() verwertet
     */
      if(r.reward == 1.0 && r.factor == 1.0) {
        return;
      }
      
      if(r.reward * r.factor > max_value) {
        max_value = r.reward * r.factor;
        max_reward = r.reward;
      }
    }
    /**
     * Aktualisiere den Eintrag mit dem ermittelten Werten
     */
    actionClassifierSet.updateReward(max_reward, 0.0, 1.0);
  }
\end{lstlisting}

\clearpage


\section{Implementation des egoistischen \emph{reward}}

\newlisting{"'Egoistische Relation"', Algorithmus zur Bestimmung des Kommunikationsfaktors basierend auf dem erwarteten Verhalten des Agenten gegenüber anderen Agenten}{egoistic_relationship:pro}
/**
 * Ähnlichkeit dieser classifier set Liste zu der übergebenen Liste im
 * Hinblick auf die Wahrscheinlichkeit, auf andere Agenten zuzugehen
 * @param other Die andere Liste mit der verglichen werden soll
 * @return Grad der Ähnlichkeit bzw. der Kommunikationsfaktor (0,0 - 1,0)
 */
  public double checkEgoisticDegreeOfRelationship(
                    final MainClassifierSet other) {
    double ego_factor = getEgoisticFactor() - other.getEgoisticFactor();
    if(ego_factor == 0.0) {
      return 0.0;
    } else {
      return 1.0 - ego_factor * ego_factor;
    }
  }

  public double getEgoisticFactor() throws Exception {
    double factor = 0.0;
    double pred_sum = 0.0;
    for(Classifier c : getClassifiers()) {
      if(!c.isPossibleSubsumer()) {
        continue;
      }
      factor += c.getEgoFactor();
      pred_sum += c.getFitness() * c.getPrediction();
    }
    if(pred_sum > 0.0) {
      factor /= pred_sum;
    } else {
      factor = 0.0;
    }
    return factor;
  }
\end{lstlisting}



\end{appendix}
