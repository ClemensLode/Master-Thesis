\chapter{Implementierung}\label{implementation:cha}

Im Folgenden sind die Kernbestandteile aus dem Quelltext des Simulators aufgelistet. Sie dienen primär zum Verständnis der Idee und sind nur bedingt lauffähig. Der vollständige Quelltext ist auf der beiliegenden DVD bzw. unter~\cite{agentsimulator} verfügbar.\\


\section{Implementierung eines Problemablaufs}\label{implementierung_ablauf:sec}

In der Schleife der Funktion zur Berechnung eines Experiments (Programm~\ref{mainExperiment:pro}) wird die Funktion zur Berechnung einer Probleminstanz (\emph{doOneMultiStepProblem()} in Programm~\ref{mainProblem:pro}) aufgerufen. Dort wird in einer weiteren Schleife über die Anzahl der maximalen Schritte die Sicht aktualisiert (\emph{updateSight()}), die Qualität bestimmt (\emph{updateStatistics()}), die neuen Sensordaten und die nächste Aktion ermittelt (\emph{calculateAgents()}, siehe Programm~\ref{mainCalculate:pro}), der \emph{reward} Wert ermittelt (\emph{rewardAgents()}, siehe Programm~\ref{mainReward:pro}) und schließlich werden die Objekte bewegt (\emph{moveAgents()}, siehe Programm~\ref{mainMove:pro}). Die konkrete Umsetzung der dort aufgerufenen Funktionen (insbesondere \emph{calculateNextMove()} und \emph{calculateReward()}) wird im Kapitel~\ref{lcs_variants:cha} erläutert (bzw. in Kapitel~\ref{agents:cha}, was die Heuristiken betrifft, wobei \emph{calculateReward()} dort keine Rolle spielt und eine leere Funktion aufgerufen wird).\\

\newlisting{Zentrale Schleife für einzelne Experimente}{mainExperiment:pro}
/**
 * Führt eine Anzahl von Probleminstanzen aus.
 * @param experiment_nr Nummer des auszuführenden Experiments
 */
  public void doOneMultiStepExperiment(final int experiment_nr) {
    int currentTimestep = 0;

  /**
   * Durchlaufe Anzahl von Probleminstanzen für die selbe Population.
   */
    for (int i = 0; i < Configuration.getNumberOfProblems(); i++) {

    /**
     * Initialisierung des neuen "Random Seed" Wert
     */
      Misc.initSeed(Configuration.getRandomSeed() + 
        experiment_nr * Configuration.getNumberOfProblems() + i);

    /**
     * Erstellt einen neuen Torus und verteilt Agenten und 
     *   das Zielobjekt neu.
     */
      BaseAgent.grid.resetState();

    /**
     * Führe Probleminstanz aus und aktualisiere aktuellen Zeitschritt.
     */
      currentTimestep = doOneMultiStepProblem(currentTimestep);
    }
  }
\end{lstlisting}

\newlisting{Zentrale Schleife für einzelne Probleminstanzen}{mainProblem:pro}
/**
 * Führt eine Anzahl von Schritten auf dem aktuellen Torus aus.
 * @param stepCounter Der aktuelle Zeitschritt
 * @return Der Zeitschritt nach der Ausführung
 */
  private int doOneMultiStepProblem(final int stepCounter) {
  /**
   * Zeitpunkt bis zu dem das Problem ausgeführt wird
   */
    int steps_next_problem = 
      Configuration.getNumberOfSteps() + stepCounter;    
    for (int currentTimestep = stepCounter; 
         currentTimestep < steps_next_problem; currentTimestep++) {

    /**
     * Ermittle die Sichtbarkeit und erhebe Statistiken.
     */
      BaseAgent.grid.updateSight();
      BaseAgent.grid.updateStatistics(currentTimestep);

    /**
     * Ermittle neue Sensordaten und berechne Aktionen der Agenten.
     */
      calculateAgents(currentTimestep);

    /**
     * Ermittle den reward Wert für alle Agenten (nach dem ersten Schritt).
     */
      if(currentTimestep > stepCounter) {
        rewardAgents(currentTimestep);
      }

    /**
     * Führe zuvor berechnete Aktionen aus.
     */
      moveAgents();
    }

    /**
     * Abschließende Ermittlung des reward Werts
     */
    BaseAgent.grid.updateSight();
    rewardAgents(steps_next_problem);
    return steps_next_problem;
  }
\end{lstlisting}





\newlisting{Zentrale Bearbeitung (Sensordaten und Berechnung der neuen Aktion) aller Agenten und des Zielobjekts innerhalb einer Probleminstanz}{mainCalculate:pro}
/**
 * Berechnet die Aktionen und führt sie in zufälliger Reihenfolge aus.
 * @param gaTimestep Der aktuelle Zeitschritt
 */
  private void calculateAgents(final long gaTimestep) {

  /**
   * Ermittle Sensordaten und bestimme die nächste Bewegung.
   */
    for(BaseAgent a : agentList) {
      a.aquireNewSensorData();
      a.calculateNextMove(gaTimestep);
    }
    BaseAgent.goalAgent.aquireNewSensorData();
    BaseAgent.goalAgent.calculateNextMove(gaTimestep);
  }
\end{lstlisting}



\newlisting{Zentrale Bearbeitung (Verteilung des \emph{reward} Werts) aller Agenten und des Zielobjekts innerhalb einer Probleminstanz}{mainReward:pro}
/**
 * Rufe die Verarbeitungsfunktion für den reward Wert aller Agenten auf.
 */
  private void rewardAgents(final long gaTimestep) {
    for(BaseAgent a : agentList) {
      a.calculateReward(gaTimestep);
    }
    BaseAgent.goalAgent.calculateReward(gaTimestep);
  }
\end{lstlisting}



\newlisting{Zentrale Bearbeitung (Ausführung der Bewegung) aller Agenten und des Zielobjekts innerhalb einer Probleminstanz}{mainMove:pro}
/**
 * Berechnet die Aktionen und führt sie in zufälliger Reihenfolge aus.
 * @param gaTimestep Der aktuelle Zeitschritt
 */
  private void moveAgents(final long gaTimestep) {
  /**
   * Erstelle Ausführungsliste für alle Objekte (Zielobjekt mehrfach).
   */
    int goal_speed = Configuration.getGoalAgentMovementSpeed();
    ArrayList<BaseAgent> random_list = 
      new ArrayList<BaseAgent>(agentList.size() + goal_speed);

    random_list.addAll(agentList);
    for(int i = 0; i < goal_speed; i++) {
      random_list.add(BaseAgent.goalAgent);
    }

  /**
   * Führe die ermittelten Aktionen in zufälliger Reihenfolge aus. 
   * Das Zielobjekt kann dabei mehrfach ausgeführt werden.
   */
    int[] array = Misc.getRandomArray(random_list.size());
    for(int i = 0; i < array.length; i++) {
      BaseAgent a = random_list.get(array[i]);
      a.doNextMove();
      if(a.isGoalAgent() && goal_speed > 1) {
        goal_speed--;
        a.aquireNewSensorData();
        a.calculateNextMove(gaTimestep);
        a.calculateReward(gaTimestep);
      }
    }
  }
\end{lstlisting}

\clearpage



\section{Typen von Agentenbewegungen}

\newlisting{Berechnung der nächsten Aktion bei der Benutzung des Algorithmus mit zufälliger Bewegung}{calculateNextMoveRandomAlgorithm:pro}
/**
 * Berechne die nächste Aktion (zufälliger Algorithmus).
 */
  private void calculateNextMove() {
  /**
   * Wähle eine zufällige Richtung als nächste Aktion.
   */
    calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);
  }
\end{lstlisting}


\newlisting{Berechnung der nächsten Aktion bei der Benutzung der einfachen Heuristik}{calculateNextMove_SimpleHeuristic:pro}
/**
 * Berechne die nächste Aktion (einfache Heuristik).
 */
  private void calculateNextMove() {
  /**
   * Ermittlung der Informationen der Gruppe der Sensoren, die auf 
   * das Zielobjekt ausgerichtet sind
   */
    boolean[] goal_sensor = lastState.getSensorGoal();
    calculatedAction = -1;
    for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
    /**
     * Zielagent in Sicht in dieser Richtung?
     */
      if(goal_sensor[2*i]) {
        calculatedAction = i;
        break;
      }
    }

  /**
   * Sonst wähle zufällige Richtung als nächste Aktion.
   */
    if(calculatedAction == -1) {
      calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);
    }      

  }
\end{lstlisting}

\newlisting{Berechnung der nächsten Aktion bei der Benutzung der intelligenten Heuristik}{calculateNextMove_IntelligentHeuristic:pro}
/**
 * Berechne nächste Aktion (intelligente Heuristik).
 */
private void calculateNextMove() {
  /**
   * Ermittlung der Informationen der Gruppe der Sensoren, die auf 
   * das Zielobjekt ausgerichtet sind
   */
    boolean[] goal_sensor = lastState.getSensorGoal();

    calculatedAction = -1;
    for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
    /**
     * Zielagent in Sicht in dieser Richtung?
     */
      if(goal_sensor[2*i]) {
        calculatedAction = i;
        break;
      }
    }

  /**
   * Zielobjekt nicht in Sicht? Dann bewege von Agenten weg.
   */
    if(calculatedAction == -1) {
      calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);

      boolean[] agent_sensors = lastState.getSensorAgent();
      boolean one_free = false;
      for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
        if(!agent_sensors[2*i]) {
          one_free = true;
          break;
        }
      }

      if(one_free) {
        while(agent_sensors[2*calculatedAction]) {
          calculatedAction = Misc.nextInt(Action.MAX_DIRECTIONS);
        }
      }
    } 
  }
\end{lstlisting}

\clearpage




\section{Bewertungsfunktion}

\newlisting{Bewertungsfunktion für die XCS Varianten}{check_reward_points:pro}

/**
 * Berechnet den base reward Wert und gibt ihn zurück.
 * @return Den base reward Wert
 */
  public boolean checkRewardPoints() {
    if(lastState == null) {
      return false;
    }
  /**
   * Prüft die eigenen Sensordaten.
   */
    boolean[] sensor_agent = lastState.getSensorAgent();
    boolean[] sensor_goal = lastState.getSensorGoal();

  /**
   * Ziel in Sicht? Dann gebe positiven base reward Wert zurück.
   */
    for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
      if((sensor_goal[2*i])) {
        return true;
      }
    }

  /**
   * Kein Zielobjekt, aber Agent in Sicht? 
   * Dann gebe negativen base reward Wert zurück.
   */
    for(int i = 0; i < Action.MAX_DIRECTIONS; i++) {
      if(sensor_agent[2*i]) {
        return false;
      }
    }

  /**
   * Kein Zielobjekt und kein Agent in Sicht? 
   * Dann gebe positiven base reward Wert zurück.
   */
    return true;
  }

\end{lstlisting}

\clearpage

\section{Korrigierte \emph{addNumerosity()} Funktion}\label{corrected_numerosity_function:sec}

Durch die Benutzung von \emph{macro classifier} ergibt sich das programmiertechnische Problem, dass man nicht mehr direkt weiß, wieviele \emph{micro classifier} sich in einer Population befinden, bei jeder Benutzung des Werts der Populationsgröße müssten die \emph{numerosity} Werte aller \emph{classifier} jedes Mal addiert werden. In der Standardimplementierung \cite{Butz_xcsclassifier} ist die Behandlung des \emph{numerosity} Werts deswegen stark optimiert, jedes \emph{classifier set} trägt eine temporäre Variable \emph{numerositySum} mit sich, in der die aktuelle Summe gespeichert ist. Die Aktualisierung ist jedoch zum einen mangelhaft umgesetzt, zum anderen auf die Verwendung von einer einzelnen \emph{action set} Liste optimiert. Dagegen wurde die für diese Arbeit erstellte Implementierung jeweils mit bis über 100 \emph{action set} Listen programmiert, denen ein \emph{classifier} Mitglied sein kann. Deswegen wurde die Optimierung entfernt und durch eine dezentrale Verwaltung mit einem \emph{Observer} ersetzt, jede Änderung des \emph{numerosity} Wertes hat also die Änderung aller \emph{action set} Listen zur Folge, in welcher der \emph{classifier} Mitglied ist.\\

Wird also ein \emph{micro classifier} entfernt, dann wird lediglich die Änderungsfunktion des jeweiligen \emph{classifier} aufgerufen, der dann wiederum den \emph{numerositySum} Wert der jeweiligen Eltern anpasst. Dies macht einige Optimierungen rückgängig, erspart aber sehr viele Umstände, den \emph{numerositySum} der Eltern immer auf den aktuellen Stand zu halten und einzelne \emph{classifier} zu löschen.\\

Positiver Nebeneffekt durch die verbesserte Struktur ist der vereinfachte Zufgriff auf die Menge der \emph{action set} Listen, denen ein \emph{classifier} angehört, hierfür wurde aber im Rahmen dieser Arbeit keine Verwendung gefunden.\\

Die Standardimplementierung weist ein weiteres Problem auf. So enthält der \emph{fitness} Wert eines \emph{classifier} als Optimierung bereits den \emph{numerosity} Wert als Faktor, während bei der Aktualisierung des \emph{numerosity} Werts der \emph{fitness} Wert nicht aktualisiert wurde. Das hat zur Folge, dass theoretisch der \emph{fitness} Wert eines \emph{classifier} fast den \emph{max population} Wert annehmen kann, wenn ein \emph{classifier} mit \emph{numerosity} und \emph{fitness} Wert in der Höhe von \emph{max population} auf einen \emph{numerosity} Wert von \(1,0\) reduziert wird. Dies betrifft die Funktion \emph{public void addNumerosity(int num)} der Klasse \emph{XClassifier} in der Datei \emph{XClassifier.java}. Die Korrektur besteht darin, den \emph{fitness} Wert mit dem Quotienten aus dem neuen durch den alten \emph{numerosity} Wert zu multiplizieren. Die korrigierte Fassung ist in Programm~\ref{corrected_numerosity_function:pro} dargestellt.\\

Möglicherweise kann man diesen Fehler durch Veränderung der Parameter oder längere Laufzeiten kompensieren. Es ergibt jedoch keinen Sinn, dass beim Subsummieren bzw. Löschen eines \emph{micro classifier} der \emph{fitness} Wert verändert wird. In Tests haben sich nur minimale Unterschiede ergeben. Beispielsweise ergab sich (auf dem Säulenszenario mit 8 Agenten mit SXCS und einem Zielobjekt mit einfacher Richtungsänderung und Geschwindigkeit 2) eine Qualität von \(32,43\%\) im Vergleich zur originalen Implementierung von \(32,20\%\) bei 500 Schritten bzw. \(36,28\%\) zu \(35,75\%\) bei 2.000 Schritten. Der Fehler scheint sich also minimal negativ auf die Qualität auszuwirken. Problematisch wird es aber auf theoretischer Ebene, wenn Modifikationen von XCS darauf aufbauen, dass der \emph{fitness} Wert für jeden \emph{micro classifier} immer kleiner gleich \(1,0\) ist. Dies kann u.U. zu größeren Fehlern führen.\\

Alles in allem betrachtet soll im Rahmen dieser Arbeit die korrigierte Fassung benutzt werden.\\

\newlisting{Korrigierte Version der \emph{addNumerosity()} Funktion}{corrected_numerosity_function:pro}
/**
 * Erhöht oder erniedrigt den numerosity Wert des classifier.
 * @param num Addiere num (kann negativ sein) zum numerosity Wert.
 */
  public void addNumerosity(int num) {
    int old_num = numerosity;
   
    numerosity += num;

  /**
   * Korrektur des fitness Werts
   */
    if(old_num > 0) {
      fitness = fitness * (double)numerosity / (double)old_num;
    } else {
      fitness = Configuration.getFitnessInitialization();
    }

  /**
   * Aktualisierung der Eltern
   */
    for (ClassifierSet p : parents) {
      p.changeNumerositySum(num);
      if (numerosity == 0) {
        p.removeClassifier(this);
      }
    }
  }
\end{lstlisting}


\clearpage



\section{Implementierung des XCS \emph{multi step} Verfahrens}

\newlisting{Erstes Kernstück des Standard XCS \emph{multi step} Verfahrens (\emph{calculateReward()}, Bestimmung und Verarbeitung des \emph{reward} Werts anhand der Sensordaten), angepasst an ein dynamisches Überwachungsszenario}{multistep_calc_reward:pro}
/**
 * Diese Funktion wird in jedem Schritt aufgerufen um den aktuellen
 * reward Wert zu bestimmen, den besten Wert der ermittelten match set 
 * Liste weiterzugeben und, bei aktuell positivem reward Wert, die 
 * aktuelle action set Liste positiv zu bewerten.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateReward(final long gaTimestep) {
  /**
   * checkRewardPoints() liefert "wahr" wenn sich das Zielobjekt in
   * Sichtweite oder sich keine Agenten in Sichtweite befindet.
   */
    boolean reward = checkRewardPoints();

    if(prevActionSet != null){
      collectReward(lastReward, lastMatchSet.getBestValue(), false);
      prevActionSet.evolutionaryAlgorithm(classifierSet, gaTimestep);
    }

    if(reward) {
      collectReward(reward, 0.0, true);
      lastActionSet.evolutionaryAlgorithm(classifierSet, gaTimestep);
      prevActionSet = null;
      return;
    }
    prevActionSet = lastActionSet;
    lastReward = reward;
  }
\end{lstlisting}


\newlisting{Zweites Kernstück des XCS \emph{multi step} Verfahrens (\emph{collectReward()}, Verteilung des \emph{reward} Werts auf die \emph{action set} Listen), angepasst an ein dynamisches Überwachungsszenario}{multistep_collect_reward:pro}
/**
 * Diese Funktion verarbeitet den übergebenen reward Wert und 
 * gibt ihn an die zugehörigen action set Listen weiter.
 *
 * @param reward Wahr wenn das Zielobjekt in Sicht war.
 * @param best_value Bester Wert des vorangegangenen action set Listen
 * @param is_event Wahr wenn diese Funktion wegen eines Ereignisses, 
 *          d.h. einem positiven reward Wert, aufgerufen wurde.
 */

  public void collectReward(final boolean reward, 
                final double best_value, final boolean is_event) {

    double corrected_reward = reward ? 1.0 : 0.0;

  /**
   * Falls der reward Wert von einem Ereignis rührt, dann aktualisiere 
   * die aktuelle action set Liste und lösche die vorherige.
   */
    if(is_event) {
      if(lastActionSet != null) {
        lastActionSet.updateReward(corrected_reward, best_value, 1.0);
        prevActionSet = null;
      }
    } 

  /**
   * Kein Ereignis, also nur die letzte action set Liste aktualisieren.
   */
    else 
    {
      if(prevActionSet != null) {
        prevActionSet.updateReward(corrected_reward, best_value, 1.0);
      }
    }
  }
\end{lstlisting}



\newlisting{Drittes Kernstück des XCS \emph{multi step} Verfahrens (\emph{calculateNextMove()}, Auswahl der nächsten Aktion und Ermittlung der zugehörigen \emph{action set} Liste), angepasst an ein dynamisches Überwachungsszenario}{multistep_calc_move:pro}
/**
 * Bestimmt die zum letzten bekannten Status passenden classifier und
 * wählt aus dieser Menge eine Aktion. Außerdem wird die aktuelle 
 * action set Liste mithilfe der gewählten Aktion ermittelt.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateNextMove(final long gaTimestep) {

 /**
  * Überdecke das classifierSet mit zum Status passenden classifier
  * welche insgesamt alle möglichen Aktionen abdecken.
  */
    classifierSet.coverAllValidActions(
                    lastState, getPosition(), gaTimestep);

 /**
  * Bestimme alle zum Status passenden classifier.
  */
    lastMatchSet = new AppliedClassifierSet(lastState, classifierSet);

 /**
  * Entscheide auf welche Weise die Aktion ausgewählt werden soll.
  */
    lastExplore = checkIfExplore(lastState.getSensorGoalAgent(),
                                           lastExplore, gaTimestep);

 /**
  * Wähle eine Aktion und bestimme die zugehörige action set Liste.
  */
    calculatedAction = lastMatchSet.chooseAbsoluteDirection(lastExplore);
    lastActionSet = new ActionClassifierSet(lastState, lastMatchSet,
                                                      calculatedAction);
  }
\end{lstlisting}



\clearpage

\section{Implementierung des SXCS Verfahrens}

\newlisting{Erstes Kernstück des SXCS-Algorithmus (\emph{calculateReward()}, Bestimmung des \emph{reward} Werts anhand der Sensordaten)}{sxcs_calc_reward:pro}
/**
 * Diese Funktion wird in jedem Schritt aufgerufen um den aktuellen
 * reward Wert zu bestimmen und positive, negative und neutrale 
 * Ereignisse zu verarbeiten.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateReward(final long gaTimestep) {
  /**
   * checkRewardPoints() liefert "wahr" wenn sich das Zielobjekt in
   * Sichtweite oder sich keine Agenten in Sichtweite befindet.
   */
    boolean reward = checkRewardPoints();

  /**
   * Positives oder negatives Ereignis?
   */
    if (reward != lastReward) {
      int start_index = historicActionSet.size() - 1;
      collectReward(start_index, actionSetSize, reward, true);
      actionSetSize = 0;
    }
    else 

  /**
   * Neutrales Ereignis?
   */
    if(actionSetSize >= Configuration.getMaxStackSize())
    {
      int start_index = Configuration.getMaxStackSize() / 2;
      int length = actionSetSize - start_index;
      collectReward(start_index, length, reward, false);
      actionSetSize = start_index;
    }

    lastReward = reward;
  }
\end{lstlisting}




\newlisting{Zweites Kernstück des SXCS-Algorithmus (\emph{collectReward()} - Verteilung des \emph{reward} Werts auf die \emph{action set} Listen)}{sxcs_collect_reward:pro}
/**
 * Diese Funktion verarbeitet den übergebenen base reward Wert und 
 * gibt ihn an die zugehörigen action set Listen weiter. Es wird
 * kein maxPrediction Wert berechnet oder weitergegeben.
 *
 * @param start_index Index in der historicActionSet Liste der als 
 *          erstes aktualisiert werden soll.
 * @param action_set_size Anzahl der action set Listen, die aktualisiert 
 *          werden sollen.
 * @param reward Wahr wenn der Zielobjekt in Sicht oder keine Agenten 
 *          in Sicht waren.
 * @param is_event Wahr wenn diese Funktion wegen eines positiven oder 
 *          negativen Ereignisses aufgerufen wurde.
 */

  public void collectReward(final int start_index, 
                final int action_set_size, final boolean reward, 
                final boolean is_event) {
    double corrected_reward = reward ? 1.0 : 0.0;

  /**
   * Aktualisiere eine ganze Anzahl von action set Listen
   */
    for(int i = 0; i < action_set_size; i++) {

  /**
   * Benutze aufsteigenden bzw. absteigenden reward Wert bei einem 
   * positiven bzw. negativen Ereignis.
   */
      if(is_event) {
        corrected_reward = reward ? 
          calculateReward(i, action_set_size) : 
          calculateReward(action_set_size - i, action_set_size);
      }
  /**
   * Aktualisiere die action set Liste mit dem bestimmten reward Wert.
   */
      ActionClassifierSet action_classifier_set = 
        historicActionSet.get(start_index - i);
      action_classifier_set.updateReward(corrected_reward, 0.0, 1.0);
    }
  }
\end{lstlisting}



\newlisting{Drittes Kernstück des SXCS-Algorithmus (\emph{calculateNextMove()} - Auswahl der nächsten Aktion und Ermittlung und Speicherung der zugehörigen \emph{action set} Liste)}{sxcs_calc_move:pro}
/**
 * Bestimmt die zum letzten bekannten Status passenden classifier und
 * wählt aus dieser Menge eine Aktion. Außerdem wird die aktuelle 
 * action set Liste mithilfe der gewählten Aktion ermittelt.
 * Im Vergleich zum originalen multi step Verfahren wird am Schluss noch 
 * die ermittelte action set Liste gespeichert.
 *
 * @param gaTimestep Der aktuelle Zeitschritt
 */

  public void calculateNextMove(final long gaTimestep) {

 /**
  * Überdecke die classifier set Liste mit zum Status passenden 
  * classifier welche insgesamt alle möglichen Aktionen abdecken.
  */
    classifierSet.coverAllValidActions(
                    lastState, getPosition(), gaTimestep);

 /**
  * Bestimme alle zum Status passenden classifier.
  */
    lastMatchSet = new AppliedClassifierSet(lastState, classifierSet);

 /**
  * Entscheide auf welche Weise die Aktion ausgewählt werden soll,
  * wähle eine Aktion und bestimme die zugehöriges action set Liste.
  */
    lastExplore = checkIfExplore(lastState.getSensorGoalAgent(),
                                           lastExplore, gaTimestep);

    calculatedAction = lastMatchSet.chooseAbsoluteDirection(lastExplore);
    lastActionSet = new ActionClassifierSet(lastState, lastMatchSet,
                                                      calculatedAction);

 /**
  * Speichere die action set Liste, erhöhe die Größe des Stacks und
  * und passe den Stack bei einem Überlauf an.
  */
    actionSetSize++;
    historicActionSet.addLast(lastActionSet);
    if (historicActionSet.size() > Configuration.getMaxStackSize()) {
      historicActionSet.removeFirst();
    }
  }
\end{lstlisting}




