\chapter{Analyse SXCS}\label{lcs_analysis:cha}


- pillar, 1 direction change, speed 1, max pred + GA, 0.01 prediction init, prediction init adaption

nur gering gg random

GA immer an, nicht testen

pillar, intelligent, low speed, maxpred aus, GA, SWITCH EXPLOIT, SEHR GUT!

dann bei Bewertung weitermachen!

\begin{table}[ht]
\caption{Vergleich ``Intelligent (Open)'' und ``Intelligent (Hide)'' (8 Agenten, Säulenszenario)}
\centering
\begin{tabular}{c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent (Open)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.55\% & 11.58\% \\
XCS                    & 71.35\% & 13.98\% \\
SXCS                   & 72.10\% & 13.50\% \\ [1ex]
\hline
``Intelligent (Hide)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.56\% & 11.78\% \\
XCS                    & 71.33\% & 14.27\% \\
SXCS                   & 72.05\% & 13.90\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_pillar}
\end{table}


\begin{table}[ht]
\caption{Vergleich ``Intelligent (Open)'' und ``Intelligent (Hide)'' (8 Agenten, Säulenszenario)}
\centering
\begin{tabular}{c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent (Open)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.55\% & 11.58\% \\
XCS                    & 71.35\% & 13.98\% \\
SXCS                   & 72.10\% & 13.50\% \\ [1ex]
\hline
``Intelligent (Hide)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.56\% & 11.78\% \\
XCS                    & 71.33\% & 14.27\% \\
SXCS                   & 72.05\% & 13.90\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_pillar}
\end{table}


TODO 
%if((sensor_goal[2*i])) {//  && (!sensor_agent[2*i+1])) { vergleichen. Ohne Sensoragent minimal schlechter

TODO auch sich langsam bewegende analysieren!
Und auch stehenbleibende : z.B. im Raumszenario.

Geschwindigkeit 2 problematisch, Geschwindigkeit 1 ok?

TODO classifier ausgeben 

\section{Vergleich unterschiedlicher Geschwindigkeiten des Zielobjekts}

In Abbildung~\ref{goal_agent_speed:fig} ist ein Vergleich der unterschiedlicher Geschwindigkeiten des Zielobjekts dargestellt. XCS (mit 500 Schritten) macht bei keiner Geschwindigkeit Lernfortschritte, die Qualität pendelt zwischen 31.69\% und 33.40\%, also in etwa identisch mit der zufälligen Bewegung. Die SXCS Implementierung scheint dagegen die geringere Geschwindigkeit ausgenutzt zu haben und ist dadurch in der Lage das Zielobjekt besser zu verfolgen. Mit 500 Schritten ist die Qualität abnehmend von 39.64\% (Geschwindigkeit 1.0) bis 35.96\% (Geschwindigkeit 2.0), im Fall mit 2000 Schritten erhöht sich dieser Bereich leicht auf 40.15\% bis 37.71\%.\\
Auch bei den Heuristiken zeichnet sich ein klares Bild ab, bei niedrigen Geschwindigkeiten ist die Ausbreitung der Agenten auf dem Feld (intelligente Heuristik) weniger wichtig als die konstante Verfolgung des Zielobjekts, während bei höheren Geschwindigkeiten die Verteilung auf dem Feld wichtiger wird.

\begin{figure}[htbp]
\centerline{	
\includegraphics{goal_agent_speed.eps}
}
\caption[Vergleich der Qualitäten verschiedener Algorithmen bezüglich der Geschwindigkeit des Zielobjekts] {Vergleich der Qualitäten verschiedener Algorithmen bezüglich der Geschwindigkeit des Zielobjekts}
\label{goal_agent_speed:fig}
\end{figure}

\begin{verbatim}
Bester Agent nach 20000 Schritten (Zielgeschwindigkeit 2.0, SXCS, 2000 Schritte)

#0######.###0#0##.#0#0###0-S : [Fi: 0.38] [Ex: 00450.0] [Pr: 0.74] [PE: 0.38]
....
TODO
\end{verbatim}

\section{Zielobjekt mit XCS und SXCS}\label{goal_agent_with_xcs:sec}

Der in Kapitel~\ref{variant_zielobjekt_xcs_sxcs:sec} besprochene Ansatz, einen Zielobjekt mit einem XCS bzw. SXCS auszustatten, soll hier nun getestet werden. Dabei sollen zuerst Agenten mit den besprochenen statischen Heuristiken gegen diesen Zielobjekt antreten, abschließend soll - hauptsächlich aus Neugier - ein solches Zielobjekt gegen ebenfalls mit SXCS ausgestatteten Agenten antreten, hier soll insbesondere der Verlauf über die Zeit interessieren, da die Qualität TODO


\section{Zusammenfassung der bisherigen Erkenntnisse}

 Algorithmen mit Ergebnissen die unter dem des zufälligen Algorithmus liegt, sind unbrauchbar und nicht vergleichbar. ``Verbesserungen'', die die Qualität des Algorithmus näher an das Ergebnis des zufälligen Algorithmus bringen, sind in Wirklichkeit Veränderungen, die den Algorithmus eher zufällige Entscheidungen treffen lassen, und keine tatsächlichen Lernerfolge.

SXCS sehr gut bei NO DIRECTION CHANGE und speed 1!


nicht geschafft: Pillar, one direction change, speed 2, XCS ...besser... weil zufälliger


\section{Standard XCS Multistepverfahren}




\subsection{SXCS und Heuristiken}

erst multistep... mit random vergleichen

In allen Tests erreichten die Heuristiken deutlich bessere Ergebnisse. Diesen Nachteil hat sich LCS in diesen Szenarien durch deutlich überlegene Flexibilität erkauft
Ein Großteil der eingehenden Informationen ist für die Auswertung nicht relevant und lokale Information ist zu ungenau.
Bei einer komplexeren Implementierung mit Distanzen

Insbesondere der Vergleich mit dem intelligenten Agenten, der anderen Agenten ausweicht, zeigt, dass die LCS Agenten unmöglich ein solches globales Ziel erreichen können, es ist also kein emergentes Verhalten zu beobachten. Dies ist dadurch zu begründen, dass bei der Berechnung des Rewards keine Information außer der eigenen, lokalen Information 

der Abstand zu anderen Agenten nicht Teil der Berechnung des Rewards ist, noch gibt keine eingebaute Heuristik. Man könnte zwar 


\subsection{Vergleich Multistep / LCS}

Szenarien, Parameter.

\subsection{Test der verschiedenen Exploration-Modi}


Prediction Error sehr hoch, da dynamisches 
