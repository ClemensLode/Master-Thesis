\chapter{Analyse SXCS}\label{lcs_analysis:cha}







\begin{table}[ht]
\caption{Vergleich ``Intelligent (Open)'' und ``Intelligent (Hide)'' (8 Agenten, Säulenszenario)}
\centering
\begin{tabular}{c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent (Open)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.55\% & 11.58\% \\
XCS                    & 71.35\% & 13.98\% \\
SXCS                   & 72.10\% & 13.50\% \\ [1ex]
\hline
``Intelligent (Hide)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.56\% & 11.78\% \\
XCS                    & 71.33\% & 14.27\% \\
SXCS                   & 72.05\% & 13.90\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_pillar}
\end{table}


\begin{table}[ht]
\caption{Vergleich ``Intelligent (Open)'' und ``Intelligent (Hide)'' (8 Agenten, Säulenszenario)}
\centering
\begin{tabular}{c c c}
\hline\hline
Algorithmus & Abdeckung & Qualität \\ [1ex]
\hline
``Intelligent (Open)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.55\% & 11.58\% \\
XCS                    & 71.35\% & 13.98\% \\
SXCS                   & 72.10\% & 13.50\% \\ [1ex]
\hline
``Intelligent (Hide)'' \\ [1ex]
\hline
Zufällige Bewegung     & 72.56\% & 11.78\% \\
XCS                    & 71.33\% & 14.27\% \\
SXCS                   & 72.05\% & 13.90\% \\ [1ex]
\hline
\end{tabular}
\label{table:intelligent_open_hide_pillar}
\end{table}


TODO auch sich langsam bewegende analysieren!
Und auch stehenbleibende :> z.B. im Raumszenario.

Geschwindigkeit 2 problematisch, Geschwindigkeit 1 ok

lcs länger laufen lassen!viele experimente

\section{Zusammenfassung der bisherigen Erkenntnisse}

Bezüglich der Tests konnten in den vorangegangenen Kapiteln bisher folgende Ergebnisse in Erfahrung gebracht werden:

\begin{itemize}
\item Algorithmen mit Ergebnissen die unter dem des zufälligen Algorithmus liegt, sind unbrauchbar und nicht vergleichbar. ``Verbesserungen'', die die Qualität des Algorithmus näher an das Ergebnis des zufälligen Algorithmus bringen, sind in Wirklichkeit Veränderungen, die den Algorithmus eher zufällige Entscheidungen treffen lassen, und keine tatsächlichen Lernerfolge.
\item Szenarien
\item 

\end{itemize}

SXCS sehr gut bei NO DIRECTION CHANGE und speed 1!


nicht geschafft: Pillar, one direction change, speed 2, XCS ...besser... weil zufälliger


\section{Standard XCS Multistepverfahren}



\subsection{SXCS und Heuristiken}

erst multistep... mit random vergleichen

In allen Tests erreichten die Heuristiken deutlich bessere Ergebnisse. Diesen Nachteil hat sich LCS in diesen Szenarien durch deutlich überlegene Flexibilität erkauft
Ein Großteil der eingehenden Informationen ist für die Auswertung nicht relevant und lokale Information ist zu ungenau.
Bei einer komplexeren Implementierung mit Distanzen

Insbesondere der Vergleich mit dem intelligenten Agenten, der anderen Agenten ausweicht, zeigt, dass die LCS Agenten unmöglich ein solches globales Ziel erreichen können, es ist also kein emergentes Verhalten zu beobachten. Dies ist dadurch zu begründen, dass bei der Berechnung des Rewards keine Information außer der eigenen, lokalen Information 

der Abstand zu anderen Agenten nicht Teil der Berechnung des Rewards ist, noch gibt keine eingebaute Heuristik. Man könnte zwar 


TODO statistical value:Error in predictions!

\subsection{Vergleich Multistep / LCS}

Szenarien, Parameter.

\subsection{Test der verschiedenen Exploration-Modi}


Prediction Error sehr hoch, da dynamisches 
