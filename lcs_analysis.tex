\chapter{Analyse SXCS}\label{lcs_analysis:cha}

\section{Erste Analyse der Agenten ohne XCS}\label{analysis_sans_lcs:cha}

In diesem Abschnitt sollen erste Analysen bezüglich der verwendeten Szenarien anhand des Algorithmus zufälliger Bewegung (siehe Kapitel~\ref{randomized_movement:sec}), des Algorithmus mit einfacher Heuristik (siehe Kapitel~\ref{simple_heuristik:sec}) und des Algorithmus mit intelligenter Heuristik (siehe Kapitel~\ref{intelligent_heuristik:sec}) angefertigt werden. Die Ergebnisse aus der Analyse werden eine Grundlage für die vergleichende Betrachtung der Agenten mit XCS Algorithmen in Kapitel~\ref{lcs_analysis:cha} dienen, insbesondere werden sie Anhaltspunkte dafür geben, welche Szenarien welche Eigenschaften der Algorithmen testen. Außerdem kann der Vergleich von Agenten intelligenten Heuristik mit Agenten mit zufälliger Bewegung Aufschluss darüber geben, wieviel und welche Aspekte ein Agent in einem solchen Szenario überhaupt lernen kann. Große Unterschiede zwischen intelligenter und einfacher Heuristik weisen beispielsweise darauf hin, dass die Verteilung auf dem Torus wichtiger ist, als das Hinterherlaufen. Dies sieht man insbesondere am Extrembeispiel des Zielobjekts mit zufälligem Sprung in Kapitel~\ref{zielobjekt_analyse_zufall_sprung:sec}.



\subsection{Zielobjekt mit zufälligem Sprung}\label{zielobjekt_analyse_zufall_sprung:sec}

Im folgenden sollen alle TODO



In allen Szenarien mit dieser Form der Bewegung des Zielobjekts kommt es nur darauf an, dass die Agenten einen möglichst großen Bereich des Torus abdecken. 


\subsection{Im leeren Szenario ohne Hindernisse}\label{jump_empty_scenario:sec}

Ohne Hindernisse gibt sich ein klares Bild (siehe Tabelle~\ref{table:empty_total_random}), die intelligente Heuristik ist etwas besser als der des zufälligen Agenten und der einfachen Heuristik. Ein möglichst weiträumiges Verteilen auf dem Torus führt zum Erfolg, was sich auch in einem hohen Wert der Abdeckung zeigt, denn genau das wird mit dem völlig zufällig springenden Agenten getestet. Auch ist die Zahl der blockierten Bewegungen deutlich niedriger, was sich auch mit der Haltung des Abstands erklären lässt.\\

Die einfache Heuristik schneidet dagegen etwas schlechter als eine zufällige Bewegung ab. Zwar ist die Zahl der blockierten Bewegungen geringer, was sich dadurch erklären lässt, dass die einfache Heuristik zumindest an einem Punkt eine Sichtbarkeitsüberprüfung für die Richtung durchführt, in der sie sich bewegen möchte (nämlich wenn das Zielobjekt in Sicht ist), andererseits ist die Abdeckung etwas geringer. Dies kommt daher, dass, wenn mehrere Agenten das Zielobjekt in derselben Richtung in Sichtweite haben, mehrere Agenten sich in dieselbe Richtung bewegen. Dies beeinträchtigt die zufällige Verteilung der Agenten auf dem Spielfeld und führt somit auch zu einer niedrigeren Abdeckung des Torus.\\

Bezüglich der Anzahl der Agenten ergeben sich keine Besonderheiten, mit steigender Agentenzahl steigt die Zahl der blockierten Bewegungen (aufgrund größerer Anzahl von blockierten Feldern), während die Abdeckung sinkt (aufgrund sich überlappender Überwachungsreichweiten).

\begin{table}[ht]
\caption{Zufällige Sprünge des Zielobjekts im leeren Szenario ohne Hindernisse}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Agentenzahl & Blockierte Bewegungen & Abdeckung & Qualität \\ [0.5ex]
\hline
Zufällige Bewegung     & 8  & 2,82\% & 73,78\% & 32,36\% \\
Einfache Heuristik     & 8  & 2,79\% & 73,22\% & 32,10\% \\
Intelligente Heuristik & 8  & 0,64\% & 81,26\% & 35,91\% \\ [1ex]
\hline
Zufällige Bewegung     & 12 & 4,32\% & 69,55\% & 44,75\% \\
Einfache Heuristik     & 12 & 4,19\% & 68,88\% & 43,86\% \\
Intelligente Heuristik & 12 & 1,49\% & 77,60\% & 49,49\% \\ [1ex]
\hline
Zufällige Bewegung     & 16 & 5,82\% & 64,28\% & 54,55\% \\
Einfache Heuristik     & 16 & 5,66\% & 63,65\% & 53,99\% \\
Intelligente Heuristik & 16 & 2,85\% & 71,44\% & 60,73\% \\ [1ex]
\hline
\end{tabular}
\label{table:empty_total_random}
\end{table}


\subsection{Säulenszenario}

Für das Säulenszenario (siehe Tabelle~\ref{table:pillar_total_random}) ergeben sich erwartungsgemäß ähnliche Werte wie im Fall des leeren Szenarios ohne Hindernisse (siehe Tabelle~\ref{table:empty_total_random}). Durch geringere Sicht und höhere Zahl an blockierten Bewegungen ergibt sich jeweils eine geringere Abdeckung und auch jeweils eine geringere Qualität. Auch hier ergeben sich keine Besonderheiten bezüglich der Agenten, im Folgenden werden sich die Tests deshalb auf den Fall mit {\bf 8 Agenten} beschränken.

\begin{table}[ht]
\caption{Zufällige Sprünge des Zielobjekts in einem Säulenszenario}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Agentenzahl & Blockierte Bewegungen & Abdeckung & Qualität \\ [0.5ex]
\hline
Zufällige Bewegung     & 8  & 4,45\% & 72,11\% & 32,13\% \\
Einfache Heuristik     & 8  & 4,08\% & 71,70\% & 31,99\% \\
Intelligente Heuristik & 8  & 2,34\% & 79,61\% & 35,29\% \\ [1ex]
\hline
Zufällige Bewegung     & 12 & 5,93\% & 67,72\% & 44,44\% \\
Einfache Heuristik     & 12 & 5,67\% & 67,23\% & 43,81\% \\
Intelligente Heuristik & 12 & 3,62\% & 75,86\% & 49,34\% \\ [1ex]
\hline
Zufällige Bewegung     & 16 & 7,62\% & 62,53\% & 54,26\% \\
Einfache Heuristik     & 16 & 7,23\% & 62,00\% & 53,58\% \\
Intelligente Heuristik & 16 & 5,18\% & 69,91\% & 60,43\% \\ [1ex]
\hline
\end{tabular}
\label{table:pillar_total_random}
\end{table}


\subsection{Zufällig verteilte Hindernisse}


Hier ergibt sich für alle Einstellungen für \(\lambda_{h}\) und \(\lambda_{p}\) (siehe Kapitel~\ref{random_scenario_definition:sec}) ebenfalls ein eindeutiges Bild (siehe Tabelle~\ref{table:full_total_random}), die intelligente Heuristik liegt wieder vorne, gefolgt wieder von der einfachen Heuristik und der zufälligen Bewegung. Im Fall mit vielen Hindernissen (\(\lambda_{h} = 0,2\)) liegt die einfache Heuristik trotz höherer Abdeckung hinter der zufälligen Bewegung. Dies ist wohl auf einen Zufall zurückzuführen, ändert man den \emph{random seed} Wert oder erhöht man die Anzahl der Experimente von 10 auf 30 ergibt sich wieder oben genannte Reihenfolge.\\

Dass der einfache Agent, wenn er das Zielobjekt in Sicht hat, eine geringere Zahl an blockierten Bewegungen als der zufällige Agent aufweist, lässt sich damit begründen, dass er davon ausgehen kann, dass sich in dieser Richtung wahrscheinlich eher kein Hindernis befindet (da die Sicht nicht blockiert ist), während der zufällige Agent Hindernisse überhaupt nicht beachtet, somit öfters gegen ein Hindernis läuft und letztlich öfters stehen bleibt. Der Unterschied zwischen beiden Agenten ist besonders hoch in Szenarien mit größerem Anteil an Hindernissen.\\

Im Vergleich zur einfachen Heuristik scheint insbesondere die intelligente Heuristik Probleme mit den Hindernissen zu haben (viele blockierte Bewegungen). Da Hindernisse in der Heuristik nicht beachtet werden, bewirkt die Strategie der maximalen Ausbreitung der Agenten, dass die Agenten gegen die Hindernisse gedrückt werden (andere Agenten sind bei hohem Verknüpfungsfaktor eher in einem Bereich ohne Hindernisse).\\

Schließlich ist zu sehen, dass die Agenten in einem Szenario mit höherem Verknüpfungsfaktor (der Fall mit \(\lambda_{h} = 0,1\) und \(\lambda_{p} = 0,99\) im Vergleich zum Fall mit \(\lambda_{h} = 0,1\) und \(\lambda_{p} = 0,5\)) besser abschneiden. Dies liegt daran, dass Szenarien mit hohem Verknüpfungsfaktor bedeuten, dass viele Hindernisse zusammenhängend einen großen Block bilden und somit dem Szenario ohne Hindernisse ähnlich sind, da es eher größere zusammenhängende Flächen gibt.\\

Insgesamt ist zu sagen, dass keine der Szenarien mit zufälligem Sprung des Zielobjekts sich als zu lernende Aufgabe lohnt, der Unterschied zwischen der zufälligen Bewegung und der intelligenten Heuristik ist zu gering, die Aufgabe somit zu schwierig und soll in Verbindung mit XCS nicht weiter betrachtet werden.


\begin{table}[ht]
\caption{Zufällige Sprünge des Zielobjekts in einem Szenario mit Hindernisse}
\centering
\begin{tabular}{c c c c c c}
\hline\hline
Algorithmus & \(\lambda_{h}\) & \(\lambda_{p}\) & Blockierte Bewegungen & Abdeckung & Qualität \\ [0.5ex]
\hline
Zufällige Bewegung     & 0,2 & 0,99 & 12,44\% & 62,50\% & 34,54\% \\
Einfache Heuristik     & 0,2 & 0,99 & 10,04\% & 63,02\% & 34,48\% \\
Intelligente Heuristik & 0,2 & 0,99 & 12,71\% & 68,22\% & 37,89\% \\ [1ex]
\hline
Zufällige Bewegung     & 0,1 & 0,99 &  7,58\% & 68,33\% & 32,81\% \\
Einfache Heuristik     & 0,1 & 0,99 &  6,15\% & 68,49\% & 33,36\% \\
Intelligente Heuristik & 0,1 & 0,99 &  6,50\% & 74,81\% & 36,29\% \\ [1ex]
\hline
Zufällige Bewegung     & 0,1 & 0,5  & 10,12\% & 66,01\% & 32,03\% \\
Einfache Heuristik     & 0,1 & 0,5  &  8,57\% & 66,52\% & 32,38\% \\
Intelligente Heuristik & 0,1 & 0,5  &  9,29\% & 72,63\% & 35,12\% \\ [1ex]
\hline
\end{tabular}
\label{table:full_total_random}
\end{table}



\subsection{Zielobjekt mit zufälliger Bewegung bzw. einfacher Richtungsänderung}

Wesentlicher Punkt bei beiden Bewegungstypen (siehe Kapitel~\ref{random_neighbor:sec} und Kapitel~\ref{direction_change:sec}) ist, dass der jetzige Ort des Zielobjekts maximal zwei Felder (die maximale Geschwindigkeit des Zielobjekts in den Tests) vom Ort in der vorangegangenen Zeiteinheit entfernt ist. Somit ist ein lokales Einfangen eher von Relevanz, der Ort an dem sich das Zielobjekt im nächsten Zeitschritt befinden wird, ist zumindest vom aktuellen Ort abhängig, wenn das Zielobjekt auch schneller sein kann als andere Agenten.\\

Wesentlicher Unterschied zwischen beiden Bewegungstypen ist, dass das Zielobjekt mit zufälliger Bewegung nach 2 Schritten mit Wahrscheinlichkeit von \(\frac{1}{4}\) auf das ursprüngliche Feld zurückkehrt, also stehenbleibt. Wie die Ergebnisse in Tabellen~\ref{table:neighbor_change_random} und ~\ref{table:neighbor_change_pillar} zeigen, ergibt sich dadurch ein leichteres Szenario. Ein mitunter stehengebliebener Agent kann mittels Heuristiken leichter überwacht werden, während es keine signifikante Veränderung bei der zufälligen Bewegung ergibt. In weiteren Tests soll deswegen immer nur Zielobjekten mit einfacher Richtungsänderung getestet werden.\\

In den Tabellen bezieht sich der Eintrag "`Sprünge"' auf den Anteil vom Zielobjekt durchgeführter Sprünge, "`Blockiert"' auf den Anteil blockierter Bewegungen des Agenten und "`Zufällig bewegend"' bzw. "`Einfache Richtungsänderung"' auf das Zielobjekt.\\

\begin{table}[ht]
\caption{Vergleich von Zielobjekt mit zufälliger Bewegung und einfacher Richtungsänderung (leeres Szenario ohne Hindernisse)}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Sprünge & Blockiert & Abdeckung & Qualität \\ [1ex]
\hline
Zufällig bewegend\\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% &  2,71\% & 73,85\% & 32,57\% \\
Einfache Heuristik     & 0,06\% & 11,51\% & 63,65\% & 79,97\% \\
Intelligente Heuristik & 0,02\% &  4,71\% & 71,15\% & 81,59\% \\ [1ex]
\hline
Einfache Richtungsänderung \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% &  2,75\% & 73,81\% & 30,99\% \\
Einfache Heuristik     & 0,01\% &  4,98\% & 66,61\% & 58,38\% \\
Intelligente Heuristik & 0,01\% &  2,93\% & 73,37\% & 62,48\% \\ [1ex]
\hline
\end{tabular}
\label{table:neighbor_change_no_obstacles}
\end{table}

\begin{table}[ht]
\caption{Vergleich von Zielobjekt mit zufälliger Bewegung und einfacher Richtungsänderung (zufälliges Szenario mit $\lambda_{h} = 0,1$, $\lambda_{p} = 0,99$)}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Sprünge & Blockiert & Abdeckung & Qualität \\ [1ex]
\hline
Zufällig bewegend\\ [1ex]
\hline
Zufällige Bewegung     & 0,01\% &  7,49\% & 66,63\% & 33,96\% \\
Einfache Heuristik     & 0,41\% & 11,51\% & 59,72\% & 79,99\% \\
Intelligente Heuristik & 0,36\% & 10,76\% & 65,87\% & 81,50\% \\ [1ex]
\hline
Einfache Richtungsänderung \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% &  7,54\% & 68,31\% & 31,66\% \\
Einfache Heuristik     & 0,06\% &  8,68\% & 62,31\% & 57,95\% \\
Intelligente Heuristik & 0,08\% &  8,57\% & 68,28\% & 61,72\% \\ [1ex]
\hline
\end{tabular}
\label{table:neighbor_change_random}
\end{table}

\begin{table}[ht]
\caption{Vergleich von Zielobjekt mit zufälliger Bewegung und einfacher Richtungsänderung (Säulenszenario)}
\centering
\begin{tabular}{c c c c c}
\hline\hline
Algorithmus & Sprünge & Blockiert & Abdeckung & Qualität \\ [1ex]
\hline
Zufällig bewegend\\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% & 4,34\% & 72,27\% & 31,80\% \\
Einfache Heuristik     & 0,07\% & 8,77\% & 62,87\% & 78,34\% \\
Intelligente Heuristik & 0,04\% & 6,40\% & 69,98\% & 80,54\% \\ [1ex]
\hline
Einfache Richtungsänderung \\ [1ex]
\hline
Zufällige Bewegung     & 0,00\% & 4,30\% & 72,28\% & 29,17\% \\
Einfache Heuristik     & 0,01\% & 6,29\% & 65,80\% & 56,19\% \\
Intelligente Heuristik & 0,01\% & 4,58\% & 72,44\% & 60,41\% \\ [1ex]
\hline
\end{tabular}
\label{table:neighbor_change_pillar}
\end{table}


\section{Auswirkung der Geschwindigkeit des Zielobjekts}

Angesichts der Ergebnisse in den zwei vorangegangenen Kapiteln, ist zu erwarten, dass die Geschwindigkeit des Zielobjekts bei der Qualität des Agenten mit zufälliger Bewegung keine Rolle spielt, da weder das Zielobjekt noch die Agenten Informationen über ihre Umgebung benutzen um sich für ein Verhalten zu entscheiden.

TODO subsections zusammenfassen


\subsection{Zielobjekt mit einfacher Richtungsänderung}\label{speed_single_direction:sec}

In Abbildung~\ref{speed_random_goal:fig} sind die Testergebnisse für einen Test auf dem Säulenszenario dargestellt, bei dem sich das Zielobjekt mit einfacher Richtungsänderung bewegt. Es ist keine Korrelation zwischen der Geschwindigkeit und der Qualität des Algorithmus mit zufälliger Bewegung festzustellen, nur bei Geschwindigkeit 0 scheint es ein deutlich besseres Ergebnis zu geben. Das lässt sich aber durch die Anfangskonfiguration erklären, beim Säulenszenario startet das Zielobjekt in der Mitte mit maximalem Abstand zu den Hindernissen, ist also immer optimal in Sicht.\\
Der Algorithmus mit zufälliger Bewegung stellt also eine Untergrenze dar, ein Agent muss mehr als diesen Wert erreichen, damit man sagen kann, dass er etwas gelernt hat.\\

In Abbildung~\ref{speed_random_goal_heuristik:fig} sind dagegen die Testergebnisse (im selben Szenario) für die einfache und die intelligente Heuristik zu sehen. Im Wesentlichen sind drei Punkte anzumerken, erstens existiert eine Korrelation zwischen Qualität und Geschwindigkeit, zweitens gibt es einen Knick bei Geschwindigkeit 1 und drittens ist ein fast stetiger Anstieg der Differenz zwischen der einfachen und der intelligenten Heuristik zu verzeichnen. Der Knick lässt sich dadurch erklären, dass es ab dieser Geschwindigkeit möglich ist, dass das Zielobjekt Verfolger abschütteln kann, der Anstieg der Differenz lässt sich dadurch erklären, dass es Abdeckung des Gebiets eine immer größere Rolle spielt, als die Verfolgung des Zielobjekts.

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_random_goal.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit auf Agenten mit zufälliger Bewegung]{Auswirkung der Zielgeschwindigkeit auf Agenten mit zufälliger Bewegung (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario)}
\label{speed_random_goal:fig}
\end{figure}

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_random_goal_heuristik.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit auf Agenten mit Heuristik]{Auswirkung der Zielgeschwindigkeit auf Agenten mit Heuristik (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario)}
\label{speed_random_goal_heuristik:fig}
\end{figure}


\subsection{Zielobjekt mit intelligenter Bewegung}\label{zielagent_analyse_intelligent:sec}

In Abbildung~\ref{speed_intelligent_goal:fig} und Abbildung~\ref{speed_intelligent_goal_obst2:fig} werden im Säulenszenario bzw. Szenario mit zufällig verteilten Hindernissen wieder die Heuristiken bei unterschiedlichen Geschwindigkeiten des Zielobjekts verglichen. Beim Säulenszenario ist wieder der Knick wie beim Fall mit Zielobjekt mit einfacher Richtungsänderung (siehe Kapitel~\ref{speed_single_direction:sec}) zu beobachten. Im Fall mit  TODO

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_intelligent_goal.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt, Säulenszenario) auf Agenten mit Heuristik]{Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt) auf Agenten mit Heuristik}
\label{speed_intelligent_goal:fig}
\end{figure}

\begin{figure}[htbp]
\centerline{	
\includegraphics{speed_intelligent_goal_obst2.eps}
}
\caption[Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt, Szenario mit zufällig verteilten Hindernissen, $\lambda_{h}=0,2$, $\lambda_{p}=0,99$) auf Agenten mit Heuristik]{Auswirkung der Zielgeschwindigkeit (intelligentes Zielobjekt, Szenario mit zufällig verteilten Hindernissen, $\lambda_{h}=0,2$, $\lambda_{p}=0,99$) auf Agenten mit Heuristik}
\label{speed_intelligent_goal_obst2:fig}
\end{figure}

TODO: Erläuterung!



\subsection{Schwieriges Szenario}\label{test_schwieriges_szenario:sec}

Für das sogenannte schwierige Szenario aus Kapitel~\ref{difficult_scenario:sec} erscheint nur der in Kapitel~\ref{no_direction_change:sec} vorgestellte Typ von Zielobjekt mit Beibehaltung der Richtung sinnvoll, da das Ziel für die Agenten sein soll, bis in den letzten Abschnitt vorzudringen und dem Zielobjekt nicht schon auf halbem Weg zu begegnen.\\
Für verschiedene Anzahl von Schritten sind für die drei Agententypen in Abbildung~\ref{steps_direction_difficult_heuristics:fig} die jeweiligen Qualitäten aufgeführt. Wie man beim Vergleich zwischen zufälliger Bewegung und einfacher Heuristik sehen kann, ist es nicht nur entscheidend, in den letzten Bereich am rechten Rand des Szenarios vorzudringen, sondern auch, dort den Agenten zu verfolgen und in diesem Bereich zu bleiben. Deutlich zeigen sich hier die Vorzüge der intelligenten Heuristik, durch das Bestreben, Agenten auszuweichen, hat es dieser Algorithmus leichter, durch die Öffnungen in von Agenten unbesetzte Bereiche vorzudringen. Der Unterschied zwischen einfacher und intelligenter Heuristik zeigt auch, dass in diesem Szenario ein deutlich größeres Lernpotential, was die Einbeziehung von wahrgenommenen Agentenpositionen betrifft, für Agenten besteht. Wie später in Kapitel~\ref{xcs_difficult_scenario:sec} gezeigt wird, können in diesem Szenario unter anderem deshalb auf XCS basierte Agenten ihre Vorteile besonders gut ausspielen und erreichen sogar bessere Ergebnisse als die intelligente Heuristik.

\begin{figure}[htbp]
\centerline{	
\includegraphics{steps_direction_difficult_heuristics.eps}
}
\caption[Auswirkung der Anzahl der Schritte (schwieriges Szenario, Geschwindigkeit 2, ohne Richtungsänderung) auf Qualität von Agenten mit Heuristik]{Auswirkung der Anzahl der Schritte (schwieriges Szenario, Geschwindigkeit 2, ohne Richtungsänderung) auf Qualität von Agenten mit Heuristik}
\label{steps_direction_difficult_heuristics:fig}
\end{figure}

\subsection{Zusammenfassung}

Wie man sehen konnte, existieren also Szenarien in denen Abdeckung kaum eine Rolle spielt und lokale Entscheidungen eine wesentliche Rolle spielen. Dies wird es erleichtern, geeignete Szenarien im Kapitel~\ref{communication:cha}  zu finden.

TODO



\section{Test der verschiedenen XCS Auswahlarten}\label{test_auswahlarten:sec}

In Tabelle~\ref{table:auswahlarten_vergleich_direction} kann man die bisherigen Vermutungen sehr gut erkennen. Die Auswahlarten \emph{random selection} und \emph{roulette wheel selection}  sind für sich alleine kaum brauchbar, das Ergebnis ist nicht besser als des sich zufällig bewegenden Agenten, für den in Kapitel~\ref{speed_single_direction:sec} festgestellt wurde, dass unabhängig von der Geschwindigkeit des Zielobjekts die Qualität bei etwa 30\% liegt.\\
Die Auswahlart \emph{best selection} sorgt gar für über 40\% blockierte Bewegungen und einer deutlich schlechteren Abdeckung. Für die \emph{exploit} Phase scheint nur \emph{tournament selection} deutlich bessere Ergebnisse zu liefern, wenn auch mit relativ hoher Zahl blockierter Bewegungen. Da die \emph{roulette wheel} Auswahlart etwas bessere Ergebnisse liefert, soll sie für die \emph{explore} Phase benutzt werden.\\

Für den Wechsel zwischen der \emph{explore} und \emph{exploit} Phase sieht man bei zufälligem Wechsel, dass die statistischen Werte zwischen denen der \emph{roulette wheel} und \emph{tournament selection} Auswahlart liegen, stellt angesichts der minimalen Steigerung zur Qualität von der \emph{roulette wheel} Auswahlart also kein signifikante Verbesserung dar. Wechselt man bei einer Änderung des \emph{base reward} Werts und startet in der \emph{explore} Phase ergibt sich ein deutlich schlechteres Ergebnis, der Algorithmus scheint sich also genau falsch zu verhalten. Umgekehrt, startet man in der \emph{exploit} Phase, ergibt sich dagegen ein deutlich besseres Ergebnis. Über die Gründe kann man spekulieren. Da die meisten Ergebnisse der Tests eine Qualität von unter 50\% erreichten, 8 Agenten benutzt wurden und in Tests sich gezeigt hat, dass deren Abdeckung etwa 70\% betrug (also im Durchschnitt etwa 30\% des maximal überwachbaren Gebiets verschwendet war), ist anzunehmen, dass der einzelne Agent das Zielobjekt sehr selten in Sichtweite bekam. Der Wechsel zur \emph{explore} Phase erlaubt also anscheinend einen Ausgleich zwischen der unverhältnismäßig langen Zeit, in der das Zielobjekt nicht in Sicht ist und der Zeit, in der das Zielobjekt in Sicht ist. Vermutlich würde dieser Vorteil bei einer größeren Anzahl von Zielobjekten verschwinden.\\

Desweiteren ist zum Vergleich wichtig, wie die Situation beim XCS Algorithmus hinsichtlich der Auswahlverfahren ist. Die Ergebnisse in Tabelle~\ref{table:auswahlarten_vergleich_direction_xcs} demonstrieren, dass hier nur die reine \emph{exploit} Phase einen positiven Effekt, relativ zum Agenten mit zufälliger Bewegung, bringt.


Insgesamt soll also im Weiteren für nur die \emph{tournament selection} und der Wechsel zwischen \emph{tournament selection} und \emph{roulette wheel selection} als Auswahlart benutzt werden.

TODO neu!?

Tabelle~\ref{table:auswahlarten_vergleich_direction2}



TODO mit obigen Tabellen vergleichen!

\begin{table}[ht]
\caption{Vergleich der verschiedenen Auswahlarten (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario, Geschwindigkeit 1, Agenten mit SXCS Algorithmus)}
\centering
\begin{tabular}{c c c c}
\hline\hline
Auswahlart & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Agent mit zufälliger Bewegung                      &  4,46\% & 72,12\% & 29,05\% \\[1ex]
\hline
\emph{roulette wheel selection}         &  4,54\% & 72,10\% & 30,30\% \\
\emph{random selection}                 &  4,34\% & 72,21\% & 28,50\% \\
\bf{\emph{tournament selection}}        & 11,21\% & 70,20\% & \bf{33,39\%} \\
\emph{best selection}                   & 41,16\% & 63,64\% & 29,22\% \\
Zufällig \emph{explore}/\emph{exploit}  &  6,29\% & 71,18\% & 30,58\% \\
Abwechselnd, zuerst \emph{explore}      &  5,63\% & 71,37\% & 26,30\% \\
\bf{Abwechselnd, zuerst \emph{exploit}} &  9,28\% & 70,40\% & \bf{35,36\%} \\[1ex]
\hline
\end{tabular}
\label{table:auswahlarten_vergleich_direction}
\end{table}



\begin{table}[ht]
\caption{Vergleich der verschiedenen Auswahlarten (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario, Geschwindigkeit 1, Agenten mit XCS Algorithmus)}
\centering
\begin{tabular}{c c c c}
\hline\hline
Auswahlart & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Agent mit zufälliger Bewegung                      &  4,46\% & 72,12\% & 29,05\% \\[1ex]
\hline
\emph{roulette wheel selection}         &  4,54\% & 72,10\% & 30,30\% \\
\emph{random selection}                 &  4,34\% & 72,21\% & 28,50\% \\
\bf{\emph{tournament selection}}        & 11,21\% & 70,20\% & \bf{33,39\%} \\
\emph{best selection}                   & 41,16\% & 63,64\% & 29,22\% \\
Zufällig \emph{explore}/\emph{exploit}  &  6,29\% & 71,18\% & 30,58\% \\
Abwechselnd, zuerst \emph{explore}      &  5,63\% & 71,37\% & 26,30\% \\
\bf{Abwechselnd, zuerst \emph{exploit}} &  9,28\% & 70,40\% & \bf{35,36\%} \\[1ex]
\hline
\end{tabular}
\label{table:auswahlarten_vergleich_direction_xcs}
\end{table}

TODO ges2, int 1 und 2

\begin{table}[ht]
\caption{Vergleich der verschiedenen Auswahlarten (Zielobjekt mit einfacher Richtungsänderung, Säulenszenario, {\bf Geschwindigkeit 2}, Agenten mit SXCS Algorithmus)}
\centering
\begin{tabular}{c c c c}
\hline\hline
Auswahlart & Blockierte Bewegungen & Abdeckung & Qualität \\ [1ex]
\hline
Agent mit zufälliger Bewegung           &  4,46\% & 72,12\% & 29,05\% \\[1ex]
\hline
\emph{roulette wheel selection}         &  4,54\% & 72,10\% & 30,30\% \\
\emph{random selection}                 &  4,34\% & 72,21\% & 28,50\% \\
\bf{\emph{tournament selection}}        & 11,21\% & 70,20\% & \bf{33,39\%} \\
\emph{best selection}                   & 41,16\% & 63,64\% & 29,22\% \\
Zufällig \emph{explore}/\emph{exploit}  &  6,29\% & 71,18\% & 30,58\% \\
Abwechselnd, zuerst \emph{explore}      &  5,63\% & 71,37\% & 26,30\% \\
\bf{Abwechselnd, zuerst \emph{exploit}} &  9,28\% & 70,40\% & \bf{35,36\%} \\[1ex]
\hline
\end{tabular}
\label{table:auswahlarten_vergleich_direction2}
\end{table}

XCS + switch wirkungslos!


\section{Auswirkung unterschiedlicher Geschwindigkeiten des Zielobjekts}


In Abbildung~\ref{goal_agent_speed:fig} ist ein Vergleich der unterschiedlicher Geschwindigkeiten des Zielobjekts dargestellt. XCS (mit 500 Schritten) macht bei keiner Geschwindigkeit Lernfortschritte, die Qualität pendelt zwischen 31,69\% und 33,40\%, also in etwa identisch mit der zufälligen Bewegung. Die SXCS Implementierung scheint dagegen die geringere Geschwindigkeit ausgenutzt zu haben und ist dadurch in der Lage das Zielobjekt besser zu verfolgen. Mit 500 Schritten ist die Qualität abnehmend von 39,64\% (Geschwindigkeit 1,0) bis 35,96\% (Geschwindigkeit 2.0), im Fall mit 2000 Schritten erhöht sich dieser Bereich leicht auf 40,15\% bis 37,71\%.\\
Auch bei den Heuristiken zeichnet sich ein klares Bild ab, bei niedrigen Geschwindigkeiten ist die Ausbreitung der Agenten auf dem Feld (intelligente Heuristik) weniger wichtig als die konstante Verfolgung des Zielobjekts, während bei höheren Geschwindigkeiten die Verteilung auf dem Feld wichtiger wird.

\begin{figure}[htbp]
\centerline{	
\includegraphics{goal_agent_speed.eps}
}
\caption[Vergleich der Qualitäten verschiedener Algorithmen bezüglich der Geschwindigkeit des Zielobjekts] {Vergleich der Qualitäten verschiedener Algorithmen bezüglich der Geschwindigkeit des Zielobjekts}
\label{goal_agent_speed:fig}
\end{figure}

\begin{verbatim}
Bester Agent nach 20000 Schritten 
(Zielgeschwindigkeit 2.0, SXCS, 2000 Schritte)

#0######.###0#0##.#0#0###0-S : Fi: 38\% Ex: 450 Pr: 0,74 PE: 38\%
....
TODO
\end{verbatim}


\section{Zielobjekt mit XCS und SXCS}\label{goal_agent_with_xcs:sec}

Der in Kapitel~\ref{variant_zielobjekt_xcs_sxcs:sec} besprochene Ansatz, einen Zielobjekt mit einem XCS bzw. SXCS auszustatten, soll hier nun getestet werden. Dabei sollen zuerst Agenten mit den besprochenen statischen Heuristiken gegen diesen Zielobjekt antreten, abschließend soll - hauptsächlich aus Neugier - ein solches Zielobjekt gegen ebenfalls mit SXCS ausgestatteten Agenten antreten, hier soll insbesondere der Verlauf über die Zeit interessieren, da die Qualität TODO

Wichtig: niedrige Lerngeschwindigkeit!
evtl viele Probleme testen

\section{Zusammenfassung der bisherigen Erkenntnisse}

 Algorithmen mit Ergebnissen die unter dem des zufälligen Algorithmus liegt, sind unbrauchbar und nicht vergleichbar. ``Verbesserungen'', die die Qualität des Algorithmus näher an das Ergebnis des zufälligen Algorithmus bringen, sind in Wirklichkeit Veränderungen, die den Algorithmus eher zufällige Entscheidungen treffen lassen, und keine tatsächlichen Lernerfolge.

\section{Von den Agenten nicht schaffbare Szenarien TODO Titel}

TODO im schwierigen Szenario, Intelligent speed 2...
weder XCS noch SXCS schaffen es. ?

Säulenszenario:
XCS mit unterschiedlichem gamma (prediction discount)!!

Schwieriges Szenario:
Populationsgrößen mit und ohne zufälliger Initialisierung (covered actions)?
Lernrate XCS/SXCS, 0,01-0,1

% OK! TODO mit und ohne numerosity Korrektur

auch mit Geschwindigkeit 0,1 oder so testen
- pillar, 1 direction change, speed 1, max pred + GA, 0,01 prediction init, prediction init adaption
- nur gering gg random

- pillar, intelligent, low speed, maxpred aus, GA, SWITCH EXPLOIT, SEHR GUT!

Säulenszenario
random, simple, intelligent, xcs, sxcs 500, sxcs 2000

DSXCS Konvergenzgeschwindigkeit vergleichen! Wenige Schritte schwieriges Szenario
DSXCS besser mit geringem max stack size... TODO max stack size test für dsxcs

SXCS sehr gut bei NO DIRECTION CHANGE und speed 1!

nicht geschafft: Pillar, one direction change, speed 2, XCS besser... weil zufälliger

TODO auch sich langsam bewegende analysieren!
Und auch stehenbleibende : z.B. im Raumszenario.

Geschwindigkeit 2 problematisch, Geschwindigkeit 1 ok?

TODO classifier ausgeben 

SXCS sehr gut bei NO DIRECTION CHANGE und speed 1!


nicht geschafft: Pillar, one direction change, speed 2, XCS ...besser... weil zufälliger

    deutlich schneller als intelligenter weil der nicht lernt




\subsection{SXCS und Heuristiken}

In den Tests erreichten die Heuristiken deutlich bessere Ergebnisse. Diesen Nachteil hat sich XCS in diesen Szenarien durch deutlich überlegene Flexibilität erkauft TODO


Ein Großteil der eingehenden Informationen ist für die Auswertung nicht relevant und lokale Information ist zu ungenau.
Bei einer komplexeren Implementierung mit Distanzen

Insbesondere der Vergleich mit dem intelligenten Agenten, der anderen Agenten ausweicht, zeigt, dass die SXCS Agenten unmöglich ein solches globales Ziel erreichen können, es ist also kein emergentes Verhalten zu beobachten. Dies ist dadurch zu begründen, dass bei der Berechnung des Rewards keine Information außer der eigenen, lokalen Information 

der Abstand zu anderen Agenten nicht Teil der Berechnung des Rewards ist, noch gibt keine eingebaute Heuristik. Man könnte zwar 


\subsection{Test Kommunikation}

\subsection{Bewertung Kommunikation:}

Die Vorteile, die man durch Kommunikation erzielen kann, hängt stark von dem Szenario ab. Beispielsweise in dem Fall, bei dem zufällige Agenten bereits fast 100\% Abdeckung erreichen, also so viele Agenten auf dem Feld sind, dass der Gewinn durch Absprache minimal ist. Auch ist, weil nur mit Binärsensoren gearbeitet wird, die Sensorik gestört, wenn sich sehr viele Agenten auf dem Feld befinden, weil die Sensoren sehr oft gesetzt sind und somit wenig Aussagekraft haben. Erweiterungen wie zusätzliche Sensoren die die genauen Abstände bestimmen, würde hier wahrscheinlich klarere Ergebnisse liefern.


Umgekehrt ist der Einfluss bei sehr wenigen Agenten gering. TODO
Vergleich unterschiedliche Agentenanzahl, unterschiedliche Kommunikationsmittel
Vergleich mit LCS?

\subsection{XCS im schwierigen Szenario}\label{xcs_difficult_scenario:sec}

Im schwierigen Szenario wurde in Kapitel~\ref{test_schwieriges_szenario:sec} gezeigt, dass hier sich zufällige bewegende Agenten wie auch Agenten mit einfacher Heuristik versagen. Auch wurde argumentiert, dass Agenten mit intelligenter Heuristik nur deshalb Erfolg haben, weil sie sich gegenseitig durch die Öffnungen "`drängen"'. Hier sollen nun lernende Agenten ihre Fähigkeiten unter Beweis stellen. Der wesentliche Vorteil der lernenden Agenten hier ist, dass sie ihr gelerntes über die, wie bisher, 10 Probleme behalten können und somit, sofern sie das Richtige gelernt haben, direkt auf den letzten Abschnitt durch die Öffnungen laufen können.\\
Auch soll sich hier wieder das Zielobjekt nur in einer Linie bewegen (siehe Kapitel~\ref{no_direction_change:sec}), es ist also im Grunde kein Überwachungsszenario im eigentlichen Sinne, wenn ein Agent das letzte Feld erreicht, ist das Problem im Grunde schon gelöst. Die Hauptschwierigkeit ist, das letzte Feld zu erreichen.\\

XCS 125 - 4000



LCS Agenten schneiden auch ohne Kommunikation (bei ausreichender Anzahl von Schritten) immer besser ab als zufällige Agenten.







TODO

Anzumerken war, dass 

TODO explore/exploit, nur bei explore lernen, erwähnen dass immer gelernt werden muss

TODO oberes Limit der Genauigkeit, 84\%, da ja geswitched wird.

Geringen Unterschied ansprechen, mit zufälligem Algorithmus argumentieren,
vielleicht ein BEispiel rechnen wo Qualität des zufälligen abgezogen wird!
